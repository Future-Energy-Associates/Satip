{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to the Satip Documentation Site \u00b6 We can add a to do list: With tasks like this And optional tick boxes Footnotes can also be added 1 We can also display MathJax like this: \\(2+2=5\\) No matter where this is added in the text it will be rendered in the footer \u21a9","title":"Home"},{"location":"#welcome-to-the-satip-documentation-site","text":"We can add a to do list: With tasks like this And optional tick boxes Footnotes can also be added 1 We can also display MathJax like this: \\(2+2=5\\) No matter where this is added in the text it will be rendered in the footer \u21a9","title":"Welcome to the Satip Documentation Site"},{"location":"00_utils/","text":"Repository Helpers \u00b6 Loading Environment Variables \u00b6 First we'll load the the environment variables env_vars_fp = '../.env' dotenv . load_dotenv ( env_vars_fp ) slack_id = os . environ . get ( 'slack_id' ) slack_webhook_url = os . environ . get ( 'slack_webhook_url' ) Notebook Information \u00b6 We can now easily construct markdown tables notebook_info = { # development 'Repository Helpers' : { 'Directory' : 'notebooks' , 'Number' : '00' , 'Description' : 'Code for keeping the repository tidy' , 'Maintainer' : 'Ayrton Bourn' }, 'EUMETSAT API Wrapper' : { 'Directory' : 'notebooks' , 'Number' : '01' , 'Description' : 'Development of the API wrapper for ems' , 'Maintainer' : 'Ayrton Bourn' }, 'Data Transformation' : { 'Directory' : 'notebooks' , 'Number' : '02' , 'Description' : 'Intial EDA and transformation comparisons' , 'Maintainer' : 'Ayrton Bourn' }, # usage_examples 'EUMETSAT Download' : { 'Directory' : 'usage_examples' , 'Number' : '00' , 'Description' : 'Guidance for using the ems download manager' , 'Maintainer' : 'Ayrton Bourn' }, } nb_table_str = create_markdown_table ( notebook_info ) print ( nb_table_str ) | Id | Directory | Number | Description | Maintainer | |:---------------------|:---------------|---------:|:--------------------------------------------|:-------------| | Repository Helpers | notebooks | 00 | Code for keeping the repository tidy | Ayrton Bourn | | EUMETSAT API Wrapper | notebooks | 01 | Development of the API wrapper for ems | Ayrton Bourn | | Data Transformation | notebooks | 02 | Intial EDA and transformation comparisons | Ayrton Bourn | | EUMETSAT Download | usage_examples | 00 | Guidance for using the ems download manager | Ayrton Bourn | Logging \u00b6 We'll now initialise the logger and make a test log logger = set_up_logging ( __name__ , '.' , slack_id = slack_id , slack_webhook_url = slack_webhook_url ) logger . log ( logging . INFO , 'This will output to file and Jupyter but not to Slack as it is not critical' ) 2020-11-12 09:58:41,301 - INFO - This will output to file and Jupyter but not to Slack as it is not critical We'll now shutdown the logger handlers and then delete the log file we just made handlers = logger . handlers [:] for handler in handlers : handler . close () logger . removeHandler ( handler ) os . remove ( f ' { __name__ } .txt' ) Finally we'll export the specified functions to the utils.py module","title":"Utilities"},{"location":"00_utils/#repository-helpers","text":"","title":"Repository Helpers"},{"location":"00_utils/#loading-environment-variables","text":"First we'll load the the environment variables env_vars_fp = '../.env' dotenv . load_dotenv ( env_vars_fp ) slack_id = os . environ . get ( 'slack_id' ) slack_webhook_url = os . environ . get ( 'slack_webhook_url' )","title":"Loading Environment Variables"},{"location":"00_utils/#notebook-information","text":"We can now easily construct markdown tables notebook_info = { # development 'Repository Helpers' : { 'Directory' : 'notebooks' , 'Number' : '00' , 'Description' : 'Code for keeping the repository tidy' , 'Maintainer' : 'Ayrton Bourn' }, 'EUMETSAT API Wrapper' : { 'Directory' : 'notebooks' , 'Number' : '01' , 'Description' : 'Development of the API wrapper for ems' , 'Maintainer' : 'Ayrton Bourn' }, 'Data Transformation' : { 'Directory' : 'notebooks' , 'Number' : '02' , 'Description' : 'Intial EDA and transformation comparisons' , 'Maintainer' : 'Ayrton Bourn' }, # usage_examples 'EUMETSAT Download' : { 'Directory' : 'usage_examples' , 'Number' : '00' , 'Description' : 'Guidance for using the ems download manager' , 'Maintainer' : 'Ayrton Bourn' }, } nb_table_str = create_markdown_table ( notebook_info ) print ( nb_table_str ) | Id | Directory | Number | Description | Maintainer | |:---------------------|:---------------|---------:|:--------------------------------------------|:-------------| | Repository Helpers | notebooks | 00 | Code for keeping the repository tidy | Ayrton Bourn | | EUMETSAT API Wrapper | notebooks | 01 | Development of the API wrapper for ems | Ayrton Bourn | | Data Transformation | notebooks | 02 | Intial EDA and transformation comparisons | Ayrton Bourn | | EUMETSAT Download | usage_examples | 00 | Guidance for using the ems download manager | Ayrton Bourn |","title":"Notebook Information"},{"location":"00_utils/#logging","text":"We'll now initialise the logger and make a test log logger = set_up_logging ( __name__ , '.' , slack_id = slack_id , slack_webhook_url = slack_webhook_url ) logger . log ( logging . INFO , 'This will output to file and Jupyter but not to Slack as it is not critical' ) 2020-11-12 09:58:41,301 - INFO - This will output to file and Jupyter but not to Slack as it is not critical We'll now shutdown the logger handlers and then delete the log file we just made handlers = logger . handlers [:] for handler in handlers : handler . close () logger . removeHandler ( handler ) os . remove ( f ' { __name__ } .txt' ) Finally we'll export the specified functions to the utils.py module","title":"Logging"},{"location":"01_eumetsat/","text":"EUMETSAT API Wrapper Development \u00b6 C:\\Users\\Ayrto\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\google\\auth\\_default.py:69: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK without a quota project. You might receive a \"quota exceeded\" or \"API not enabled\" error. We recommend you rerun `gcloud auth application-default login` and make sure a quota project is added. Or you can use service accounts instead. For more information about service accounts, see https://cloud.google.com/docs/authentication/ warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING) Downloading: 100%|\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6| 1/1 [00:00<00:00, 1.41rows/s] User Input \u00b6 data_dir = '../data/raw' sorted_dir = '../data/sorted' debug_fp = '../logs/EUMETSAT_download.txt' env_vars_fp = '../.env' metadata_db_fp = '../data/EUMETSAT_metadata.db' download_data = True Authorising API Access \u00b6 First we'll load the the environment variables dotenv . load_dotenv ( env_vars_fp ) user_key = os . environ . get ( 'USER_KEY' ) user_secret = os . environ . get ( 'USER_SECRET' ) slack_id = os . environ . get ( 'SLACK_ID' ) slack_webhook_url = os . environ . get ( 'SLACK_WEBHOOK_URL' ) And test they were loaded successfully def check_env_vars_have_loaded ( env_vars ): for name , value in env_vars . items (): assert value is not None , f ' { name } ` should not be None' return env_vars = { 'user_key' : user_key , 'user_secret' : user_secret , } check_env_vars_have_loaded ( env_vars ) We'll then use them to request an access token for the API We'll then use them to request an access token for the API access_token = request_access_token ( user_key , user_secret ) Querying Available Data \u00b6 Before we can download any data we have to know where it's stored. To learn this we can query their search-products API, which returns a JSON containing a list of file metadata. We'll quickly make a test request to this end-point start_date = '2019-10-01' end_date = '2019-10-07' r = query_data_products ( start_date , end_date ) r_json = r . json () JSON ( r_json ) <IPython.core.display.JSON object> However the search-api is capped (at 10,000) for the number of files it will return metadata for, so we'll create a while loop that waits until all the relevant data has been returned. We'll then extract just the list of features from the returned JSONs. We'll check that the same number of available datasets are identified %% time datasets = identify_available_datasets ( start_date , end_date ) print ( f ' { len ( datasets ) } datasets have been identified' ) 1728 datasets have been identified CPU times: user 200 ms, sys: 17.9 ms, total: 218 ms Wall time: 1.27 s Finally we'll create a helper function for converting the dataset ids into their file urls. We'll now test this works. N.b. You cannot use the link returned here directly as it will not be OAuth'ed dataset_ids = sorted ([ dataset [ 'id' ] for dataset in datasets ]) example_data_link = dataset_id_to_link ( dataset_ids [ 0 ]) example_data_link 'https://api.eumetsat.int/data/download/products/MSG3-SEVI-MSG15-0100-NA-20201001000414.060000000Z-NA' Downloading Data \u00b6 Now that we know where our data is located we want to download it. First we'll check that the directory we wish to save the data in exists, if not we'll create it for folder in [ data_dir , sorted_dir ]: if not os . path . exists ( folder ): os . makedirs ( folder ) We also want to extract the relevant metadata information from each file. Here we'll create a generalised framework for extracting data from any product, to add a new one please add its metadata mapping under the relevant product_id . We're now ready to create a download manager that will handle all of the querying, processing and retrieving for us We'll now see what it looks like when we initialise the download manager # dm = DownloadManager(user_key, user_secret, data_dir, metadata_db_fp, debug_fp) # start_date = '2019-10-01 00:00' # end_date = '2019-10-01 00:05' # if download_data == True: # dm.download_datasets(start_date, end_date) dm = DownloadManager ( user_key , user_secret , data_dir , metadata_db_fp , debug_fp , slack_webhook_url = slack_webhook_url , slack_id = slack_id ) start_date = '2020-10-01 12:00' end_date = '2020-10-01 12:05' if download_data == True : dm . download_datasets ( start_date , end_date ) df_metadata = dm . get_df_metadata () df_metadata . head () The get_size function was adapted from this stackoverflow answer data_dir_size = get_dir_size ( data_dir ) print ( f 'The data directory is currently { round ( data_dir_size / 1_000_000_000 , 2 ) : , } Gb' ) Trying out bucket files \u00b6 If GCP flags are passed ( bucket_name and bucket_prefix ), when downloading the DownloadManager should first check to see if the files already exist in the specified bucket. If the files already exist, then don't download them. BUCKET_NAME = \"solar-pv-nowcasting-data\" # PREFIX = \"satellite/EUMETSAT/SEVIRI_RSS/native/\" PREFIX = \"satellite/EUMETSAT/SEVIRI_RSS/native/2020\" dm = DownloadManager ( user_key , user_secret , data_dir , metadata_db_fp , debug_fp , bucket_name = BUCKET_NAME , bucket_prefix = PREFIX ) 2020-11-30 10:21:30,271 - INFO - ********** Download Manager Initialised ************** Checking files in GCP bucket solar-pv-nowcasting-data, this will take a few seconds len ( dm . bucket_filenames ) 2 % time # took around 2 hours to download 1 day. # DownloadManager should find these 2019 files on the VM start_date = '2020-01-01 00:00' end_date = '2020-01-02 00:00' dm . download_datasets ( start_date , end_date ) CPU times: user 3 \u00c2\u00b5s, sys: 1 \u00c2\u00b5s, total: 4 \u00c2\u00b5s Wall time: 6.2 \u00c2\u00b5s 2020-11-30 10:22:42,304 - INFO - 288 files queried, 2 found in bucket, 0 found in ../data/raw, 286 to download. 100% 286/286 [02:08:45 < 00:22, 27.01s/it] 2020-11-30 11:31:58,945 - INFO - The EUMETSAT access token has been refreshed However, the already downloaded files follow a different file name convention. Files through new API: MSG3-SEVI-MSG15-0100-NA-20191001120415.883000000Z-NA.nat SatProgram-Instrument-SatNumber-AlgoVersion-InstrumentMode(?)-ReceptionStartDateUTC Files on GCP: MSG3-SEVI-MSG15-0100-NA-20191001120415.883000000Z-20191001120433-1399526-1.nat.bz2 SatProgram-Instrument-SatNumber-AlgoVersion-InstrumentMode(?)-ReceptionStartDateUTC-SensingStartDateUTC-OrderNumber-PartNumber Let's have a look at the filename lengths. filenames = pd . DataFrame ( dm . bucket_filenames , columns = [ 'filenames' ]) filenames [ 'length' ] = filenames [ 'filenames' ] . str . len () filenames . groupby ( 'length' ) . count () print ( filenames . loc [ 0 ] . filenames ) print ( filenames . loc [ 102460 ] . filenames ) Looks like the length difference is just due to the part number at the end of the filename We can use regex to take the first part of the filename for comparisons txt = \"MSG3-SEVI-MSG15-0100-NA-20190101000417.314000000Z-20190101000435-1377854-1.nat\" re . match ( \"([A-Z\\d.]+-) {6} \" , txt )[ 0 ][: - 1 ] # [:-1] to trim the trailing - Now we're comparing the same filenames by adding this regex into DownloadManager and the GCP helpers. Logging \u00b6 debug_fp = '../logs/EUMETSAT_download.txt' log = utils . set_up_logging ( 'EUMETSAT Processing' , debug_fp ) Helper utils \u00b6 Download files from GCP \u00b6 # get test files from GCP - these are compressed # !gsutil cp -r gs://solar-pv-nowcasting-data/satellite/EUMETSAT/SEVIRI_RSS/native/2019/10/01/00/04 ../data/raw Compress and move files \u00b6 full_native_filenames = glob . glob ( os . path . join ( data_dir , '*.nat' )) full_native_filenames [] We will compress locally downloaded files here using pbzip2 On ubuntu: sudo apt-get install -y pbzip2 On mac: brew install pbzip2 compress_downloaded_files ( data_dir = data_dir , sorted_dir = sorted_dir ) 2020-11-29 22:10:05,805 - INFO - Found 0 native files. Found 0 native files. Sync to GCP Storage \u00b6 store the bzip2 in gs://solar-pv-nowcasting-data/satellite/EUMETSAT/SEVIRI_RSS/native/<year>/<month>/<day>/<hour>/<minute>/ # sync downloaded files in sorted_dir to the bucket BUCKET_NAME = \"solar-pv-nowcasting-data\" PREFIX = \"satellite/EUMETSAT/SEVIRI_RSS/native/\"","title":"Downloading"},{"location":"01_eumetsat/#eumetsat-api-wrapper-development","text":"C:\\Users\\Ayrto\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\google\\auth\\_default.py:69: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK without a quota project. You might receive a \"quota exceeded\" or \"API not enabled\" error. We recommend you rerun `gcloud auth application-default login` and make sure a quota project is added. Or you can use service accounts instead. For more information about service accounts, see https://cloud.google.com/docs/authentication/ warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING) Downloading: 100%|\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6| 1/1 [00:00<00:00, 1.41rows/s]","title":"EUMETSAT API Wrapper Development"},{"location":"01_eumetsat/#user-input","text":"data_dir = '../data/raw' sorted_dir = '../data/sorted' debug_fp = '../logs/EUMETSAT_download.txt' env_vars_fp = '../.env' metadata_db_fp = '../data/EUMETSAT_metadata.db' download_data = True","title":"User Input"},{"location":"01_eumetsat/#authorising-api-access","text":"First we'll load the the environment variables dotenv . load_dotenv ( env_vars_fp ) user_key = os . environ . get ( 'USER_KEY' ) user_secret = os . environ . get ( 'USER_SECRET' ) slack_id = os . environ . get ( 'SLACK_ID' ) slack_webhook_url = os . environ . get ( 'SLACK_WEBHOOK_URL' ) And test they were loaded successfully def check_env_vars_have_loaded ( env_vars ): for name , value in env_vars . items (): assert value is not None , f ' { name } ` should not be None' return env_vars = { 'user_key' : user_key , 'user_secret' : user_secret , } check_env_vars_have_loaded ( env_vars ) We'll then use them to request an access token for the API We'll then use them to request an access token for the API access_token = request_access_token ( user_key , user_secret )","title":"Authorising API Access"},{"location":"01_eumetsat/#querying-available-data","text":"Before we can download any data we have to know where it's stored. To learn this we can query their search-products API, which returns a JSON containing a list of file metadata. We'll quickly make a test request to this end-point start_date = '2019-10-01' end_date = '2019-10-07' r = query_data_products ( start_date , end_date ) r_json = r . json () JSON ( r_json ) <IPython.core.display.JSON object> However the search-api is capped (at 10,000) for the number of files it will return metadata for, so we'll create a while loop that waits until all the relevant data has been returned. We'll then extract just the list of features from the returned JSONs. We'll check that the same number of available datasets are identified %% time datasets = identify_available_datasets ( start_date , end_date ) print ( f ' { len ( datasets ) } datasets have been identified' ) 1728 datasets have been identified CPU times: user 200 ms, sys: 17.9 ms, total: 218 ms Wall time: 1.27 s Finally we'll create a helper function for converting the dataset ids into their file urls. We'll now test this works. N.b. You cannot use the link returned here directly as it will not be OAuth'ed dataset_ids = sorted ([ dataset [ 'id' ] for dataset in datasets ]) example_data_link = dataset_id_to_link ( dataset_ids [ 0 ]) example_data_link 'https://api.eumetsat.int/data/download/products/MSG3-SEVI-MSG15-0100-NA-20201001000414.060000000Z-NA'","title":"Querying Available Data"},{"location":"01_eumetsat/#downloading-data","text":"Now that we know where our data is located we want to download it. First we'll check that the directory we wish to save the data in exists, if not we'll create it for folder in [ data_dir , sorted_dir ]: if not os . path . exists ( folder ): os . makedirs ( folder ) We also want to extract the relevant metadata information from each file. Here we'll create a generalised framework for extracting data from any product, to add a new one please add its metadata mapping under the relevant product_id . We're now ready to create a download manager that will handle all of the querying, processing and retrieving for us We'll now see what it looks like when we initialise the download manager # dm = DownloadManager(user_key, user_secret, data_dir, metadata_db_fp, debug_fp) # start_date = '2019-10-01 00:00' # end_date = '2019-10-01 00:05' # if download_data == True: # dm.download_datasets(start_date, end_date) dm = DownloadManager ( user_key , user_secret , data_dir , metadata_db_fp , debug_fp , slack_webhook_url = slack_webhook_url , slack_id = slack_id ) start_date = '2020-10-01 12:00' end_date = '2020-10-01 12:05' if download_data == True : dm . download_datasets ( start_date , end_date ) df_metadata = dm . get_df_metadata () df_metadata . head () The get_size function was adapted from this stackoverflow answer data_dir_size = get_dir_size ( data_dir ) print ( f 'The data directory is currently { round ( data_dir_size / 1_000_000_000 , 2 ) : , } Gb' )","title":"Downloading Data"},{"location":"01_eumetsat/#trying-out-bucket-files","text":"If GCP flags are passed ( bucket_name and bucket_prefix ), when downloading the DownloadManager should first check to see if the files already exist in the specified bucket. If the files already exist, then don't download them. BUCKET_NAME = \"solar-pv-nowcasting-data\" # PREFIX = \"satellite/EUMETSAT/SEVIRI_RSS/native/\" PREFIX = \"satellite/EUMETSAT/SEVIRI_RSS/native/2020\" dm = DownloadManager ( user_key , user_secret , data_dir , metadata_db_fp , debug_fp , bucket_name = BUCKET_NAME , bucket_prefix = PREFIX ) 2020-11-30 10:21:30,271 - INFO - ********** Download Manager Initialised ************** Checking files in GCP bucket solar-pv-nowcasting-data, this will take a few seconds len ( dm . bucket_filenames ) 2 % time # took around 2 hours to download 1 day. # DownloadManager should find these 2019 files on the VM start_date = '2020-01-01 00:00' end_date = '2020-01-02 00:00' dm . download_datasets ( start_date , end_date ) CPU times: user 3 \u00c2\u00b5s, sys: 1 \u00c2\u00b5s, total: 4 \u00c2\u00b5s Wall time: 6.2 \u00c2\u00b5s 2020-11-30 10:22:42,304 - INFO - 288 files queried, 2 found in bucket, 0 found in ../data/raw, 286 to download. 100% 286/286 [02:08:45 < 00:22, 27.01s/it] 2020-11-30 11:31:58,945 - INFO - The EUMETSAT access token has been refreshed However, the already downloaded files follow a different file name convention. Files through new API: MSG3-SEVI-MSG15-0100-NA-20191001120415.883000000Z-NA.nat SatProgram-Instrument-SatNumber-AlgoVersion-InstrumentMode(?)-ReceptionStartDateUTC Files on GCP: MSG3-SEVI-MSG15-0100-NA-20191001120415.883000000Z-20191001120433-1399526-1.nat.bz2 SatProgram-Instrument-SatNumber-AlgoVersion-InstrumentMode(?)-ReceptionStartDateUTC-SensingStartDateUTC-OrderNumber-PartNumber Let's have a look at the filename lengths. filenames = pd . DataFrame ( dm . bucket_filenames , columns = [ 'filenames' ]) filenames [ 'length' ] = filenames [ 'filenames' ] . str . len () filenames . groupby ( 'length' ) . count () print ( filenames . loc [ 0 ] . filenames ) print ( filenames . loc [ 102460 ] . filenames ) Looks like the length difference is just due to the part number at the end of the filename We can use regex to take the first part of the filename for comparisons txt = \"MSG3-SEVI-MSG15-0100-NA-20190101000417.314000000Z-20190101000435-1377854-1.nat\" re . match ( \"([A-Z\\d.]+-) {6} \" , txt )[ 0 ][: - 1 ] # [:-1] to trim the trailing - Now we're comparing the same filenames by adding this regex into DownloadManager and the GCP helpers.","title":"Trying out bucket files"},{"location":"01_eumetsat/#logging","text":"debug_fp = '../logs/EUMETSAT_download.txt' log = utils . set_up_logging ( 'EUMETSAT Processing' , debug_fp )","title":"Logging"},{"location":"01_eumetsat/#helper-utils","text":"","title":"Helper utils"},{"location":"01_eumetsat/#download-files-from-gcp","text":"# get test files from GCP - these are compressed # !gsutil cp -r gs://solar-pv-nowcasting-data/satellite/EUMETSAT/SEVIRI_RSS/native/2019/10/01/00/04 ../data/raw","title":"Download files from GCP"},{"location":"01_eumetsat/#compress-and-move-files","text":"full_native_filenames = glob . glob ( os . path . join ( data_dir , '*.nat' )) full_native_filenames [] We will compress locally downloaded files here using pbzip2 On ubuntu: sudo apt-get install -y pbzip2 On mac: brew install pbzip2 compress_downloaded_files ( data_dir = data_dir , sorted_dir = sorted_dir ) 2020-11-29 22:10:05,805 - INFO - Found 0 native files. Found 0 native files.","title":"Compress and move files"},{"location":"01_eumetsat/#sync-to-gcp-storage","text":"store the bzip2 in gs://solar-pv-nowcasting-data/satellite/EUMETSAT/SEVIRI_RSS/native/<year>/<month>/<day>/<hour>/<minute>/ # sync downloaded files in sorted_dir to the bucket BUCKET_NAME = \"solar-pv-nowcasting-data\" PREFIX = \"satellite/EUMETSAT/SEVIRI_RSS/native/\"","title":"Sync to GCP Storage"},{"location":"02_reproj/","text":"Data Transformation \u00b6 #exports import json import pandas as pd import xarray as xr import numpy as np import numpy.ma as ma import matplotlib as mpl import matplotlib.pyplot as plt from matplotlib import colors import seaborn as sns import os import time from itertools import product from collections import OrderedDict from datetime import datetime from ipypb import track import FEAutils as hlp import satpy from satpy import Scene from satpy.readers import seviri_l1b_native import pyresample from pyresample.geometry import AreaDefinition try : import pyinterp import pyinterp.backends.xarray except : pass We'll separately install libraries that wont be needed for the satip module import rasterio from rasterio import Affine as A from rasterio.warp import reproject , Resampling , calculate_default_transform , transform from rasterio.control import GroundControlPoint from rasterio.transform import xy import geopandas as gpd from shapely.geometry import Point import cartopy.crs as ccrs from IPython.display import JSON User Input \u00b6 data_dir = '../data/raw' intermediate_data_dir = '../data/intermediate' calculate_reproj_coords = False Exploratory Data Analysis \u00b6 We'll start by identifying the available files native_fps = sorted ([ f ' { data_dir } / { f } ' for f in os . listdir ( data_dir ) if '.nat' in f ]) native_fps [ 0 ] '../data/raw/MSG2-SEVI-MSG15-0100-NA-20201208090415.301000000Z-NA.nat' Then load one of them in as a SatPy scene native_fp = native_fps [ 0 ] scene = Scene ( filenames = [ native_fp ], reader = 'seviri_l1b_native' ) scene <satpy.scene.Scene at 0x1911d108580> We can get a list of the available datasets (bands) scene . all_dataset_names () ['HRV', 'IR_016', 'IR_039', 'IR_087', 'IR_097', 'IR_108', 'IR_120', 'IR_134', 'VIS006', 'VIS008', 'WV_062', 'WV_073'] Each band contains an XArray DataArray scene . load ([ 'HRV' ]) scene [ 'HRV' ] C:\\Users\\Ayrto\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\pyproj\\crs\\crs.py:543: UserWarning: You will likely lose important projection information when converting to a PROJ string from another format. See: https://proj.org/faq.html#what-is-the-best-format-for-describing-coordinate-reference-systems proj_string = self.to_proj4() /* CSS stylesheet for displaying xarray objects in jupyterlab. * */ :root { --xr-font-color0: var(--jp-content-font-color0, rgba(0, 0, 0, 1)); --xr-font-color2: var(--jp-content-font-color2, rgba(0, 0, 0, 0.54)); --xr-font-color3: var(--jp-content-font-color3, rgba(0, 0, 0, 0.38)); --xr-border-color: var(--jp-border-color2, #e0e0e0); --xr-disabled-color: var(--jp-layout-color3, #bdbdbd); --xr-background-color: var(--jp-layout-color0, white); --xr-background-color-row-even: var(--jp-layout-color1, white); --xr-background-color-row-odd: var(--jp-layout-color2, #eeeeee); } html[theme=dark], body.vscode-dark { --xr-font-color0: rgba(255, 255, 255, 1); --xr-font-color2: rgba(255, 255, 255, 0.54); --xr-font-color3: rgba(255, 255, 255, 0.38); --xr-border-color: #1F1F1F; --xr-disabled-color: #515151; --xr-background-color: #111111; --xr-background-color-row-even: #111111; --xr-background-color-row-odd: #313131; } .xr-wrap { display: block; min-width: 300px; max-width: 700px; } .xr-text-repr-fallback { /* fallback to plain text repr when CSS is not injected (untrusted notebook) */ display: none; } .xr-header { padding-top: 6px; padding-bottom: 6px; margin-bottom: 4px; border-bottom: solid 1px var(--xr-border-color); } .xr-header > div, .xr-header > ul { display: inline; margin-top: 0; margin-bottom: 0; } .xr-obj-type, .xr-array-name { margin-left: 2px; margin-right: 10px; } .xr-obj-type { color: var(--xr-font-color2); } .xr-sections { padding-left: 0 !important; display: grid; grid-template-columns: 150px auto auto 1fr 20px 20px; } .xr-section-item { display: contents; } .xr-section-item input { display: none; } .xr-section-item input + label { color: var(--xr-disabled-color); } .xr-section-item input:enabled + label { cursor: pointer; color: var(--xr-font-color2); } .xr-section-item input:enabled + label:hover { color: var(--xr-font-color0); } .xr-section-summary { grid-column: 1; color: var(--xr-font-color2); font-weight: 500; } .xr-section-summary > span { display: inline-block; padding-left: 0.5em; } .xr-section-summary-in:disabled + label { color: var(--xr-font-color2); } .xr-section-summary-in + label:before { display: inline-block; content: '\u00e2\u2013\u00ba'; font-size: 11px; width: 15px; text-align: center; } .xr-section-summary-in:disabled + label:before { color: var(--xr-disabled-color); } .xr-section-summary-in:checked + label:before { content: '\u00e2\u2013\u00bc'; } .xr-section-summary-in:checked + label > span { display: none; } .xr-section-summary, .xr-section-inline-details { padding-top: 4px; padding-bottom: 4px; } .xr-section-inline-details { grid-column: 2 / -1; } .xr-section-details { display: none; grid-column: 1 / -1; margin-bottom: 5px; } .xr-section-summary-in:checked ~ .xr-section-details { display: contents; } .xr-array-wrap { grid-column: 1 / -1; display: grid; grid-template-columns: 20px auto; } .xr-array-wrap > label { grid-column: 1; vertical-align: top; } .xr-preview { color: var(--xr-font-color3); } .xr-array-preview, .xr-array-data { padding: 0 5px !important; grid-column: 2; } .xr-array-data, .xr-array-in:checked ~ .xr-array-preview { display: none; } .xr-array-in:checked ~ .xr-array-data, .xr-array-preview { display: inline-block; } .xr-dim-list { display: inline-block !important; list-style: none; padding: 0 !important; margin: 0; } .xr-dim-list li { display: inline-block; padding: 0; margin: 0; } .xr-dim-list:before { content: '('; } .xr-dim-list:after { content: ')'; } .xr-dim-list li:not(:last-child):after { content: ','; padding-right: 5px; } .xr-has-index { font-weight: bold; } .xr-var-list, .xr-var-item { display: contents; } .xr-var-item > div, .xr-var-item label, .xr-var-item > .xr-var-name span { background-color: var(--xr-background-color-row-even); margin-bottom: 0; } .xr-var-item > .xr-var-name:hover span { padding-right: 5px; } .xr-var-list > li:nth-child(odd) > div, .xr-var-list > li:nth-child(odd) > label, .xr-var-list > li:nth-child(odd) > .xr-var-name span { background-color: var(--xr-background-color-row-odd); } .xr-var-name { grid-column: 1; } .xr-var-dims { grid-column: 2; } .xr-var-dtype { grid-column: 3; text-align: right; color: var(--xr-font-color2); } .xr-var-preview { grid-column: 4; } .xr-var-name, .xr-var-dims, .xr-var-dtype, .xr-preview, .xr-attrs dt { white-space: nowrap; overflow: hidden; text-overflow: ellipsis; padding-right: 10px; } .xr-var-name:hover, .xr-var-dims:hover, .xr-var-dtype:hover, .xr-attrs dt:hover { overflow: visible; width: auto; z-index: 1; } .xr-var-attrs, .xr-var-data { display: none; background-color: var(--xr-background-color) !important; padding-bottom: 5px !important; } .xr-var-attrs-in:checked ~ .xr-var-attrs, .xr-var-data-in:checked ~ .xr-var-data { display: block; } .xr-var-data > table { float: right; } .xr-var-name span, .xr-var-data, .xr-attrs { padding-left: 25px !important; } .xr-attrs, .xr-var-attrs, .xr-var-data { grid-column: 1 / -1; } dl.xr-attrs { padding: 0; margin: 0; display: grid; grid-template-columns: 125px auto; } .xr-attrs dt, .xr-attrs dd { padding: 0; margin: 0; float: left; padding-right: 10px; width: auto; } .xr-attrs dt { font-weight: normal; grid-column: 1; } .xr-attrs dt:hover span { display: inline-block; background: var(--xr-background-color); padding-right: 10px; } .xr-attrs dd { grid-column: 2; white-space: pre-wrap; word-break: break-all; } .xr-icon-database, .xr-icon-file-text2 { display: inline-block; vertical-align: middle; width: 1em; height: 1.5em !important; stroke-width: 0; stroke: currentColor; fill: currentColor; } <xarray.DataArray 'reshape-3b944f9ca9a40a223ab6382d90bfb37d' (y: 4176, x: 5568)> dask.array<mul, shape=(4176, 5568), dtype=float32, chunksize=(1392, 5568), chunktype=numpy.ndarray> Coordinates: crs object PROJCRS[\"unknown\",BASEGEOGCRS[\"unknown\",DATUM[\"unknown\",E... * y (y) float64 1.395e+06 1.396e+06 1.397e+06 ... 5.57e+06 5.571e+06 * x (x) float64 3.164e+06 3.163e+06 3.162e+06 ... -2.402e+06 -2.403e+06 Attributes: orbital_parameters: {'projection_longitude': 9.5, 'pr... sun_earth_distance_correction_applied: True sun_earth_distance_correction_factor: 0.9697642568677852 units: % wavelength: 0.7\u00e2\u20ac\u00af\u00c2\u00b5m\u00c2 (0.5-0.9\u00e2\u20ac\u00af\u00c2\u00b5m) standard_name: toa_bidirectional_reflectance platform_name: Meteosat-9 sensor: seviri start_time: 2020-12-08 09:00:08.206321 end_time: 2020-12-08 09:05:08.329479 area: Area ID: geos_seviri_hrv\\nDescrip... name: HRV resolution: 1000.134348869 calibration: reflectance modifiers: () _satpy_id: DataID(name='HRV', wavelength=Wav... ancillary_variables: [] xarray.DataArray 'reshape-3b944f9ca9a40a223ab6382d90bfb37d' y : 4176 x : 5568 dask.array<chunksize=(1392, 5568), meta=np.ndarray> Array Chunk Bytes 93.01 MB 31.00 MB Shape (4176, 5568) (1392, 5568) Count 214 Tasks 3 Chunks Type float32 numpy.ndarray 5568 4176 Coordinates: (3) crs () object PROJCRS[\"unknown\",BASEGEOGCRS[\"u... array(<Projected CRS: PROJCRS[\"unknown\",BASEGEOGCRS[\"unknown\",DATUM[\"unk ...> Name: unknown Axis Info [cartesian]: - E[east]: Easting (metre) - N[north]: Northing (metre) Area of Use: - undefined Coordinate Operation: - name: unknown - method: Geostationary Satellite (Sweep Y) Datum: unknown - Ellipsoid: unknown - Prime Meridian: Greenwich , dtype=object) y (y) float64 1.395e+06 1.396e+06 ... 5.571e+06 units : meter array([1395187.416673, 1396187.551022, 1397187.68537 , ..., 5568748.054504, 5569748.188853, 5570748.323202]) x (x) float64 3.164e+06 3.163e+06 ... -2.403e+06 units : meter array([ 3164425.079823, 3163424.945474, 3162424.811125, ..., -2401322.571635, -2402322.705984, -2403322.840333]) Attributes: (17) orbital_parameters : {'projection_longitude': 9.5, 'projection_latitude': 0.0, 'projection_altitude': 35785831.0} sun_earth_distance_correction_applied : True sun_earth_distance_correction_factor : 0.9697642568677852 units : % wavelength : 0.7\u00e2\u20ac\u00af\u00c2\u00b5m\u00c2 (0.5-0.9\u00e2\u20ac\u00af\u00c2\u00b5m) standard_name : toa_bidirectional_reflectance platform_name : Meteosat-9 sensor : seviri start_time : 2020-12-08 09:00:08.206321 end_time : 2020-12-08 09:05:08.329479 area : Area ID: geos_seviri_hrv Description: SEVIRI high resolution channel area Projection ID: seviri_hrv Projection: {'a': '6378169', 'h': '35785831', 'lon_0': '9.5', 'no_defs': 'None', 'proj': 'geos', 'rf': '295.488065897014', 'type': 'crs', 'units': 'm', 'x_0': '0', 'y_0': '0'} Number of columns: 5568 Number of rows: 4176 Area extent: (3164925.147, 5571248.3904, -2403822.9075, 1394687.3495) name : HRV resolution : 1000.134348869 calibration : reflectance modifiers : () _satpy_id : DataID(name='HRV', wavelength=WavelengthRange(min=0.5, central=0.7, max=0.9, unit='\u00c2\u00b5m'), resolution=1000.134348869, calibration=<calibration.reflectance>, modifiers=()) ancillary_variables : [] We can see that the DataArray contains a crs, however we'll make our own custom area definition that's more accurate. First we'll create a helper function that will create our area definitions. #exports def calculate_x_offset ( native_fp ): handler = seviri_l1b_native . NativeMSGFileHandler ( native_fp , {}, None ) lower_east_column_planned = handler . header [ '15_DATA_HEADER' ][ 'ImageDescription' ][ 'PlannedCoverageHRV' ][ 'LowerEastColumnPlanned' ] x_offset = 32500 + (( 2733 - lower_east_column_planned ) * 1000 ) return x_offset def get_seviri_area_def ( native_fp , num_x_pixels = 5568 , num_y_pixels = 4176 ) -> AreaDefinition : \"\"\" The HRV channel on Meteosat Second Generation satellites doesn't scan the full number of columns. The east boundary of the HRV channel changes (e.g. to maximise the amount of the image which is illuminated by sunlight. Parameters: native_fp: Data filepath \"\"\" x_offset = calculate_x_offset ( native_fp ) # The EUMETSAT docs say \"The distance between spacecraft and centre of earth is 42,164 km. The idealized earth # is a perfect ellipsoid with an equator radius of 6378.1690 km and a polar radius of 6356.5838 km.\" # The projection used by SatPy expresses height as height above the Earth's surface (not distance # to the centre of the Earth). projection = { 'proj' : 'geos' , 'lon_0' : 9.5 , 'a' : 6378169.0 , 'b' : 6356583.8 , 'h' : 35785831.00 , 'units' : 'm' } seviri = AreaDefinition ( area_id = 'seviri' , description = 'SEVIRI RSS HRV' , proj_id = 'seviri' , projection = projection , width = num_x_pixels , height = num_y_pixels , area_extent = [ - 2768872.0236 + x_offset , # left 1394687.3495 , # bottom (from scene['HRV'].area) 2799876.1893 + x_offset , # right 5570248.4773 ] # top (from scene['HRV'].area) ) return seviri Then we'll use it to construct the relevant one for Seviri seviri = get_seviri_area_def ( native_fp ) seviri_crs = seviri . to_cartopy_crs () seviri_crs C:\\Users\\Ayrto\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\pyproj\\crs\\crs.py:543: UserWarning: You will likely lose important projection information when converting to a PROJ string from another format. See: https://proj.org/faq.html#what-is-the-best-format-for-describing-coordinate-reference-systems proj_string = self.to_proj4() 2020-12-16T20:22:49.021885 image/svg+xml Matplotlib v3.3.2, https://matplotlib.org/ *{stroke-linecap:butt;stroke-linejoin:round;} _PROJ4Projection(+ellps=WGS84 +a=6378169.0 +rf=295.488065897001 +h=35785831.0 +lon_0=9.5 +no_defs=True +proj=geos +type=crs +units=m +x_0=0.0 +y_0=0.0 +no_defs) We'll create a loader function that will extract the relevant data for lower_east_column_planned automatically #exports def load_scene ( native_fp ): # Reading scene and loading HRV scene = Scene ( filenames = [ native_fp ], reader = 'seviri_l1b_native' ) # Identifying and recording lower_east_column_planned handler = seviri_l1b_native . NativeMSGFileHandler ( native_fp , {}, None ) scene . attrs [ 'lower_east_column_planned' ] = handler . header [ '15_DATA_HEADER' ][ 'ImageDescription' ][ 'PlannedCoverageHRV' ][ 'LowerEastColumnPlanned' ] return scene We'll see how quickly this loads %% time scene = load_scene ( native_fp ) scene . load ([ 'HRV' ]) Wall time: 464 ms C:\\Users\\Ayrto\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\pyproj\\crs\\crs.py:543: UserWarning: You will likely lose important projection information when converting to a PROJ string from another format. See: https://proj.org/faq.html#what-is-the-best-format-for-describing-coordinate-reference-systems proj_string = self.to_proj4() We can visualise what a specific band looks like fig = plt . figure ( dpi = 250 , figsize = ( 10 , 10 )) ax = plt . axes ( projection = seviri_crs ) scene [ 'HRV' ] . plot . imshow ( ax = ax , add_colorbar = False , cmap = 'magma' , vmin = 0 , vmax = 50 ) ax . set_title ( '' ) ax . coastlines ( resolution = '50m' , alpha = 0.8 , color = 'white' ) <cartopy.mpl.feature_artist.FeatureArtist at 0x1912aa1d1f0> One of the benefits of having access to the underlying XArray object is that we can more easily start to do some analysis with the data, for example defining a reflectance threshold reflectance_threshold = 35 cmap = colors . ListedColormap ([ ( 0 , 0 , 0 , 0 ), # transparent ( 251 / 255 , 242 / 255 , 180 / 255 , 1 ) # yellow # (0.533, 0.808, 0.922, 1) # grey-like blue ]) # Plotting fig = plt . figure ( dpi = 250 , figsize = ( 10 , 10 )) ax = plt . axes ( projection = seviri_crs ) scene [ 'HRV' ] . plot . imshow ( ax = ax , vmin = 0 , vmax = 50 , cmap = 'magma' , add_colorbar = False ) ( scene [ 'HRV' ] > reflectance_threshold ) . plot . imshow ( ax = ax , cmap = cmap , add_colorbar = False ) ax . set_title ( '' ) ax . coastlines ( resolution = '50m' , alpha = 0.8 , color = 'white' ) <cartopy.mpl.feature_artist.FeatureArtist at 0x1912ac2b430> We'll extract the values from the XArray object, then mask all NaN values to enable us to carry out statistical analysis HRV = scene [ \"HRV\" ] . values HRV_masked = ma . masked_array ( HRV , mask = xr . ufuncs . isnan ( scene [ \"HRV\" ]) . values ) np . mean ( HRV_masked ) 12.372444717553362 We can also visualise the full distribution. N.b. to reduce the time it takes to calculate the best KDE fit we'll take only a sample of the data. HRV_sample = np . random . choice ( HRV_masked . flatten (), 1_000_000 ) # Plotting fig , ax = plt . subplots ( dpi = 250 ) sns . kdeplot ( HRV_sample , ax = ax , fill = True ) ax . set_yticks ([]) ax . set_ylabel ( '' ) ax . set_xlabel ( 'HRV Reflectance' ) hlp . hide_spines ( ax , positions = [ 'top' , 'left' , 'right' ]) Evaluating Reprojection to Tranverse Mercator \u00b6 Before we can resample we need to define the area we're resampling to, we'll write a constructor to help us do this #exports def construct_area_def ( scene , area_id , description , proj_id , projection , west , south , east , north , pixel_size = None ): # If None then will use same number of x and y points # HRV's resolution will be more like 4km for Europe if pixel_size is not None : width = int (( east - west ) / pixel_size ) height = int (( north - south ) / pixel_size ) else : width = scene [ list ( scene . keys ())[ 0 ][ 'name' ]] . x . values . shape [ 0 ] height = scene [ list ( scene . keys ())[ 0 ][ 'name' ]] . y . values . shape [ 0 ] area_extent = ( west , south , east , north ) area_def = AreaDefinition ( area_id , description , proj_id , projection , width , height , area_extent ) return area_def def construct_TM_area_def ( scene ): meters_per_pixel = 4000 west , south , east , north = ( - 3090000 , 1690000 , 4390000 , 9014000 ) area_id = 'TM' description = 'Transverse Mercator' proj_id = 'TM' projection = { 'ellps' : 'WGS84' , 'proj' : 'tmerc' , # Transverse Mercator 'units' : 'm' # meters } tm_area_def = construct_area_def ( scene , area_id , description , proj_id , projection , west , south , east , north , meters_per_pixel ) return tm_area_def tm_area_def = construct_TM_area_def ( scene ) tm_area_def . to_cartopy_crs () C:\\Users\\Ayrto\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\pyproj\\crs\\crs.py:543: UserWarning: You will likely lose important projection information when converting to a PROJ string from another format. See: https://proj.org/faq.html#what-is-the-best-format-for-describing-coordinate-reference-systems proj_string = self.to_proj4() 2020-12-16T20:23:04.493284 image/svg+xml Matplotlib v3.3.2, https://matplotlib.org/ *{stroke-linecap:butt;stroke-linejoin:round;} _PROJ4Projection(+ellps=WGS84 +k=1.0 +lat_0=0.0 +lon_0=0.0 +no_defs=True +proj=tmerc +type=crs +units=m +x_0=0.0 +y_0=0.0 +no_defs) We can now carry out the resampling using the pyresample library %% time resampled_scene = scene . resample ( tm_area_def , resampler = 'nearest' ) C:\\Users\\Ayrto\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\pyresample\\spherical.py:123: RuntimeWarning: invalid value encountered in true_divide self.cart /= np.sqrt(np.einsum('...i, ...i', self.cart, self.cart)) C:\\Users\\Ayrto\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\pyresample\\spherical.py:178: RuntimeWarning: invalid value encountered in double_scalars return (val + mod) % (2 * mod) - mod C:\\Users\\Ayrto\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\pyproj\\crs\\crs.py:543: UserWarning: You will likely lose important projection information when converting to a PROJ string from another format. See: https://proj.org/faq.html#what-is-the-best-format-for-describing-coordinate-reference-systems proj_string = self.to_proj4() C:\\Users\\Ayrto\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\numpy\\lib\\function_base.py:1280: RuntimeWarning: invalid value encountered in subtract a = op(a[slice1], a[slice2]) Wall time: 2.59 s We'll quickly check that the reprojection looks ok fig = plt . figure ( dpi = 250 , figsize = ( 10 , 10 )) ax = plt . axes ( projection = ccrs . TransverseMercator ()) resampled_scene [ 'HRV' ] . plot . imshow ( ax = ax ) ax . coastlines ( resolution = '50m' , alpha = 0.8 , color = 'white' ) <ipython-input-20-ec0e500c536a>:2: UserWarning: The default value for the *approx* keyword argument to TransverseMercator will change from True to False after 0.18. ax = plt.axes(projection=ccrs.TransverseMercator()) C:\\Users\\Ayrto\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\dask\\core.py:121: RuntimeWarning: invalid value encountered in sin return func(*(_execute_task(a, cache) for a in args)) C:\\Users\\Ayrto\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\dask\\core.py:121: RuntimeWarning: invalid value encountered in cos return func(*(_execute_task(a, cache) for a in args)) <cartopy.mpl.feature_artist.FeatureArtist at 0x1912bf95190> We want to gain a deeper understanding of the reprojection that's being carried out, to do this we'll manually reproject a sample of the original gridded coordinates %% time orig_x_values = scene [ 'HRV' ] . x . values [:: 50 ] orig_y_values = scene [ 'HRV' ] . y . values [:: 50 ] XX , YY = np . meshgrid ( orig_x_values , orig_y_values ) df_proj_points = ( gpd . GeoSeries ([ Point ( x , y ) for x , y in np . stack ([ XX . flatten (), YY . flatten ()], axis = 1 ) ]) . set_crs ( crs = scene [ 'HRV' ] . area . crs_wkt ) . to_crs ( crs = resampled_scene [ 'HRV' ] . area . crs_wkt ) . apply ( lambda point : pd . Series ( list ( point . coords )[ 0 ])) . rename ( columns = { 0 : 'x_reproj' , 1 : 'y_reproj' }) . replace ( np . inf , np . nan ) . pipe ( lambda df : df . assign ( x_orig = XX . flatten ())) . pipe ( lambda df : df . assign ( y_orig = YY . flatten ())) ) df_proj_points . head () Wall time: 2.77 s .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } x_reproj y_reproj x_orig y_orig 0 4.863405e+06 1.917787e+06 3.164425e+06 1.395187e+06 1 4.781323e+06 1.899419e+06 3.114418e+06 1.395187e+06 2 4.700620e+06 1.881746e+06 3.064412e+06 1.395187e+06 3 4.621232e+06 1.864731e+06 3.014405e+06 1.395187e+06 4 4.543102e+06 1.848341e+06 2.964398e+06 1.395187e+06 We can then visualise the reprojection of the original grid against the regridded reprojection %% time fig = plt . figure ( dpi = 250 , figsize = ( 10 , 10 )) ax = plt . axes ( projection = ccrs . TransverseMercator ()) resampled_scene [ 'HRV' ] . plot . imshow ( ax = ax , cmap = 'Greys_r' ) ax . coastlines ( resolution = '50m' , alpha = 0.8 , color = 'white' ) ax . scatter ( df_proj_points [ 'x_reproj' ][:: 10 ], df_proj_points [ 'y_reproj' ][:: 10 ], s = 2 , color = 'red' ) <timed exec>:2: UserWarning: The default value for the *approx* keyword argument to TransverseMercator will change from True to False after 0.18. C:\\Users\\Ayrto\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\dask\\core.py:121: RuntimeWarning: invalid value encountered in sin return func(*(_execute_task(a, cache) for a in args)) C:\\Users\\Ayrto\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\dask\\core.py:121: RuntimeWarning: invalid value encountered in cos return func(*(_execute_task(a, cache) for a in args)) Wall time: 30.7 s <matplotlib.collections.PathCollection at 0x1910024cd00> This is useful for quick visual inspection, for example we can see that the y axis gets stretched further the nearer to the pole. However, we want to get a better understanding of how the local cell resolution is changing for any given point, we'll begin by looking at this change for Greenwich. def lon_lat_to_new_crs ( lon , lat , crs ): x , y = list ( gpd . GeoSeries ([ Point ( lon , lat )]) . set_crs ( 4326 ) . to_crs ( crs ) . iloc [ 0 ] . coords )[ 0 ] return x , y def calc_res_change ( src_x , src_y , src_da , dst_da , src_dx = 10 , src_dy = 10 ): src_crs = src_da . area . crs_wkt dst_crs = dst_da . area . crs_wkt src_x_width = np . abs ( np . diff ( src_da . x . values )[ 0 ]) src_y_width = np . abs ( np . diff ( src_da . y . values )[ 0 ]) dst_x_width = np . abs ( np . diff ( dst_da . x . values )[ 0 ]) dst_y_width = np . abs ( np . diff ( dst_da . y . values )[ 0 ]) s_points = ( gpd . GeoSeries ([ Point ( src_x , src_y ), Point ( src_x + src_dx , src_y ), Point ( src_x , src_y + src_dy ) ]) . set_crs ( src_crs ) . to_crs ( dst_crs ) ) dst_dx = s_points . iloc [ 0 ] . distance ( s_points . iloc [ 1 ]) dst_dy = s_points . iloc [ 0 ] . distance ( s_points . iloc [ 2 ]) x_ratio_change = ( dst_dx / dst_x_width ) / ( src_dx / src_x_width ) y_ratio_change = ( dst_dy / dst_y_width ) / ( src_dy / src_y_width ) return x_ratio_change , y_ratio_change lon = 0 lat = 51.4934 src_x , src_y = lon_lat_to_new_crs ( lon , lat , scene [ 'HRV' ] . area . crs_wkt ) x_ratio_change , y_ratio_change = calc_res_change ( src_x , src_y , scene [ 'HRV' ], resampled_scene [ 'HRV' ]) x_ratio_change , y_ratio_change (0.27381567467569573, 0.528776076616483) We'll double check this by calculating it through a different method, in this case by locating the nearest cell for each scene and comparing their sizes in a common coordinate system def get_da_nearest_cell_width_height ( da , x , y , units_crs ): nearest_loc = da . sel ( x = x , y = y , method = 'nearest' ) nearest_x = nearest_loc . x . values nearest_y = nearest_loc . y . values next_nearest_x = da . x . values [ list ( da . x . values ) . index ( nearest_x ) + 1 ] next_nearest_y = da . y . values [ list ( da . y . values ) . index ( nearest_y ) + 1 ] s_points = ( gpd . GeoSeries ([ Point ( nearest_x , nearest_y ), Point ( next_nearest_x , nearest_y ), Point ( nearest_x , next_nearest_y ) ]) . set_crs ( da . area . crs_wkt ) . to_crs ( units_crs ) ) x_width = s_points . iloc [ 0 ] . distance ( s_points . iloc [ 1 ]) y_height = s_points . iloc [ 0 ] . distance ( s_points . iloc [ 2 ]) return x_width , y_height src_x , src_y = lon_lat_to_new_crs ( lon , lat , scene [ 'HRV' ] . area . crs_wkt ) dst_x , dst_y = lon_lat_to_new_crs ( lon , lat , resampled_scene [ 'HRV' ] . area . crs_wkt ) src_x_width , src_y_height = get_da_nearest_cell_width_height ( scene [ 'HRV' ], src_x , src_y , 27700 ) dst_x_width , dst_y_height = get_da_nearest_cell_width_height ( resampled_scene [ 'HRV' ], dst_x , dst_y , 27700 ) print ( f 'The width has changed from { round ( src_x_width / 1000 , 2 ) } km to { round ( dst_x_width / 1000 , 2 ) } km' ) print ( f 'The height has changed from { round ( src_y_height / 1000 , 2 ) } km to { round ( dst_y_height / 1000 , 2 ) } km' ) The width has changed from 1.09 km to 4.0 km The height has changed from 2.12 km to 4.0 km This can easily be converted into a x and y pixel size ratio change which almost exactly matches our previous calculation. The first calculation is more accurate as the dx and dy can approach 0 and get closer to the true ratio change, however the get_da_nearest_cell_width_height function is still useful as it allows us to determine the cell width and height in more interpretable units x_ratio_change , y_ratio_change = src_x_width / dst_x_width , src_y_height / dst_y_height x_ratio_change , y_ratio_change (0.2738180115545141, 0.5290020702784486) Iceland is stretched further still def print_pixel_change ( lon , lat , da_src , da_dst ): src_x , src_y = lon_lat_to_new_crs ( lon , lat , da_src . area . crs_wkt ) dst_x , dst_y = lon_lat_to_new_crs ( lon , lat , da_dst . area . crs_wkt ) src_x_width , src_y_height = get_da_nearest_cell_width_height ( da_src , src_x , src_y , 27700 ) dst_x_width , dst_y_height = get_da_nearest_cell_width_height ( da_dst , dst_x , dst_y , 27700 ) print ( f 'The width has changed from { round ( src_x_width / 1000 , 2 ) } km to { round ( dst_x_width / 1000 , 2 ) } km' ) print ( f 'The height has changed from { round ( src_y_height / 1000 , 2 ) } km to { round ( dst_y_height / 1000 , 2 ) } km' ) return lon = - 18.779208 lat = 64.887370 print_pixel_change ( lon , lat , scene [ 'HRV' ], resampled_scene [ 'HRV' ]) The width has changed from 1.52 km to 3.99 km The height has changed from 4.75 km to 3.99 km And contrasts with Marrakesh which is stretched less than Greenwich in the y axis lon = - 8.005657 lat = 31.636355 print_pixel_change ( lon , lat , scene [ 'HRV' ], resampled_scene [ 'HRV' ]) The width has changed from 1.11 km to 3.99 km The height has changed from 1.33 km to 3.99 km We can check what the cell height and width are at the center of the image, they should both be close to 1km according to the SEVIRI documentation LineDirGridStep gives the grid step size in km SSP in the line direction. Default value is 3km for VIS and IR, and 1km for HRV. The on-ground grid step size of 3 km at the SSP represents an instrument scan step of 251.53 microrad divided by 3. - EUMETSAT round_m_to_km = lambda m : round ( m / 1000 , 2 ) UTM_35N_epsg = 32632 # should be relatively accurate and is in meters src_x = np . median ( scene [ 'HRV' ] . x . values ) src_y = np . median ( scene [ 'HRV' ] . y . values ) src_x_width , src_y_height = get_da_nearest_cell_width_height ( scene [ 'HRV' ], src_x , src_y , UTM_35N_epsg ) round_m_to_km ( src_x_width ), round_m_to_km ( src_y_height ) (1.04, 1.36) Comparing Reprojection Libraries \u00b6 In the last section we used pyresample to carry out the data reprojection, here we'll explore pyinterp . Before we start we'll quickly extract the xarrays for the original and reprojected coordinates. def extract_formatted_scene ( scene , variable = 'HRV' , x_coords_name = 'x' , y_coords_name = 'y' , x_units = 'metre' , y_units = 'metre' ): da = ( scene [ variable ] . copy () . rename ({ 'x' : x_coords_name , 'y' : y_coords_name }) ) da [ x_coords_name ] . attrs [ 'units' ] = x_units da [ y_coords_name ] . attrs [ 'units' ] = y_units return da da = extract_formatted_scene ( scene ) da_resampled = extract_formatted_scene ( resampled_scene ) da_resampled /* CSS stylesheet for displaying xarray objects in jupyterlab. * */ :root { --xr-font-color0: var(--jp-content-font-color0, rgba(0, 0, 0, 1)); --xr-font-color2: var(--jp-content-font-color2, rgba(0, 0, 0, 0.54)); --xr-font-color3: var(--jp-content-font-color3, rgba(0, 0, 0, 0.38)); --xr-border-color: var(--jp-border-color2, #e0e0e0); --xr-disabled-color: var(--jp-layout-color3, #bdbdbd); --xr-background-color: var(--jp-layout-color0, white); --xr-background-color-row-even: var(--jp-layout-color1, white); --xr-background-color-row-odd: var(--jp-layout-color2, #eeeeee); } html[theme=dark], body.vscode-dark { --xr-font-color0: rgba(255, 255, 255, 1); --xr-font-color2: rgba(255, 255, 255, 0.54); --xr-font-color3: rgba(255, 255, 255, 0.38); --xr-border-color: #1F1F1F; --xr-disabled-color: #515151; --xr-background-color: #111111; --xr-background-color-row-even: #111111; --xr-background-color-row-odd: #313131; } .xr-wrap { display: block; min-width: 300px; max-width: 700px; } .xr-text-repr-fallback { /* fallback to plain text repr when CSS is not injected (untrusted notebook) */ display: none; } .xr-header { padding-top: 6px; padding-bottom: 6px; margin-bottom: 4px; border-bottom: solid 1px var(--xr-border-color); } .xr-header > div, .xr-header > ul { display: inline; margin-top: 0; margin-bottom: 0; } .xr-obj-type, .xr-array-name { margin-left: 2px; margin-right: 10px; } .xr-obj-type { color: var(--xr-font-color2); } .xr-sections { padding-left: 0 !important; display: grid; grid-template-columns: 150px auto auto 1fr 20px 20px; } .xr-section-item { display: contents; } .xr-section-item input { display: none; } .xr-section-item input + label { color: var(--xr-disabled-color); } .xr-section-item input:enabled + label { cursor: pointer; color: var(--xr-font-color2); } .xr-section-item input:enabled + label:hover { color: var(--xr-font-color0); } .xr-section-summary { grid-column: 1; color: var(--xr-font-color2); font-weight: 500; } .xr-section-summary > span { display: inline-block; padding-left: 0.5em; } .xr-section-summary-in:disabled + label { color: var(--xr-font-color2); } .xr-section-summary-in + label:before { display: inline-block; content: '\u00e2\u2013\u00ba'; font-size: 11px; width: 15px; text-align: center; } .xr-section-summary-in:disabled + label:before { color: var(--xr-disabled-color); } .xr-section-summary-in:checked + label:before { content: '\u00e2\u2013\u00bc'; } .xr-section-summary-in:checked + label > span { display: none; } .xr-section-summary, .xr-section-inline-details { padding-top: 4px; padding-bottom: 4px; } .xr-section-inline-details { grid-column: 2 / -1; } .xr-section-details { display: none; grid-column: 1 / -1; margin-bottom: 5px; } .xr-section-summary-in:checked ~ .xr-section-details { display: contents; } .xr-array-wrap { grid-column: 1 / -1; display: grid; grid-template-columns: 20px auto; } .xr-array-wrap > label { grid-column: 1; vertical-align: top; } .xr-preview { color: var(--xr-font-color3); } .xr-array-preview, .xr-array-data { padding: 0 5px !important; grid-column: 2; } .xr-array-data, .xr-array-in:checked ~ .xr-array-preview { display: none; } .xr-array-in:checked ~ .xr-array-data, .xr-array-preview { display: inline-block; } .xr-dim-list { display: inline-block !important; list-style: none; padding: 0 !important; margin: 0; } .xr-dim-list li { display: inline-block; padding: 0; margin: 0; } .xr-dim-list:before { content: '('; } .xr-dim-list:after { content: ')'; } .xr-dim-list li:not(:last-child):after { content: ','; padding-right: 5px; } .xr-has-index { font-weight: bold; } .xr-var-list, .xr-var-item { display: contents; } .xr-var-item > div, .xr-var-item label, .xr-var-item > .xr-var-name span { background-color: var(--xr-background-color-row-even); margin-bottom: 0; } .xr-var-item > .xr-var-name:hover span { padding-right: 5px; } .xr-var-list > li:nth-child(odd) > div, .xr-var-list > li:nth-child(odd) > label, .xr-var-list > li:nth-child(odd) > .xr-var-name span { background-color: var(--xr-background-color-row-odd); } .xr-var-name { grid-column: 1; } .xr-var-dims { grid-column: 2; } .xr-var-dtype { grid-column: 3; text-align: right; color: var(--xr-font-color2); } .xr-var-preview { grid-column: 4; } .xr-var-name, .xr-var-dims, .xr-var-dtype, .xr-preview, .xr-attrs dt { white-space: nowrap; overflow: hidden; text-overflow: ellipsis; padding-right: 10px; } .xr-var-name:hover, .xr-var-dims:hover, .xr-var-dtype:hover, .xr-attrs dt:hover { overflow: visible; width: auto; z-index: 1; } .xr-var-attrs, .xr-var-data { display: none; background-color: var(--xr-background-color) !important; padding-bottom: 5px !important; } .xr-var-attrs-in:checked ~ .xr-var-attrs, .xr-var-data-in:checked ~ .xr-var-data { display: block; } .xr-var-data > table { float: right; } .xr-var-name span, .xr-var-data, .xr-attrs { padding-left: 25px !important; } .xr-attrs, .xr-var-attrs, .xr-var-data { grid-column: 1 / -1; } dl.xr-attrs { padding: 0; margin: 0; display: grid; grid-template-columns: 125px auto; } .xr-attrs dt, .xr-attrs dd { padding: 0; margin: 0; float: left; padding-right: 10px; width: auto; } .xr-attrs dt { font-weight: normal; grid-column: 1; } .xr-attrs dt:hover span { display: inline-block; background: var(--xr-background-color); padding-right: 10px; } .xr-attrs dd { grid-column: 2; white-space: pre-wrap; word-break: break-all; } .xr-icon-database, .xr-icon-file-text2 { display: inline-block; vertical-align: middle; width: 1em; height: 1.5em !important; stroke-width: 0; stroke: currentColor; fill: currentColor; } <xarray.DataArray 'my_index-25c95e08ed138cbd282b6596ed55c066' (y: 1831, x: 1870)> dask.array<copy, shape=(1831, 1870), dtype=float32, chunksize=(1831, 1870), chunktype=numpy.ndarray> Coordinates: crs object PROJCRS[\"unknown\",BASEGEOGCRS[\"unknown\",DATUM[\"Unknown ba... * y (y) float64 9.012e+06 9.008e+06 9.004e+06 ... 1.696e+06 1.692e+06 * x (x) float64 -3.088e+06 -3.084e+06 -3.08e+06 ... 4.384e+06 4.388e+06 Attributes: orbital_parameters: {'projection_longitude': 9.5, 'pr... sun_earth_distance_correction_applied: True sun_earth_distance_correction_factor: 0.9697642568677852 units: % wavelength: 0.7\u00e2\u20ac\u00af\u00c2\u00b5m\u00c2 (0.5-0.9\u00e2\u20ac\u00af\u00c2\u00b5m) standard_name: toa_bidirectional_reflectance platform_name: Meteosat-9 sensor: seviri start_time: 2020-12-08 09:00:08.206321 end_time: 2020-12-08 09:05:08.329479 area: Area ID: TM\\nDescription: Transve... name: HRV resolution: 1000.134348869 calibration: reflectance modifiers: () _satpy_id: DataID(name='HRV', wavelength=Wav... ancillary_variables: [] xarray.DataArray 'my_index-25c95e08ed138cbd282b6596ed55c066' y : 1831 x : 1870 dask.array<chunksize=(1831, 1870), meta=np.ndarray> Array Chunk Bytes 13.70 MB 13.70 MB Shape (1831, 1870) (1831, 1870) Count 360 Tasks 1 Chunks Type float32 numpy.ndarray 1870 1831 Coordinates: (3) crs () object PROJCRS[\"unknown\",BASEGEOGCRS[\"u... array(<Projected CRS: PROJCRS[\"unknown\",BASEGEOGCRS[\"unknown\",DATUM[\"Unk ...> Name: unknown Axis Info [cartesian]: - E[east]: Easting (metre) - N[north]: Northing (metre) Area of Use: - undefined Coordinate Operation: - name: unknown - method: Transverse Mercator Datum: Unknown based on WGS84 ellipsoid - Ellipsoid: WGS 84 - Prime Meridian: Greenwich , dtype=object) y (y) float64 9.012e+06 9.008e+06 ... 1.692e+06 units : metre array([9012000., 9008000., 9004000., ..., 1700000., 1696000., 1692000.]) x (x) float64 -3.088e+06 -3.084e+06 ... 4.388e+06 units : metre array([-3088000., -3084000., -3080000., ..., 4380000., 4384000., 4388000.]) Attributes: (17) orbital_parameters : {'projection_longitude': 9.5, 'projection_latitude': 0.0, 'projection_altitude': 35785831.0} sun_earth_distance_correction_applied : True sun_earth_distance_correction_factor : 0.9697642568677852 units : % wavelength : 0.7\u00e2\u20ac\u00af\u00c2\u00b5m\u00c2 (0.5-0.9\u00e2\u20ac\u00af\u00c2\u00b5m) standard_name : toa_bidirectional_reflectance platform_name : Meteosat-9 sensor : seviri start_time : 2020-12-08 09:00:08.206321 end_time : 2020-12-08 09:05:08.329479 area : Area ID: TM Description: Transverse Mercator Projection ID: TM Projection: {'ellps': 'WGS84', 'k': '1', 'lat_0': '0', 'lon_0': '0', 'no_defs': 'None', 'proj': 'tmerc', 'type': 'crs', 'units': 'm', 'x_0': '0', 'y_0': '0'} Number of columns: 1870 Number of rows: 1831 Area extent: (-3090000, 1690000, 4390000, 9014000) name : HRV resolution : 1000.134348869 calibration : reflectance modifiers : () _satpy_id : DataID(name='HRV', wavelength=WavelengthRange(min=0.5, central=0.7, max=0.9, unit='\u00c2\u00b5m'), resolution=1000.134348869, calibration=<calibration.reflectance>, modifiers=()) ancillary_variables : [] We'll now save the coordinates of the grid we're using in the new projection new_grid_4km_TM = { 'x_coords' : list ( da_resampled . x . values ), 'y_coords' : list ( da_resampled . y . values ) } save_data = True if save_data == True : with open ( '../data/intermediate/new_grid_4km_TM.json' , 'w' ) as fp : json . dump ( new_grid_4km_TM , fp ) JSON ( new_grid_4km_TM ) <IPython.core.display.JSON object> As well as calculate the locations of those points in the original CRS %% time def chunks ( list_ , n ): \"\"\" Yield successive n-sized chunks from `list_`. \"\"\" for i in range ( 0 , len ( list_ ), n ): yield list_ [ i : i + n ] def reproject_geometries ( da , old_crs , new_crs , chunk_size = 5000 ): xx , yy = np . meshgrid ( da . x . values , da . y . values , indexing = 'ij' ) geometry = gpd . points_from_xy ( xx . flatten (), yy . flatten ()) new_coords_samples = [] for geometry_sample in chunks ( geometry , chunk_size ): df_new_coords_sample = ( gpd . GeoSeries ( geometry_sample , crs = old_crs ) . to_crs ( new_crs ) . apply ( lambda x : list ( x . coords [ 0 ])) . apply ( pd . Series ) . rename ( columns = { 0 : 'x' , 1 : 'y' }) ) new_coords_samples += [ df_new_coords_sample ] df_new_coords = pd . concat ( new_coords_samples , ignore_index = True ) return df_new_coords if not os . path . exists ( intermediate_data_dir ): os . makedirs ( intermediate_data_dir ) if calculate_reproj_coords == True : df_new_coords = reproject_geometries ( da_resampled , '+proj=tmerc' , seviri_crs . proj4_init ) df_new_coords . to_csv ( f ' { intermediate_data_dir } /reproj_coords_TM_4km.csv' , index = False ) elif 'reproj_coords.csv' not in os . listdir ( intermediate_data_dir ): df_new_coords = pd . read_csv ( 'https://storage.googleapis.com/reprojection_cache/reproj_coords_TM_4km.csv' ) else : df_new_coords = pd . read_csv ( f ' { intermediate_data_dir } /reproj_coords_TM_4km.csv' ) df_new_coords . head () Wall time: 1.51 s .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } x y 0 inf inf 1 inf inf 2 inf inf 3 inf inf 4 inf inf We can layer these on top of each other to get an alternative view of the transform operation %% time old_x_positions , old_y_positions = [ elem . flatten () for elem in np . meshgrid ( da . x . values [:: 100 ], da . y . values [:: 100 ], indexing = 'ij' )] new_x_positions , new_y_positions = df_new_coords [ 'x' ][:: 100 ], df_new_coords [ 'y' ][:: 100 ] # Plotting fig , ax = plt . subplots ( dpi = 150 ) ax . scatter ( old_x_positions , old_y_positions , s = 0.1 ) ax . scatter ( new_x_positions , new_y_positions , s = 0.1 ) hlp . hide_spines ( ax ) Wall time: 55.9 ms We'll now use pyinterp to take these and use them to carry out the resampling. We'll also create a wrapper for converting the result back into an Xarray object. #exports def reproj_with_manual_grid ( da , x_coords , y_coords , new_grid ): x_axis = pyinterp . Axis ( da . x . values ) y_axis = pyinterp . Axis ( da . y . values ) grid = pyinterp . Grid2D ( x_axis , y_axis , da . data . T ) reproj_data = ( pyinterp . bivariate ( grid , x_coords , y_coords ) . reshape (( len ( new_grid [ 'x_coords' ]), len ( new_grid [ 'y_coords' ]))) ) return reproj_data def reproj_to_xarray ( da , x_coords , y_coords , new_grid ): # We'll reproject the data reproj_data = reproj_with_manual_grid ( da , x_coords , y_coords , new_grid ) # Then put it in an XArray DataArray da_reproj = xr . DataArray ( np . flip ( reproj_data . T , axis = ( 0 , 1 )), dims = ( 'y' , 'x' ), coords = { 'x' : new_grid [ 'x_coords' ][:: - 1 ], 'y' : new_grid [ 'y_coords' ][:: - 1 ] }, attrs = da . attrs ) return da_reproj We'll load the grid back in with open ( '../data/intermediate/new_grid_4km_TM.json' , 'r' ) as fp : new_grid = json . load ( fp ) JSON ( new_grid ) <IPython.core.display.JSON object> Confirm that the size of the grid definition arrays match the number of coordinates we have df_new_coords [ 'y' ] . size == len ( new_grid [ 'x_coords' ]) * len ( new_grid [ 'y_coords' ]) True And finally carry out the reprojection %% timeit da_reproj = reproj_to_xarray ( da , df_new_coords [ 'x' ], df_new_coords [ 'y' ], new_grid ) 710 ms \u00c2\u00b1 39.4 ms per loop (mean \u00c2\u00b1 std. dev. of 7 runs, 1 loop each) Most importantly we'll carry out a visual check that the reprojection was carried out properly. da_reproj = reproj_to_xarray ( da , df_new_coords [ 'x' ], df_new_coords [ 'y' ], new_grid ) # Plotting fig = plt . figure ( dpi = 250 , figsize = ( 10 , 10 )) ax = plt . axes ( projection = ccrs . TransverseMercator ()) da_reproj . plot . imshow ( ax = ax , cmap = 'Greys_r' ) ax . coastlines ( resolution = '50m' , alpha = 0.8 , color = 'white' ) <ipython-input-37-c765a7c3ab68>:5: UserWarning: The default value for the *approx* keyword argument to TransverseMercator will change from True to False after 0.18. ax = plt.axes(projection=ccrs.TransverseMercator()) <cartopy.mpl.feature_artist.FeatureArtist at 0x1912ac7b040> #exports def full_scene_pyresample ( native_fp ): # Loading scene scene = load_scene ( native_fp ) dataset_names = scene . all_dataset_names () scene . load ( dataset_names ) # Constructing target area definition tm_area_def = construct_TM_area_def ( scene ) # Reprojecting reproj_vars = list () for dataset_name in dataset_names : da = scene [ dataset_name ] . sortby ( 'y' , ascending = False ) . sortby ( 'x' ) num_y_pixels , num_x_pixels = da . shape seviri_area_def = get_seviri_area_def ( native_fp , num_x_pixels = num_x_pixels , num_y_pixels = num_y_pixels ) resampler = satpy . resample . KDTreeResampler ( seviri_area_def , tm_area_def ) da_reproj = resampler . resample ( da ) reproj_vars += [ da_reproj ] variable_idx = pd . Index ( dataset_names , name = 'variable' ) ds_reproj = ( xr . concat ( reproj_vars , dim = variable_idx ) . to_dataset ( name = 'stacked_eumetsat_data' ) . drop ( labels = 'crs' ) ) return ds_reproj def full_scene_pyinterp ( native_fp , new_x_coords , new_y_coords , new_grid_fp ): # Loading data scene = load_scene ( native_fp ) dataset_names = scene . all_dataset_names () scene . load ( dataset_names ) with open ( new_grid_fp , 'r' ) as fp : new_grid = json . load ( fp ) # Correcting x coordinates seviri_area_def = get_seviri_area_def ( native_fp ) area_extent = seviri_area_def . area_extent x_offset = calculate_x_offset ( native_fp ) width = scene [ 'HRV' ] . x . size corrected_x_coords = np . linspace ( area_extent [ 2 ], area_extent [ 0 ], width ) scene [ 'HRV' ] = scene [ 'HRV' ] . assign_coords ({ 'x' : corrected_x_coords }) # Reprojecting reproj_vars = list () for dataset_name in dataset_names : da_reproj = reproj_to_xarray ( scene [ dataset_name ], new_x_coords , new_y_coords , new_grid ) reproj_vars += [ da_reproj ] variable_idx = pd . Index ( dataset_names , name = 'variable' ) ds_reproj = xr . concat ( reproj_vars , dim = variable_idx ) . to_dataset ( name = 'stacked_eumetsat_data' ) return ds_reproj class Reprojector : def __init__ ( self , new_coords_fp = None , new_grid_fp = None ): if new_coords_fp is None and new_grid_fp is None : return df_new_coords = pd . read_csv ( new_coords_fp ) self . new_x_coords = df_new_coords [ 'x' ] self . new_y_coords = df_new_coords [ 'y' ] self . new_grid_fp = new_grid_fp return def reproject ( self , native_fp , reproj_library = 'pyresample' ): if reproj_library == 'pyinterp' : ds_reproj = full_scene_pyinterp ( native_fp , self . new_x_coords , self . new_y_coords , self . new_grid_fp ) elif reproj_library == 'pyresample' : ds_reproj = full_scene_pyresample ( native_fp ) else : raise ValueError ( f '`reproj_library` must be one of: pyresample, pyinterp. { reproj_library } can not be passed.' ) return ds_reproj %% capture -- no - stdout %% timeit new_coords_fp = f ' { intermediate_data_dir } /reproj_coords_TM_4km.csv' new_grid_fp = '../data/intermediate/new_grid_4km_TM.json' reprojector = Reprojector ( new_coords_fp , new_grid_fp ) ds_reproj = reprojector . reproject ( native_fp , reproj_library = 'pyinterp' ) 6.06 s \u00c2\u00b1 528 ms per loop (mean \u00c2\u00b1 std. dev. of 7 runs, 1 loop each) %% capture -- no - stdout %% timeit reprojector = Reprojector () ds_reproj = reprojector . reproject ( native_fp , reproj_library = 'pyresample' ) 4.9 s \u00c2\u00b1 590 ms per loop (mean \u00c2\u00b1 std. dev. of 7 runs, 1 loop each) ds_reproj = reprojector . reproject ( native_fp ) # Plotting fig = plt . figure ( dpi = 250 , figsize = ( 10 , 10 )) ax = plt . axes ( projection = ccrs . TransverseMercator ()) ds_reproj [ 'stacked_eumetsat_data' ] . sel ( variable = 'HRV' ) . plot . imshow ( ax = ax , cmap = 'Greys_r' ) ax . coastlines ( resolution = '50m' , alpha = 0.8 , color = 'white' ) C:\\Users\\Ayrto\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\pyproj\\crs\\crs.py:543: UserWarning: You will likely lose important projection information when converting to a PROJ string from another format. See: https://proj.org/faq.html#what-is-the-best-format-for-describing-coordinate-reference-systems proj_string = self.to_proj4() C:\\Users\\Ayrto\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\pyproj\\crs\\crs.py:543: UserWarning: You will likely lose important projection information when converting to a PROJ string from another format. See: https://proj.org/faq.html#what-is-the-best-format-for-describing-coordinate-reference-systems proj_string = self.to_proj4() C:\\Users\\Ayrto\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\pyproj\\crs\\crs.py:543: UserWarning: You will likely lose important projection information when converting to a PROJ string from another format. See: https://proj.org/faq.html#what-is-the-best-format-for-describing-coordinate-reference-systems proj_string = self.to_proj4() C:\\Users\\Ayrto\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\pyproj\\crs\\crs.py:543: UserWarning: You will likely lose important projection information when converting to a PROJ string from another format. See: https://proj.org/faq.html#what-is-the-best-format-for-describing-coordinate-reference-systems proj_string = self.to_proj4() C:\\Users\\Ayrto\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\pyproj\\crs\\crs.py:543: UserWarning: You will likely lose important projection information when converting to a PROJ string from another format. See: https://proj.org/faq.html#what-is-the-best-format-for-describing-coordinate-reference-systems proj_string = self.to_proj4() C:\\Users\\Ayrto\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\pyproj\\crs\\crs.py:543: UserWarning: You will likely lose important projection information when converting to a PROJ string from another format. See: https://proj.org/faq.html#what-is-the-best-format-for-describing-coordinate-reference-systems proj_string = self.to_proj4() C:\\Users\\Ayrto\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\pyproj\\crs\\crs.py:543: UserWarning: You will likely lose important projection information when converting to a PROJ string from another format. See: https://proj.org/faq.html#what-is-the-best-format-for-describing-coordinate-reference-systems proj_string = self.to_proj4() C:\\Users\\Ayrto\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\pyproj\\crs\\crs.py:543: UserWarning: You will likely lose important projection information when converting to a PROJ string from another format. See: https://proj.org/faq.html#what-is-the-best-format-for-describing-coordinate-reference-systems proj_string = self.to_proj4() C:\\Users\\Ayrto\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\pyproj\\crs\\crs.py:543: UserWarning: You will likely lose important projection information when converting to a PROJ string from another format. See: https://proj.org/faq.html#what-is-the-best-format-for-describing-coordinate-reference-systems proj_string = self.to_proj4() C:\\Users\\Ayrto\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\pyproj\\crs\\crs.py:543: UserWarning: You will likely lose important projection information when converting to a PROJ string from another format. See: https://proj.org/faq.html#what-is-the-best-format-for-describing-coordinate-reference-systems proj_string = self.to_proj4() C:\\Users\\Ayrto\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\pyproj\\crs\\crs.py:543: UserWarning: You will likely lose important projection information when converting to a PROJ string from another format. See: https://proj.org/faq.html#what-is-the-best-format-for-describing-coordinate-reference-systems proj_string = self.to_proj4() C:\\Users\\Ayrto\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\pyproj\\crs\\crs.py:543: UserWarning: You will likely lose important projection information when converting to a PROJ string from another format. See: https://proj.org/faq.html#what-is-the-best-format-for-describing-coordinate-reference-systems proj_string = self.to_proj4() C:\\Users\\Ayrto\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\pyproj\\crs\\crs.py:543: UserWarning: You will likely lose important projection information when converting to a PROJ string from another format. See: https://proj.org/faq.html#what-is-the-best-format-for-describing-coordinate-reference-systems proj_string = self.to_proj4() C:\\Users\\Ayrto\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\numpy\\lib\\function_base.py:1280: RuntimeWarning: invalid value encountered in subtract a = op(a[slice1], a[slice2]) C:\\Users\\Ayrto\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\pyproj\\crs\\crs.py:543: UserWarning: You will likely lose important projection information when converting to a PROJ string from another format. See: https://proj.org/faq.html#what-is-the-best-format-for-describing-coordinate-reference-systems proj_string = self.to_proj4() C:\\Users\\Ayrto\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\pyproj\\crs\\crs.py:543: UserWarning: You will likely lose important projection information when converting to a PROJ string from another format. See: https://proj.org/faq.html#what-is-the-best-format-for-describing-coordinate-reference-systems proj_string = self.to_proj4() C:\\Users\\Ayrto\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\numpy\\lib\\function_base.py:1280: RuntimeWarning: invalid value encountered in subtract a = op(a[slice1], a[slice2]) C:\\Users\\Ayrto\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\pyproj\\crs\\crs.py:543: UserWarning: You will likely lose important projection information when converting to a PROJ string from another format. See: https://proj.org/faq.html#what-is-the-best-format-for-describing-coordinate-reference-systems proj_string = self.to_proj4() C:\\Users\\Ayrto\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\numpy\\lib\\function_base.py:1280: RuntimeWarning: invalid value encountered in subtract a = op(a[slice1], a[slice2]) C:\\Users\\Ayrto\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\pyproj\\crs\\crs.py:543: UserWarning: You will likely lose important projection information when converting to a PROJ string from another format. See: https://proj.org/faq.html#what-is-the-best-format-for-describing-coordinate-reference-systems proj_string = self.to_proj4() C:\\Users\\Ayrto\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\numpy\\lib\\function_base.py:1280: RuntimeWarning: invalid value encountered in subtract a = op(a[slice1], a[slice2]) C:\\Users\\Ayrto\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\pyproj\\crs\\crs.py:543: UserWarning: You will likely lose important projection information when converting to a PROJ string from another format. See: https://proj.org/faq.html#what-is-the-best-format-for-describing-coordinate-reference-systems proj_string = self.to_proj4() C:\\Users\\Ayrto\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\numpy\\lib\\function_base.py:1280: RuntimeWarning: invalid value encountered in subtract a = op(a[slice1], a[slice2]) C:\\Users\\Ayrto\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\pyproj\\crs\\crs.py:543: UserWarning: You will likely lose important projection information when converting to a PROJ string from another format. See: https://proj.org/faq.html#what-is-the-best-format-for-describing-coordinate-reference-systems proj_string = self.to_proj4() C:\\Users\\Ayrto\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\numpy\\lib\\function_base.py:1280: RuntimeWarning: invalid value encountered in subtract a = op(a[slice1], a[slice2]) C:\\Users\\Ayrto\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\pyproj\\crs\\crs.py:543: UserWarning: You will likely lose important projection information when converting to a PROJ string from another format. See: https://proj.org/faq.html#what-is-the-best-format-for-describing-coordinate-reference-systems proj_string = self.to_proj4() C:\\Users\\Ayrto\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\numpy\\lib\\function_base.py:1280: RuntimeWarning: invalid value encountered in subtract a = op(a[slice1], a[slice2]) C:\\Users\\Ayrto\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\pyproj\\crs\\crs.py:543: UserWarning: You will likely lose important projection information when converting to a PROJ string from another format. See: https://proj.org/faq.html#what-is-the-best-format-for-describing-coordinate-reference-systems proj_string = self.to_proj4() C:\\Users\\Ayrto\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\numpy\\lib\\function_base.py:1280: RuntimeWarning: invalid value encountered in subtract a = op(a[slice1], a[slice2]) C:\\Users\\Ayrto\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\pyproj\\crs\\crs.py:543: UserWarning: You will likely lose important projection information when converting to a PROJ string from another format. See: https://proj.org/faq.html#what-is-the-best-format-for-describing-coordinate-reference-systems proj_string = self.to_proj4() C:\\Users\\Ayrto\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\numpy\\lib\\function_base.py:1280: RuntimeWarning: invalid value encountered in subtract a = op(a[slice1], a[slice2]) C:\\Users\\Ayrto\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\pyproj\\crs\\crs.py:543: UserWarning: You will likely lose important projection information when converting to a PROJ string from another format. See: https://proj.org/faq.html#what-is-the-best-format-for-describing-coordinate-reference-systems proj_string = self.to_proj4() C:\\Users\\Ayrto\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\numpy\\lib\\function_base.py:1280: RuntimeWarning: invalid value encountered in subtract a = op(a[slice1], a[slice2]) C:\\Users\\Ayrto\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\pyproj\\crs\\crs.py:543: UserWarning: You will likely lose important projection information when converting to a PROJ string from another format. See: https://proj.org/faq.html#what-is-the-best-format-for-describing-coordinate-reference-systems proj_string = self.to_proj4() C:\\Users\\Ayrto\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\numpy\\lib\\function_base.py:1280: RuntimeWarning: invalid value encountered in subtract a = op(a[slice1], a[slice2]) C:\\Users\\Ayrto\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\pyproj\\crs\\crs.py:543: UserWarning: You will likely lose important projection information when converting to a PROJ string from another format. See: https://proj.org/faq.html#what-is-the-best-format-for-describing-coordinate-reference-systems proj_string = self.to_proj4() C:\\Users\\Ayrto\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\numpy\\lib\\function_base.py:1280: RuntimeWarning: invalid value encountered in subtract a = op(a[slice1], a[slice2]) <ipython-input-42-4aa2b08f07bf>:5: UserWarning: The default value for the *approx* keyword argument to TransverseMercator will change from True to False after 0.18. ax = plt.axes(projection=ccrs.TransverseMercator()) C:\\Users\\Ayrto\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\dask\\core.py:121: RuntimeWarning: invalid value encountered in sin return func(*(_execute_task(a, cache) for a in args)) C:\\Users\\Ayrto\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\dask\\core.py:121: RuntimeWarning: invalid value encountered in cos return func(*(_execute_task(a, cache) for a in args)) <cartopy.mpl.feature_artist.FeatureArtist at 0x19102ac7430>","title":"Reprojection"},{"location":"02_reproj/#data-transformation","text":"#exports import json import pandas as pd import xarray as xr import numpy as np import numpy.ma as ma import matplotlib as mpl import matplotlib.pyplot as plt from matplotlib import colors import seaborn as sns import os import time from itertools import product from collections import OrderedDict from datetime import datetime from ipypb import track import FEAutils as hlp import satpy from satpy import Scene from satpy.readers import seviri_l1b_native import pyresample from pyresample.geometry import AreaDefinition try : import pyinterp import pyinterp.backends.xarray except : pass We'll separately install libraries that wont be needed for the satip module import rasterio from rasterio import Affine as A from rasterio.warp import reproject , Resampling , calculate_default_transform , transform from rasterio.control import GroundControlPoint from rasterio.transform import xy import geopandas as gpd from shapely.geometry import Point import cartopy.crs as ccrs from IPython.display import JSON","title":"Data Transformation"},{"location":"02_reproj/#user-input","text":"data_dir = '../data/raw' intermediate_data_dir = '../data/intermediate' calculate_reproj_coords = False","title":"User Input"},{"location":"02_reproj/#exploratory-data-analysis","text":"We'll start by identifying the available files native_fps = sorted ([ f ' { data_dir } / { f } ' for f in os . listdir ( data_dir ) if '.nat' in f ]) native_fps [ 0 ] '../data/raw/MSG2-SEVI-MSG15-0100-NA-20201208090415.301000000Z-NA.nat' Then load one of them in as a SatPy scene native_fp = native_fps [ 0 ] scene = Scene ( filenames = [ native_fp ], reader = 'seviri_l1b_native' ) scene <satpy.scene.Scene at 0x1911d108580> We can get a list of the available datasets (bands) scene . all_dataset_names () ['HRV', 'IR_016', 'IR_039', 'IR_087', 'IR_097', 'IR_108', 'IR_120', 'IR_134', 'VIS006', 'VIS008', 'WV_062', 'WV_073'] Each band contains an XArray DataArray scene . load ([ 'HRV' ]) scene [ 'HRV' ] C:\\Users\\Ayrto\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\pyproj\\crs\\crs.py:543: UserWarning: You will likely lose important projection information when converting to a PROJ string from another format. See: https://proj.org/faq.html#what-is-the-best-format-for-describing-coordinate-reference-systems proj_string = self.to_proj4() /* CSS stylesheet for displaying xarray objects in jupyterlab. * */ :root { --xr-font-color0: var(--jp-content-font-color0, rgba(0, 0, 0, 1)); --xr-font-color2: var(--jp-content-font-color2, rgba(0, 0, 0, 0.54)); --xr-font-color3: var(--jp-content-font-color3, rgba(0, 0, 0, 0.38)); --xr-border-color: var(--jp-border-color2, #e0e0e0); --xr-disabled-color: var(--jp-layout-color3, #bdbdbd); --xr-background-color: var(--jp-layout-color0, white); --xr-background-color-row-even: var(--jp-layout-color1, white); --xr-background-color-row-odd: var(--jp-layout-color2, #eeeeee); } html[theme=dark], body.vscode-dark { --xr-font-color0: rgba(255, 255, 255, 1); --xr-font-color2: rgba(255, 255, 255, 0.54); --xr-font-color3: rgba(255, 255, 255, 0.38); --xr-border-color: #1F1F1F; --xr-disabled-color: #515151; --xr-background-color: #111111; --xr-background-color-row-even: #111111; --xr-background-color-row-odd: #313131; } .xr-wrap { display: block; min-width: 300px; max-width: 700px; } .xr-text-repr-fallback { /* fallback to plain text repr when CSS is not injected (untrusted notebook) */ display: none; } .xr-header { padding-top: 6px; padding-bottom: 6px; margin-bottom: 4px; border-bottom: solid 1px var(--xr-border-color); } .xr-header > div, .xr-header > ul { display: inline; margin-top: 0; margin-bottom: 0; } .xr-obj-type, .xr-array-name { margin-left: 2px; margin-right: 10px; } .xr-obj-type { color: var(--xr-font-color2); } .xr-sections { padding-left: 0 !important; display: grid; grid-template-columns: 150px auto auto 1fr 20px 20px; } .xr-section-item { display: contents; } .xr-section-item input { display: none; } .xr-section-item input + label { color: var(--xr-disabled-color); } .xr-section-item input:enabled + label { cursor: pointer; color: var(--xr-font-color2); } .xr-section-item input:enabled + label:hover { color: var(--xr-font-color0); } .xr-section-summary { grid-column: 1; color: var(--xr-font-color2); font-weight: 500; } .xr-section-summary > span { display: inline-block; padding-left: 0.5em; } .xr-section-summary-in:disabled + label { color: var(--xr-font-color2); } .xr-section-summary-in + label:before { display: inline-block; content: '\u00e2\u2013\u00ba'; font-size: 11px; width: 15px; text-align: center; } .xr-section-summary-in:disabled + label:before { color: var(--xr-disabled-color); } .xr-section-summary-in:checked + label:before { content: '\u00e2\u2013\u00bc'; } .xr-section-summary-in:checked + label > span { display: none; } .xr-section-summary, .xr-section-inline-details { padding-top: 4px; padding-bottom: 4px; } .xr-section-inline-details { grid-column: 2 / -1; } .xr-section-details { display: none; grid-column: 1 / -1; margin-bottom: 5px; } .xr-section-summary-in:checked ~ .xr-section-details { display: contents; } .xr-array-wrap { grid-column: 1 / -1; display: grid; grid-template-columns: 20px auto; } .xr-array-wrap > label { grid-column: 1; vertical-align: top; } .xr-preview { color: var(--xr-font-color3); } .xr-array-preview, .xr-array-data { padding: 0 5px !important; grid-column: 2; } .xr-array-data, .xr-array-in:checked ~ .xr-array-preview { display: none; } .xr-array-in:checked ~ .xr-array-data, .xr-array-preview { display: inline-block; } .xr-dim-list { display: inline-block !important; list-style: none; padding: 0 !important; margin: 0; } .xr-dim-list li { display: inline-block; padding: 0; margin: 0; } .xr-dim-list:before { content: '('; } .xr-dim-list:after { content: ')'; } .xr-dim-list li:not(:last-child):after { content: ','; padding-right: 5px; } .xr-has-index { font-weight: bold; } .xr-var-list, .xr-var-item { display: contents; } .xr-var-item > div, .xr-var-item label, .xr-var-item > .xr-var-name span { background-color: var(--xr-background-color-row-even); margin-bottom: 0; } .xr-var-item > .xr-var-name:hover span { padding-right: 5px; } .xr-var-list > li:nth-child(odd) > div, .xr-var-list > li:nth-child(odd) > label, .xr-var-list > li:nth-child(odd) > .xr-var-name span { background-color: var(--xr-background-color-row-odd); } .xr-var-name { grid-column: 1; } .xr-var-dims { grid-column: 2; } .xr-var-dtype { grid-column: 3; text-align: right; color: var(--xr-font-color2); } .xr-var-preview { grid-column: 4; } .xr-var-name, .xr-var-dims, .xr-var-dtype, .xr-preview, .xr-attrs dt { white-space: nowrap; overflow: hidden; text-overflow: ellipsis; padding-right: 10px; } .xr-var-name:hover, .xr-var-dims:hover, .xr-var-dtype:hover, .xr-attrs dt:hover { overflow: visible; width: auto; z-index: 1; } .xr-var-attrs, .xr-var-data { display: none; background-color: var(--xr-background-color) !important; padding-bottom: 5px !important; } .xr-var-attrs-in:checked ~ .xr-var-attrs, .xr-var-data-in:checked ~ .xr-var-data { display: block; } .xr-var-data > table { float: right; } .xr-var-name span, .xr-var-data, .xr-attrs { padding-left: 25px !important; } .xr-attrs, .xr-var-attrs, .xr-var-data { grid-column: 1 / -1; } dl.xr-attrs { padding: 0; margin: 0; display: grid; grid-template-columns: 125px auto; } .xr-attrs dt, .xr-attrs dd { padding: 0; margin: 0; float: left; padding-right: 10px; width: auto; } .xr-attrs dt { font-weight: normal; grid-column: 1; } .xr-attrs dt:hover span { display: inline-block; background: var(--xr-background-color); padding-right: 10px; } .xr-attrs dd { grid-column: 2; white-space: pre-wrap; word-break: break-all; } .xr-icon-database, .xr-icon-file-text2 { display: inline-block; vertical-align: middle; width: 1em; height: 1.5em !important; stroke-width: 0; stroke: currentColor; fill: currentColor; } <xarray.DataArray 'reshape-3b944f9ca9a40a223ab6382d90bfb37d' (y: 4176, x: 5568)> dask.array<mul, shape=(4176, 5568), dtype=float32, chunksize=(1392, 5568), chunktype=numpy.ndarray> Coordinates: crs object PROJCRS[\"unknown\",BASEGEOGCRS[\"unknown\",DATUM[\"unknown\",E... * y (y) float64 1.395e+06 1.396e+06 1.397e+06 ... 5.57e+06 5.571e+06 * x (x) float64 3.164e+06 3.163e+06 3.162e+06 ... -2.402e+06 -2.403e+06 Attributes: orbital_parameters: {'projection_longitude': 9.5, 'pr... sun_earth_distance_correction_applied: True sun_earth_distance_correction_factor: 0.9697642568677852 units: % wavelength: 0.7\u00e2\u20ac\u00af\u00c2\u00b5m\u00c2 (0.5-0.9\u00e2\u20ac\u00af\u00c2\u00b5m) standard_name: toa_bidirectional_reflectance platform_name: Meteosat-9 sensor: seviri start_time: 2020-12-08 09:00:08.206321 end_time: 2020-12-08 09:05:08.329479 area: Area ID: geos_seviri_hrv\\nDescrip... name: HRV resolution: 1000.134348869 calibration: reflectance modifiers: () _satpy_id: DataID(name='HRV', wavelength=Wav... ancillary_variables: [] xarray.DataArray 'reshape-3b944f9ca9a40a223ab6382d90bfb37d' y : 4176 x : 5568 dask.array<chunksize=(1392, 5568), meta=np.ndarray> Array Chunk Bytes 93.01 MB 31.00 MB Shape (4176, 5568) (1392, 5568) Count 214 Tasks 3 Chunks Type float32 numpy.ndarray 5568 4176 Coordinates: (3) crs () object PROJCRS[\"unknown\",BASEGEOGCRS[\"u... array(<Projected CRS: PROJCRS[\"unknown\",BASEGEOGCRS[\"unknown\",DATUM[\"unk ...> Name: unknown Axis Info [cartesian]: - E[east]: Easting (metre) - N[north]: Northing (metre) Area of Use: - undefined Coordinate Operation: - name: unknown - method: Geostationary Satellite (Sweep Y) Datum: unknown - Ellipsoid: unknown - Prime Meridian: Greenwich , dtype=object) y (y) float64 1.395e+06 1.396e+06 ... 5.571e+06 units : meter array([1395187.416673, 1396187.551022, 1397187.68537 , ..., 5568748.054504, 5569748.188853, 5570748.323202]) x (x) float64 3.164e+06 3.163e+06 ... -2.403e+06 units : meter array([ 3164425.079823, 3163424.945474, 3162424.811125, ..., -2401322.571635, -2402322.705984, -2403322.840333]) Attributes: (17) orbital_parameters : {'projection_longitude': 9.5, 'projection_latitude': 0.0, 'projection_altitude': 35785831.0} sun_earth_distance_correction_applied : True sun_earth_distance_correction_factor : 0.9697642568677852 units : % wavelength : 0.7\u00e2\u20ac\u00af\u00c2\u00b5m\u00c2 (0.5-0.9\u00e2\u20ac\u00af\u00c2\u00b5m) standard_name : toa_bidirectional_reflectance platform_name : Meteosat-9 sensor : seviri start_time : 2020-12-08 09:00:08.206321 end_time : 2020-12-08 09:05:08.329479 area : Area ID: geos_seviri_hrv Description: SEVIRI high resolution channel area Projection ID: seviri_hrv Projection: {'a': '6378169', 'h': '35785831', 'lon_0': '9.5', 'no_defs': 'None', 'proj': 'geos', 'rf': '295.488065897014', 'type': 'crs', 'units': 'm', 'x_0': '0', 'y_0': '0'} Number of columns: 5568 Number of rows: 4176 Area extent: (3164925.147, 5571248.3904, -2403822.9075, 1394687.3495) name : HRV resolution : 1000.134348869 calibration : reflectance modifiers : () _satpy_id : DataID(name='HRV', wavelength=WavelengthRange(min=0.5, central=0.7, max=0.9, unit='\u00c2\u00b5m'), resolution=1000.134348869, calibration=<calibration.reflectance>, modifiers=()) ancillary_variables : [] We can see that the DataArray contains a crs, however we'll make our own custom area definition that's more accurate. First we'll create a helper function that will create our area definitions. #exports def calculate_x_offset ( native_fp ): handler = seviri_l1b_native . NativeMSGFileHandler ( native_fp , {}, None ) lower_east_column_planned = handler . header [ '15_DATA_HEADER' ][ 'ImageDescription' ][ 'PlannedCoverageHRV' ][ 'LowerEastColumnPlanned' ] x_offset = 32500 + (( 2733 - lower_east_column_planned ) * 1000 ) return x_offset def get_seviri_area_def ( native_fp , num_x_pixels = 5568 , num_y_pixels = 4176 ) -> AreaDefinition : \"\"\" The HRV channel on Meteosat Second Generation satellites doesn't scan the full number of columns. The east boundary of the HRV channel changes (e.g. to maximise the amount of the image which is illuminated by sunlight. Parameters: native_fp: Data filepath \"\"\" x_offset = calculate_x_offset ( native_fp ) # The EUMETSAT docs say \"The distance between spacecraft and centre of earth is 42,164 km. The idealized earth # is a perfect ellipsoid with an equator radius of 6378.1690 km and a polar radius of 6356.5838 km.\" # The projection used by SatPy expresses height as height above the Earth's surface (not distance # to the centre of the Earth). projection = { 'proj' : 'geos' , 'lon_0' : 9.5 , 'a' : 6378169.0 , 'b' : 6356583.8 , 'h' : 35785831.00 , 'units' : 'm' } seviri = AreaDefinition ( area_id = 'seviri' , description = 'SEVIRI RSS HRV' , proj_id = 'seviri' , projection = projection , width = num_x_pixels , height = num_y_pixels , area_extent = [ - 2768872.0236 + x_offset , # left 1394687.3495 , # bottom (from scene['HRV'].area) 2799876.1893 + x_offset , # right 5570248.4773 ] # top (from scene['HRV'].area) ) return seviri Then we'll use it to construct the relevant one for Seviri seviri = get_seviri_area_def ( native_fp ) seviri_crs = seviri . to_cartopy_crs () seviri_crs C:\\Users\\Ayrto\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\pyproj\\crs\\crs.py:543: UserWarning: You will likely lose important projection information when converting to a PROJ string from another format. See: https://proj.org/faq.html#what-is-the-best-format-for-describing-coordinate-reference-systems proj_string = self.to_proj4() 2020-12-16T20:22:49.021885 image/svg+xml Matplotlib v3.3.2, https://matplotlib.org/ *{stroke-linecap:butt;stroke-linejoin:round;} _PROJ4Projection(+ellps=WGS84 +a=6378169.0 +rf=295.488065897001 +h=35785831.0 +lon_0=9.5 +no_defs=True +proj=geos +type=crs +units=m +x_0=0.0 +y_0=0.0 +no_defs) We'll create a loader function that will extract the relevant data for lower_east_column_planned automatically #exports def load_scene ( native_fp ): # Reading scene and loading HRV scene = Scene ( filenames = [ native_fp ], reader = 'seviri_l1b_native' ) # Identifying and recording lower_east_column_planned handler = seviri_l1b_native . NativeMSGFileHandler ( native_fp , {}, None ) scene . attrs [ 'lower_east_column_planned' ] = handler . header [ '15_DATA_HEADER' ][ 'ImageDescription' ][ 'PlannedCoverageHRV' ][ 'LowerEastColumnPlanned' ] return scene We'll see how quickly this loads %% time scene = load_scene ( native_fp ) scene . load ([ 'HRV' ]) Wall time: 464 ms C:\\Users\\Ayrto\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\pyproj\\crs\\crs.py:543: UserWarning: You will likely lose important projection information when converting to a PROJ string from another format. See: https://proj.org/faq.html#what-is-the-best-format-for-describing-coordinate-reference-systems proj_string = self.to_proj4() We can visualise what a specific band looks like fig = plt . figure ( dpi = 250 , figsize = ( 10 , 10 )) ax = plt . axes ( projection = seviri_crs ) scene [ 'HRV' ] . plot . imshow ( ax = ax , add_colorbar = False , cmap = 'magma' , vmin = 0 , vmax = 50 ) ax . set_title ( '' ) ax . coastlines ( resolution = '50m' , alpha = 0.8 , color = 'white' ) <cartopy.mpl.feature_artist.FeatureArtist at 0x1912aa1d1f0> One of the benefits of having access to the underlying XArray object is that we can more easily start to do some analysis with the data, for example defining a reflectance threshold reflectance_threshold = 35 cmap = colors . ListedColormap ([ ( 0 , 0 , 0 , 0 ), # transparent ( 251 / 255 , 242 / 255 , 180 / 255 , 1 ) # yellow # (0.533, 0.808, 0.922, 1) # grey-like blue ]) # Plotting fig = plt . figure ( dpi = 250 , figsize = ( 10 , 10 )) ax = plt . axes ( projection = seviri_crs ) scene [ 'HRV' ] . plot . imshow ( ax = ax , vmin = 0 , vmax = 50 , cmap = 'magma' , add_colorbar = False ) ( scene [ 'HRV' ] > reflectance_threshold ) . plot . imshow ( ax = ax , cmap = cmap , add_colorbar = False ) ax . set_title ( '' ) ax . coastlines ( resolution = '50m' , alpha = 0.8 , color = 'white' ) <cartopy.mpl.feature_artist.FeatureArtist at 0x1912ac2b430> We'll extract the values from the XArray object, then mask all NaN values to enable us to carry out statistical analysis HRV = scene [ \"HRV\" ] . values HRV_masked = ma . masked_array ( HRV , mask = xr . ufuncs . isnan ( scene [ \"HRV\" ]) . values ) np . mean ( HRV_masked ) 12.372444717553362 We can also visualise the full distribution. N.b. to reduce the time it takes to calculate the best KDE fit we'll take only a sample of the data. HRV_sample = np . random . choice ( HRV_masked . flatten (), 1_000_000 ) # Plotting fig , ax = plt . subplots ( dpi = 250 ) sns . kdeplot ( HRV_sample , ax = ax , fill = True ) ax . set_yticks ([]) ax . set_ylabel ( '' ) ax . set_xlabel ( 'HRV Reflectance' ) hlp . hide_spines ( ax , positions = [ 'top' , 'left' , 'right' ])","title":"Exploratory Data Analysis"},{"location":"02_reproj/#evaluating-reprojection-to-tranverse-mercator","text":"Before we can resample we need to define the area we're resampling to, we'll write a constructor to help us do this #exports def construct_area_def ( scene , area_id , description , proj_id , projection , west , south , east , north , pixel_size = None ): # If None then will use same number of x and y points # HRV's resolution will be more like 4km for Europe if pixel_size is not None : width = int (( east - west ) / pixel_size ) height = int (( north - south ) / pixel_size ) else : width = scene [ list ( scene . keys ())[ 0 ][ 'name' ]] . x . values . shape [ 0 ] height = scene [ list ( scene . keys ())[ 0 ][ 'name' ]] . y . values . shape [ 0 ] area_extent = ( west , south , east , north ) area_def = AreaDefinition ( area_id , description , proj_id , projection , width , height , area_extent ) return area_def def construct_TM_area_def ( scene ): meters_per_pixel = 4000 west , south , east , north = ( - 3090000 , 1690000 , 4390000 , 9014000 ) area_id = 'TM' description = 'Transverse Mercator' proj_id = 'TM' projection = { 'ellps' : 'WGS84' , 'proj' : 'tmerc' , # Transverse Mercator 'units' : 'm' # meters } tm_area_def = construct_area_def ( scene , area_id , description , proj_id , projection , west , south , east , north , meters_per_pixel ) return tm_area_def tm_area_def = construct_TM_area_def ( scene ) tm_area_def . to_cartopy_crs () C:\\Users\\Ayrto\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\pyproj\\crs\\crs.py:543: UserWarning: You will likely lose important projection information when converting to a PROJ string from another format. See: https://proj.org/faq.html#what-is-the-best-format-for-describing-coordinate-reference-systems proj_string = self.to_proj4() 2020-12-16T20:23:04.493284 image/svg+xml Matplotlib v3.3.2, https://matplotlib.org/ *{stroke-linecap:butt;stroke-linejoin:round;} _PROJ4Projection(+ellps=WGS84 +k=1.0 +lat_0=0.0 +lon_0=0.0 +no_defs=True +proj=tmerc +type=crs +units=m +x_0=0.0 +y_0=0.0 +no_defs) We can now carry out the resampling using the pyresample library %% time resampled_scene = scene . resample ( tm_area_def , resampler = 'nearest' ) C:\\Users\\Ayrto\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\pyresample\\spherical.py:123: RuntimeWarning: invalid value encountered in true_divide self.cart /= np.sqrt(np.einsum('...i, ...i', self.cart, self.cart)) C:\\Users\\Ayrto\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\pyresample\\spherical.py:178: RuntimeWarning: invalid value encountered in double_scalars return (val + mod) % (2 * mod) - mod C:\\Users\\Ayrto\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\pyproj\\crs\\crs.py:543: UserWarning: You will likely lose important projection information when converting to a PROJ string from another format. See: https://proj.org/faq.html#what-is-the-best-format-for-describing-coordinate-reference-systems proj_string = self.to_proj4() C:\\Users\\Ayrto\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\numpy\\lib\\function_base.py:1280: RuntimeWarning: invalid value encountered in subtract a = op(a[slice1], a[slice2]) Wall time: 2.59 s We'll quickly check that the reprojection looks ok fig = plt . figure ( dpi = 250 , figsize = ( 10 , 10 )) ax = plt . axes ( projection = ccrs . TransverseMercator ()) resampled_scene [ 'HRV' ] . plot . imshow ( ax = ax ) ax . coastlines ( resolution = '50m' , alpha = 0.8 , color = 'white' ) <ipython-input-20-ec0e500c536a>:2: UserWarning: The default value for the *approx* keyword argument to TransverseMercator will change from True to False after 0.18. ax = plt.axes(projection=ccrs.TransverseMercator()) C:\\Users\\Ayrto\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\dask\\core.py:121: RuntimeWarning: invalid value encountered in sin return func(*(_execute_task(a, cache) for a in args)) C:\\Users\\Ayrto\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\dask\\core.py:121: RuntimeWarning: invalid value encountered in cos return func(*(_execute_task(a, cache) for a in args)) <cartopy.mpl.feature_artist.FeatureArtist at 0x1912bf95190> We want to gain a deeper understanding of the reprojection that's being carried out, to do this we'll manually reproject a sample of the original gridded coordinates %% time orig_x_values = scene [ 'HRV' ] . x . values [:: 50 ] orig_y_values = scene [ 'HRV' ] . y . values [:: 50 ] XX , YY = np . meshgrid ( orig_x_values , orig_y_values ) df_proj_points = ( gpd . GeoSeries ([ Point ( x , y ) for x , y in np . stack ([ XX . flatten (), YY . flatten ()], axis = 1 ) ]) . set_crs ( crs = scene [ 'HRV' ] . area . crs_wkt ) . to_crs ( crs = resampled_scene [ 'HRV' ] . area . crs_wkt ) . apply ( lambda point : pd . Series ( list ( point . coords )[ 0 ])) . rename ( columns = { 0 : 'x_reproj' , 1 : 'y_reproj' }) . replace ( np . inf , np . nan ) . pipe ( lambda df : df . assign ( x_orig = XX . flatten ())) . pipe ( lambda df : df . assign ( y_orig = YY . flatten ())) ) df_proj_points . head () Wall time: 2.77 s .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } x_reproj y_reproj x_orig y_orig 0 4.863405e+06 1.917787e+06 3.164425e+06 1.395187e+06 1 4.781323e+06 1.899419e+06 3.114418e+06 1.395187e+06 2 4.700620e+06 1.881746e+06 3.064412e+06 1.395187e+06 3 4.621232e+06 1.864731e+06 3.014405e+06 1.395187e+06 4 4.543102e+06 1.848341e+06 2.964398e+06 1.395187e+06 We can then visualise the reprojection of the original grid against the regridded reprojection %% time fig = plt . figure ( dpi = 250 , figsize = ( 10 , 10 )) ax = plt . axes ( projection = ccrs . TransverseMercator ()) resampled_scene [ 'HRV' ] . plot . imshow ( ax = ax , cmap = 'Greys_r' ) ax . coastlines ( resolution = '50m' , alpha = 0.8 , color = 'white' ) ax . scatter ( df_proj_points [ 'x_reproj' ][:: 10 ], df_proj_points [ 'y_reproj' ][:: 10 ], s = 2 , color = 'red' ) <timed exec>:2: UserWarning: The default value for the *approx* keyword argument to TransverseMercator will change from True to False after 0.18. C:\\Users\\Ayrto\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\dask\\core.py:121: RuntimeWarning: invalid value encountered in sin return func(*(_execute_task(a, cache) for a in args)) C:\\Users\\Ayrto\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\dask\\core.py:121: RuntimeWarning: invalid value encountered in cos return func(*(_execute_task(a, cache) for a in args)) Wall time: 30.7 s <matplotlib.collections.PathCollection at 0x1910024cd00> This is useful for quick visual inspection, for example we can see that the y axis gets stretched further the nearer to the pole. However, we want to get a better understanding of how the local cell resolution is changing for any given point, we'll begin by looking at this change for Greenwich. def lon_lat_to_new_crs ( lon , lat , crs ): x , y = list ( gpd . GeoSeries ([ Point ( lon , lat )]) . set_crs ( 4326 ) . to_crs ( crs ) . iloc [ 0 ] . coords )[ 0 ] return x , y def calc_res_change ( src_x , src_y , src_da , dst_da , src_dx = 10 , src_dy = 10 ): src_crs = src_da . area . crs_wkt dst_crs = dst_da . area . crs_wkt src_x_width = np . abs ( np . diff ( src_da . x . values )[ 0 ]) src_y_width = np . abs ( np . diff ( src_da . y . values )[ 0 ]) dst_x_width = np . abs ( np . diff ( dst_da . x . values )[ 0 ]) dst_y_width = np . abs ( np . diff ( dst_da . y . values )[ 0 ]) s_points = ( gpd . GeoSeries ([ Point ( src_x , src_y ), Point ( src_x + src_dx , src_y ), Point ( src_x , src_y + src_dy ) ]) . set_crs ( src_crs ) . to_crs ( dst_crs ) ) dst_dx = s_points . iloc [ 0 ] . distance ( s_points . iloc [ 1 ]) dst_dy = s_points . iloc [ 0 ] . distance ( s_points . iloc [ 2 ]) x_ratio_change = ( dst_dx / dst_x_width ) / ( src_dx / src_x_width ) y_ratio_change = ( dst_dy / dst_y_width ) / ( src_dy / src_y_width ) return x_ratio_change , y_ratio_change lon = 0 lat = 51.4934 src_x , src_y = lon_lat_to_new_crs ( lon , lat , scene [ 'HRV' ] . area . crs_wkt ) x_ratio_change , y_ratio_change = calc_res_change ( src_x , src_y , scene [ 'HRV' ], resampled_scene [ 'HRV' ]) x_ratio_change , y_ratio_change (0.27381567467569573, 0.528776076616483) We'll double check this by calculating it through a different method, in this case by locating the nearest cell for each scene and comparing their sizes in a common coordinate system def get_da_nearest_cell_width_height ( da , x , y , units_crs ): nearest_loc = da . sel ( x = x , y = y , method = 'nearest' ) nearest_x = nearest_loc . x . values nearest_y = nearest_loc . y . values next_nearest_x = da . x . values [ list ( da . x . values ) . index ( nearest_x ) + 1 ] next_nearest_y = da . y . values [ list ( da . y . values ) . index ( nearest_y ) + 1 ] s_points = ( gpd . GeoSeries ([ Point ( nearest_x , nearest_y ), Point ( next_nearest_x , nearest_y ), Point ( nearest_x , next_nearest_y ) ]) . set_crs ( da . area . crs_wkt ) . to_crs ( units_crs ) ) x_width = s_points . iloc [ 0 ] . distance ( s_points . iloc [ 1 ]) y_height = s_points . iloc [ 0 ] . distance ( s_points . iloc [ 2 ]) return x_width , y_height src_x , src_y = lon_lat_to_new_crs ( lon , lat , scene [ 'HRV' ] . area . crs_wkt ) dst_x , dst_y = lon_lat_to_new_crs ( lon , lat , resampled_scene [ 'HRV' ] . area . crs_wkt ) src_x_width , src_y_height = get_da_nearest_cell_width_height ( scene [ 'HRV' ], src_x , src_y , 27700 ) dst_x_width , dst_y_height = get_da_nearest_cell_width_height ( resampled_scene [ 'HRV' ], dst_x , dst_y , 27700 ) print ( f 'The width has changed from { round ( src_x_width / 1000 , 2 ) } km to { round ( dst_x_width / 1000 , 2 ) } km' ) print ( f 'The height has changed from { round ( src_y_height / 1000 , 2 ) } km to { round ( dst_y_height / 1000 , 2 ) } km' ) The width has changed from 1.09 km to 4.0 km The height has changed from 2.12 km to 4.0 km This can easily be converted into a x and y pixel size ratio change which almost exactly matches our previous calculation. The first calculation is more accurate as the dx and dy can approach 0 and get closer to the true ratio change, however the get_da_nearest_cell_width_height function is still useful as it allows us to determine the cell width and height in more interpretable units x_ratio_change , y_ratio_change = src_x_width / dst_x_width , src_y_height / dst_y_height x_ratio_change , y_ratio_change (0.2738180115545141, 0.5290020702784486) Iceland is stretched further still def print_pixel_change ( lon , lat , da_src , da_dst ): src_x , src_y = lon_lat_to_new_crs ( lon , lat , da_src . area . crs_wkt ) dst_x , dst_y = lon_lat_to_new_crs ( lon , lat , da_dst . area . crs_wkt ) src_x_width , src_y_height = get_da_nearest_cell_width_height ( da_src , src_x , src_y , 27700 ) dst_x_width , dst_y_height = get_da_nearest_cell_width_height ( da_dst , dst_x , dst_y , 27700 ) print ( f 'The width has changed from { round ( src_x_width / 1000 , 2 ) } km to { round ( dst_x_width / 1000 , 2 ) } km' ) print ( f 'The height has changed from { round ( src_y_height / 1000 , 2 ) } km to { round ( dst_y_height / 1000 , 2 ) } km' ) return lon = - 18.779208 lat = 64.887370 print_pixel_change ( lon , lat , scene [ 'HRV' ], resampled_scene [ 'HRV' ]) The width has changed from 1.52 km to 3.99 km The height has changed from 4.75 km to 3.99 km And contrasts with Marrakesh which is stretched less than Greenwich in the y axis lon = - 8.005657 lat = 31.636355 print_pixel_change ( lon , lat , scene [ 'HRV' ], resampled_scene [ 'HRV' ]) The width has changed from 1.11 km to 3.99 km The height has changed from 1.33 km to 3.99 km We can check what the cell height and width are at the center of the image, they should both be close to 1km according to the SEVIRI documentation LineDirGridStep gives the grid step size in km SSP in the line direction. Default value is 3km for VIS and IR, and 1km for HRV. The on-ground grid step size of 3 km at the SSP represents an instrument scan step of 251.53 microrad divided by 3. - EUMETSAT round_m_to_km = lambda m : round ( m / 1000 , 2 ) UTM_35N_epsg = 32632 # should be relatively accurate and is in meters src_x = np . median ( scene [ 'HRV' ] . x . values ) src_y = np . median ( scene [ 'HRV' ] . y . values ) src_x_width , src_y_height = get_da_nearest_cell_width_height ( scene [ 'HRV' ], src_x , src_y , UTM_35N_epsg ) round_m_to_km ( src_x_width ), round_m_to_km ( src_y_height ) (1.04, 1.36)","title":"Evaluating Reprojection to Tranverse Mercator"},{"location":"02_reproj/#comparing-reprojection-libraries","text":"In the last section we used pyresample to carry out the data reprojection, here we'll explore pyinterp . Before we start we'll quickly extract the xarrays for the original and reprojected coordinates. def extract_formatted_scene ( scene , variable = 'HRV' , x_coords_name = 'x' , y_coords_name = 'y' , x_units = 'metre' , y_units = 'metre' ): da = ( scene [ variable ] . copy () . rename ({ 'x' : x_coords_name , 'y' : y_coords_name }) ) da [ x_coords_name ] . attrs [ 'units' ] = x_units da [ y_coords_name ] . attrs [ 'units' ] = y_units return da da = extract_formatted_scene ( scene ) da_resampled = extract_formatted_scene ( resampled_scene ) da_resampled /* CSS stylesheet for displaying xarray objects in jupyterlab. * */ :root { --xr-font-color0: var(--jp-content-font-color0, rgba(0, 0, 0, 1)); --xr-font-color2: var(--jp-content-font-color2, rgba(0, 0, 0, 0.54)); --xr-font-color3: var(--jp-content-font-color3, rgba(0, 0, 0, 0.38)); --xr-border-color: var(--jp-border-color2, #e0e0e0); --xr-disabled-color: var(--jp-layout-color3, #bdbdbd); --xr-background-color: var(--jp-layout-color0, white); --xr-background-color-row-even: var(--jp-layout-color1, white); --xr-background-color-row-odd: var(--jp-layout-color2, #eeeeee); } html[theme=dark], body.vscode-dark { --xr-font-color0: rgba(255, 255, 255, 1); --xr-font-color2: rgba(255, 255, 255, 0.54); --xr-font-color3: rgba(255, 255, 255, 0.38); --xr-border-color: #1F1F1F; --xr-disabled-color: #515151; --xr-background-color: #111111; --xr-background-color-row-even: #111111; --xr-background-color-row-odd: #313131; } .xr-wrap { display: block; min-width: 300px; max-width: 700px; } .xr-text-repr-fallback { /* fallback to plain text repr when CSS is not injected (untrusted notebook) */ display: none; } .xr-header { padding-top: 6px; padding-bottom: 6px; margin-bottom: 4px; border-bottom: solid 1px var(--xr-border-color); } .xr-header > div, .xr-header > ul { display: inline; margin-top: 0; margin-bottom: 0; } .xr-obj-type, .xr-array-name { margin-left: 2px; margin-right: 10px; } .xr-obj-type { color: var(--xr-font-color2); } .xr-sections { padding-left: 0 !important; display: grid; grid-template-columns: 150px auto auto 1fr 20px 20px; } .xr-section-item { display: contents; } .xr-section-item input { display: none; } .xr-section-item input + label { color: var(--xr-disabled-color); } .xr-section-item input:enabled + label { cursor: pointer; color: var(--xr-font-color2); } .xr-section-item input:enabled + label:hover { color: var(--xr-font-color0); } .xr-section-summary { grid-column: 1; color: var(--xr-font-color2); font-weight: 500; } .xr-section-summary > span { display: inline-block; padding-left: 0.5em; } .xr-section-summary-in:disabled + label { color: var(--xr-font-color2); } .xr-section-summary-in + label:before { display: inline-block; content: '\u00e2\u2013\u00ba'; font-size: 11px; width: 15px; text-align: center; } .xr-section-summary-in:disabled + label:before { color: var(--xr-disabled-color); } .xr-section-summary-in:checked + label:before { content: '\u00e2\u2013\u00bc'; } .xr-section-summary-in:checked + label > span { display: none; } .xr-section-summary, .xr-section-inline-details { padding-top: 4px; padding-bottom: 4px; } .xr-section-inline-details { grid-column: 2 / -1; } .xr-section-details { display: none; grid-column: 1 / -1; margin-bottom: 5px; } .xr-section-summary-in:checked ~ .xr-section-details { display: contents; } .xr-array-wrap { grid-column: 1 / -1; display: grid; grid-template-columns: 20px auto; } .xr-array-wrap > label { grid-column: 1; vertical-align: top; } .xr-preview { color: var(--xr-font-color3); } .xr-array-preview, .xr-array-data { padding: 0 5px !important; grid-column: 2; } .xr-array-data, .xr-array-in:checked ~ .xr-array-preview { display: none; } .xr-array-in:checked ~ .xr-array-data, .xr-array-preview { display: inline-block; } .xr-dim-list { display: inline-block !important; list-style: none; padding: 0 !important; margin: 0; } .xr-dim-list li { display: inline-block; padding: 0; margin: 0; } .xr-dim-list:before { content: '('; } .xr-dim-list:after { content: ')'; } .xr-dim-list li:not(:last-child):after { content: ','; padding-right: 5px; } .xr-has-index { font-weight: bold; } .xr-var-list, .xr-var-item { display: contents; } .xr-var-item > div, .xr-var-item label, .xr-var-item > .xr-var-name span { background-color: var(--xr-background-color-row-even); margin-bottom: 0; } .xr-var-item > .xr-var-name:hover span { padding-right: 5px; } .xr-var-list > li:nth-child(odd) > div, .xr-var-list > li:nth-child(odd) > label, .xr-var-list > li:nth-child(odd) > .xr-var-name span { background-color: var(--xr-background-color-row-odd); } .xr-var-name { grid-column: 1; } .xr-var-dims { grid-column: 2; } .xr-var-dtype { grid-column: 3; text-align: right; color: var(--xr-font-color2); } .xr-var-preview { grid-column: 4; } .xr-var-name, .xr-var-dims, .xr-var-dtype, .xr-preview, .xr-attrs dt { white-space: nowrap; overflow: hidden; text-overflow: ellipsis; padding-right: 10px; } .xr-var-name:hover, .xr-var-dims:hover, .xr-var-dtype:hover, .xr-attrs dt:hover { overflow: visible; width: auto; z-index: 1; } .xr-var-attrs, .xr-var-data { display: none; background-color: var(--xr-background-color) !important; padding-bottom: 5px !important; } .xr-var-attrs-in:checked ~ .xr-var-attrs, .xr-var-data-in:checked ~ .xr-var-data { display: block; } .xr-var-data > table { float: right; } .xr-var-name span, .xr-var-data, .xr-attrs { padding-left: 25px !important; } .xr-attrs, .xr-var-attrs, .xr-var-data { grid-column: 1 / -1; } dl.xr-attrs { padding: 0; margin: 0; display: grid; grid-template-columns: 125px auto; } .xr-attrs dt, .xr-attrs dd { padding: 0; margin: 0; float: left; padding-right: 10px; width: auto; } .xr-attrs dt { font-weight: normal; grid-column: 1; } .xr-attrs dt:hover span { display: inline-block; background: var(--xr-background-color); padding-right: 10px; } .xr-attrs dd { grid-column: 2; white-space: pre-wrap; word-break: break-all; } .xr-icon-database, .xr-icon-file-text2 { display: inline-block; vertical-align: middle; width: 1em; height: 1.5em !important; stroke-width: 0; stroke: currentColor; fill: currentColor; } <xarray.DataArray 'my_index-25c95e08ed138cbd282b6596ed55c066' (y: 1831, x: 1870)> dask.array<copy, shape=(1831, 1870), dtype=float32, chunksize=(1831, 1870), chunktype=numpy.ndarray> Coordinates: crs object PROJCRS[\"unknown\",BASEGEOGCRS[\"unknown\",DATUM[\"Unknown ba... * y (y) float64 9.012e+06 9.008e+06 9.004e+06 ... 1.696e+06 1.692e+06 * x (x) float64 -3.088e+06 -3.084e+06 -3.08e+06 ... 4.384e+06 4.388e+06 Attributes: orbital_parameters: {'projection_longitude': 9.5, 'pr... sun_earth_distance_correction_applied: True sun_earth_distance_correction_factor: 0.9697642568677852 units: % wavelength: 0.7\u00e2\u20ac\u00af\u00c2\u00b5m\u00c2 (0.5-0.9\u00e2\u20ac\u00af\u00c2\u00b5m) standard_name: toa_bidirectional_reflectance platform_name: Meteosat-9 sensor: seviri start_time: 2020-12-08 09:00:08.206321 end_time: 2020-12-08 09:05:08.329479 area: Area ID: TM\\nDescription: Transve... name: HRV resolution: 1000.134348869 calibration: reflectance modifiers: () _satpy_id: DataID(name='HRV', wavelength=Wav... ancillary_variables: [] xarray.DataArray 'my_index-25c95e08ed138cbd282b6596ed55c066' y : 1831 x : 1870 dask.array<chunksize=(1831, 1870), meta=np.ndarray> Array Chunk Bytes 13.70 MB 13.70 MB Shape (1831, 1870) (1831, 1870) Count 360 Tasks 1 Chunks Type float32 numpy.ndarray 1870 1831 Coordinates: (3) crs () object PROJCRS[\"unknown\",BASEGEOGCRS[\"u... array(<Projected CRS: PROJCRS[\"unknown\",BASEGEOGCRS[\"unknown\",DATUM[\"Unk ...> Name: unknown Axis Info [cartesian]: - E[east]: Easting (metre) - N[north]: Northing (metre) Area of Use: - undefined Coordinate Operation: - name: unknown - method: Transverse Mercator Datum: Unknown based on WGS84 ellipsoid - Ellipsoid: WGS 84 - Prime Meridian: Greenwich , dtype=object) y (y) float64 9.012e+06 9.008e+06 ... 1.692e+06 units : metre array([9012000., 9008000., 9004000., ..., 1700000., 1696000., 1692000.]) x (x) float64 -3.088e+06 -3.084e+06 ... 4.388e+06 units : metre array([-3088000., -3084000., -3080000., ..., 4380000., 4384000., 4388000.]) Attributes: (17) orbital_parameters : {'projection_longitude': 9.5, 'projection_latitude': 0.0, 'projection_altitude': 35785831.0} sun_earth_distance_correction_applied : True sun_earth_distance_correction_factor : 0.9697642568677852 units : % wavelength : 0.7\u00e2\u20ac\u00af\u00c2\u00b5m\u00c2 (0.5-0.9\u00e2\u20ac\u00af\u00c2\u00b5m) standard_name : toa_bidirectional_reflectance platform_name : Meteosat-9 sensor : seviri start_time : 2020-12-08 09:00:08.206321 end_time : 2020-12-08 09:05:08.329479 area : Area ID: TM Description: Transverse Mercator Projection ID: TM Projection: {'ellps': 'WGS84', 'k': '1', 'lat_0': '0', 'lon_0': '0', 'no_defs': 'None', 'proj': 'tmerc', 'type': 'crs', 'units': 'm', 'x_0': '0', 'y_0': '0'} Number of columns: 1870 Number of rows: 1831 Area extent: (-3090000, 1690000, 4390000, 9014000) name : HRV resolution : 1000.134348869 calibration : reflectance modifiers : () _satpy_id : DataID(name='HRV', wavelength=WavelengthRange(min=0.5, central=0.7, max=0.9, unit='\u00c2\u00b5m'), resolution=1000.134348869, calibration=<calibration.reflectance>, modifiers=()) ancillary_variables : [] We'll now save the coordinates of the grid we're using in the new projection new_grid_4km_TM = { 'x_coords' : list ( da_resampled . x . values ), 'y_coords' : list ( da_resampled . y . values ) } save_data = True if save_data == True : with open ( '../data/intermediate/new_grid_4km_TM.json' , 'w' ) as fp : json . dump ( new_grid_4km_TM , fp ) JSON ( new_grid_4km_TM ) <IPython.core.display.JSON object> As well as calculate the locations of those points in the original CRS %% time def chunks ( list_ , n ): \"\"\" Yield successive n-sized chunks from `list_`. \"\"\" for i in range ( 0 , len ( list_ ), n ): yield list_ [ i : i + n ] def reproject_geometries ( da , old_crs , new_crs , chunk_size = 5000 ): xx , yy = np . meshgrid ( da . x . values , da . y . values , indexing = 'ij' ) geometry = gpd . points_from_xy ( xx . flatten (), yy . flatten ()) new_coords_samples = [] for geometry_sample in chunks ( geometry , chunk_size ): df_new_coords_sample = ( gpd . GeoSeries ( geometry_sample , crs = old_crs ) . to_crs ( new_crs ) . apply ( lambda x : list ( x . coords [ 0 ])) . apply ( pd . Series ) . rename ( columns = { 0 : 'x' , 1 : 'y' }) ) new_coords_samples += [ df_new_coords_sample ] df_new_coords = pd . concat ( new_coords_samples , ignore_index = True ) return df_new_coords if not os . path . exists ( intermediate_data_dir ): os . makedirs ( intermediate_data_dir ) if calculate_reproj_coords == True : df_new_coords = reproject_geometries ( da_resampled , '+proj=tmerc' , seviri_crs . proj4_init ) df_new_coords . to_csv ( f ' { intermediate_data_dir } /reproj_coords_TM_4km.csv' , index = False ) elif 'reproj_coords.csv' not in os . listdir ( intermediate_data_dir ): df_new_coords = pd . read_csv ( 'https://storage.googleapis.com/reprojection_cache/reproj_coords_TM_4km.csv' ) else : df_new_coords = pd . read_csv ( f ' { intermediate_data_dir } /reproj_coords_TM_4km.csv' ) df_new_coords . head () Wall time: 1.51 s .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } x y 0 inf inf 1 inf inf 2 inf inf 3 inf inf 4 inf inf We can layer these on top of each other to get an alternative view of the transform operation %% time old_x_positions , old_y_positions = [ elem . flatten () for elem in np . meshgrid ( da . x . values [:: 100 ], da . y . values [:: 100 ], indexing = 'ij' )] new_x_positions , new_y_positions = df_new_coords [ 'x' ][:: 100 ], df_new_coords [ 'y' ][:: 100 ] # Plotting fig , ax = plt . subplots ( dpi = 150 ) ax . scatter ( old_x_positions , old_y_positions , s = 0.1 ) ax . scatter ( new_x_positions , new_y_positions , s = 0.1 ) hlp . hide_spines ( ax ) Wall time: 55.9 ms We'll now use pyinterp to take these and use them to carry out the resampling. We'll also create a wrapper for converting the result back into an Xarray object. #exports def reproj_with_manual_grid ( da , x_coords , y_coords , new_grid ): x_axis = pyinterp . Axis ( da . x . values ) y_axis = pyinterp . Axis ( da . y . values ) grid = pyinterp . Grid2D ( x_axis , y_axis , da . data . T ) reproj_data = ( pyinterp . bivariate ( grid , x_coords , y_coords ) . reshape (( len ( new_grid [ 'x_coords' ]), len ( new_grid [ 'y_coords' ]))) ) return reproj_data def reproj_to_xarray ( da , x_coords , y_coords , new_grid ): # We'll reproject the data reproj_data = reproj_with_manual_grid ( da , x_coords , y_coords , new_grid ) # Then put it in an XArray DataArray da_reproj = xr . DataArray ( np . flip ( reproj_data . T , axis = ( 0 , 1 )), dims = ( 'y' , 'x' ), coords = { 'x' : new_grid [ 'x_coords' ][:: - 1 ], 'y' : new_grid [ 'y_coords' ][:: - 1 ] }, attrs = da . attrs ) return da_reproj We'll load the grid back in with open ( '../data/intermediate/new_grid_4km_TM.json' , 'r' ) as fp : new_grid = json . load ( fp ) JSON ( new_grid ) <IPython.core.display.JSON object> Confirm that the size of the grid definition arrays match the number of coordinates we have df_new_coords [ 'y' ] . size == len ( new_grid [ 'x_coords' ]) * len ( new_grid [ 'y_coords' ]) True And finally carry out the reprojection %% timeit da_reproj = reproj_to_xarray ( da , df_new_coords [ 'x' ], df_new_coords [ 'y' ], new_grid ) 710 ms \u00c2\u00b1 39.4 ms per loop (mean \u00c2\u00b1 std. dev. of 7 runs, 1 loop each) Most importantly we'll carry out a visual check that the reprojection was carried out properly. da_reproj = reproj_to_xarray ( da , df_new_coords [ 'x' ], df_new_coords [ 'y' ], new_grid ) # Plotting fig = plt . figure ( dpi = 250 , figsize = ( 10 , 10 )) ax = plt . axes ( projection = ccrs . TransverseMercator ()) da_reproj . plot . imshow ( ax = ax , cmap = 'Greys_r' ) ax . coastlines ( resolution = '50m' , alpha = 0.8 , color = 'white' ) <ipython-input-37-c765a7c3ab68>:5: UserWarning: The default value for the *approx* keyword argument to TransverseMercator will change from True to False after 0.18. ax = plt.axes(projection=ccrs.TransverseMercator()) <cartopy.mpl.feature_artist.FeatureArtist at 0x1912ac7b040> #exports def full_scene_pyresample ( native_fp ): # Loading scene scene = load_scene ( native_fp ) dataset_names = scene . all_dataset_names () scene . load ( dataset_names ) # Constructing target area definition tm_area_def = construct_TM_area_def ( scene ) # Reprojecting reproj_vars = list () for dataset_name in dataset_names : da = scene [ dataset_name ] . sortby ( 'y' , ascending = False ) . sortby ( 'x' ) num_y_pixels , num_x_pixels = da . shape seviri_area_def = get_seviri_area_def ( native_fp , num_x_pixels = num_x_pixels , num_y_pixels = num_y_pixels ) resampler = satpy . resample . KDTreeResampler ( seviri_area_def , tm_area_def ) da_reproj = resampler . resample ( da ) reproj_vars += [ da_reproj ] variable_idx = pd . Index ( dataset_names , name = 'variable' ) ds_reproj = ( xr . concat ( reproj_vars , dim = variable_idx ) . to_dataset ( name = 'stacked_eumetsat_data' ) . drop ( labels = 'crs' ) ) return ds_reproj def full_scene_pyinterp ( native_fp , new_x_coords , new_y_coords , new_grid_fp ): # Loading data scene = load_scene ( native_fp ) dataset_names = scene . all_dataset_names () scene . load ( dataset_names ) with open ( new_grid_fp , 'r' ) as fp : new_grid = json . load ( fp ) # Correcting x coordinates seviri_area_def = get_seviri_area_def ( native_fp ) area_extent = seviri_area_def . area_extent x_offset = calculate_x_offset ( native_fp ) width = scene [ 'HRV' ] . x . size corrected_x_coords = np . linspace ( area_extent [ 2 ], area_extent [ 0 ], width ) scene [ 'HRV' ] = scene [ 'HRV' ] . assign_coords ({ 'x' : corrected_x_coords }) # Reprojecting reproj_vars = list () for dataset_name in dataset_names : da_reproj = reproj_to_xarray ( scene [ dataset_name ], new_x_coords , new_y_coords , new_grid ) reproj_vars += [ da_reproj ] variable_idx = pd . Index ( dataset_names , name = 'variable' ) ds_reproj = xr . concat ( reproj_vars , dim = variable_idx ) . to_dataset ( name = 'stacked_eumetsat_data' ) return ds_reproj class Reprojector : def __init__ ( self , new_coords_fp = None , new_grid_fp = None ): if new_coords_fp is None and new_grid_fp is None : return df_new_coords = pd . read_csv ( new_coords_fp ) self . new_x_coords = df_new_coords [ 'x' ] self . new_y_coords = df_new_coords [ 'y' ] self . new_grid_fp = new_grid_fp return def reproject ( self , native_fp , reproj_library = 'pyresample' ): if reproj_library == 'pyinterp' : ds_reproj = full_scene_pyinterp ( native_fp , self . new_x_coords , self . new_y_coords , self . new_grid_fp ) elif reproj_library == 'pyresample' : ds_reproj = full_scene_pyresample ( native_fp ) else : raise ValueError ( f '`reproj_library` must be one of: pyresample, pyinterp. { reproj_library } can not be passed.' ) return ds_reproj %% capture -- no - stdout %% timeit new_coords_fp = f ' { intermediate_data_dir } /reproj_coords_TM_4km.csv' new_grid_fp = '../data/intermediate/new_grid_4km_TM.json' reprojector = Reprojector ( new_coords_fp , new_grid_fp ) ds_reproj = reprojector . reproject ( native_fp , reproj_library = 'pyinterp' ) 6.06 s \u00c2\u00b1 528 ms per loop (mean \u00c2\u00b1 std. dev. of 7 runs, 1 loop each) %% capture -- no - stdout %% timeit reprojector = Reprojector () ds_reproj = reprojector . reproject ( native_fp , reproj_library = 'pyresample' ) 4.9 s \u00c2\u00b1 590 ms per loop (mean \u00c2\u00b1 std. dev. of 7 runs, 1 loop each) ds_reproj = reprojector . reproject ( native_fp ) # Plotting fig = plt . figure ( dpi = 250 , figsize = ( 10 , 10 )) ax = plt . axes ( projection = ccrs . TransverseMercator ()) ds_reproj [ 'stacked_eumetsat_data' ] . sel ( variable = 'HRV' ) . plot . imshow ( ax = ax , cmap = 'Greys_r' ) ax . coastlines ( resolution = '50m' , alpha = 0.8 , color = 'white' ) C:\\Users\\Ayrto\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\pyproj\\crs\\crs.py:543: UserWarning: You will likely lose important projection information when converting to a PROJ string from another format. See: https://proj.org/faq.html#what-is-the-best-format-for-describing-coordinate-reference-systems proj_string = self.to_proj4() C:\\Users\\Ayrto\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\pyproj\\crs\\crs.py:543: UserWarning: You will likely lose important projection information when converting to a PROJ string from another format. See: https://proj.org/faq.html#what-is-the-best-format-for-describing-coordinate-reference-systems proj_string = self.to_proj4() C:\\Users\\Ayrto\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\pyproj\\crs\\crs.py:543: UserWarning: You will likely lose important projection information when converting to a PROJ string from another format. See: https://proj.org/faq.html#what-is-the-best-format-for-describing-coordinate-reference-systems proj_string = self.to_proj4() C:\\Users\\Ayrto\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\pyproj\\crs\\crs.py:543: UserWarning: You will likely lose important projection information when converting to a PROJ string from another format. See: https://proj.org/faq.html#what-is-the-best-format-for-describing-coordinate-reference-systems proj_string = self.to_proj4() C:\\Users\\Ayrto\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\pyproj\\crs\\crs.py:543: UserWarning: You will likely lose important projection information when converting to a PROJ string from another format. See: https://proj.org/faq.html#what-is-the-best-format-for-describing-coordinate-reference-systems proj_string = self.to_proj4() C:\\Users\\Ayrto\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\pyproj\\crs\\crs.py:543: UserWarning: You will likely lose important projection information when converting to a PROJ string from another format. See: https://proj.org/faq.html#what-is-the-best-format-for-describing-coordinate-reference-systems proj_string = self.to_proj4() C:\\Users\\Ayrto\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\pyproj\\crs\\crs.py:543: UserWarning: You will likely lose important projection information when converting to a PROJ string from another format. See: https://proj.org/faq.html#what-is-the-best-format-for-describing-coordinate-reference-systems proj_string = self.to_proj4() C:\\Users\\Ayrto\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\pyproj\\crs\\crs.py:543: UserWarning: You will likely lose important projection information when converting to a PROJ string from another format. See: https://proj.org/faq.html#what-is-the-best-format-for-describing-coordinate-reference-systems proj_string = self.to_proj4() C:\\Users\\Ayrto\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\pyproj\\crs\\crs.py:543: UserWarning: You will likely lose important projection information when converting to a PROJ string from another format. See: https://proj.org/faq.html#what-is-the-best-format-for-describing-coordinate-reference-systems proj_string = self.to_proj4() C:\\Users\\Ayrto\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\pyproj\\crs\\crs.py:543: UserWarning: You will likely lose important projection information when converting to a PROJ string from another format. See: https://proj.org/faq.html#what-is-the-best-format-for-describing-coordinate-reference-systems proj_string = self.to_proj4() C:\\Users\\Ayrto\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\pyproj\\crs\\crs.py:543: UserWarning: You will likely lose important projection information when converting to a PROJ string from another format. See: https://proj.org/faq.html#what-is-the-best-format-for-describing-coordinate-reference-systems proj_string = self.to_proj4() C:\\Users\\Ayrto\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\pyproj\\crs\\crs.py:543: UserWarning: You will likely lose important projection information when converting to a PROJ string from another format. See: https://proj.org/faq.html#what-is-the-best-format-for-describing-coordinate-reference-systems proj_string = self.to_proj4() C:\\Users\\Ayrto\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\pyproj\\crs\\crs.py:543: UserWarning: You will likely lose important projection information when converting to a PROJ string from another format. See: https://proj.org/faq.html#what-is-the-best-format-for-describing-coordinate-reference-systems proj_string = self.to_proj4() C:\\Users\\Ayrto\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\numpy\\lib\\function_base.py:1280: RuntimeWarning: invalid value encountered in subtract a = op(a[slice1], a[slice2]) C:\\Users\\Ayrto\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\pyproj\\crs\\crs.py:543: UserWarning: You will likely lose important projection information when converting to a PROJ string from another format. See: https://proj.org/faq.html#what-is-the-best-format-for-describing-coordinate-reference-systems proj_string = self.to_proj4() C:\\Users\\Ayrto\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\pyproj\\crs\\crs.py:543: UserWarning: You will likely lose important projection information when converting to a PROJ string from another format. See: https://proj.org/faq.html#what-is-the-best-format-for-describing-coordinate-reference-systems proj_string = self.to_proj4() C:\\Users\\Ayrto\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\numpy\\lib\\function_base.py:1280: RuntimeWarning: invalid value encountered in subtract a = op(a[slice1], a[slice2]) C:\\Users\\Ayrto\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\pyproj\\crs\\crs.py:543: UserWarning: You will likely lose important projection information when converting to a PROJ string from another format. See: https://proj.org/faq.html#what-is-the-best-format-for-describing-coordinate-reference-systems proj_string = self.to_proj4() C:\\Users\\Ayrto\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\numpy\\lib\\function_base.py:1280: RuntimeWarning: invalid value encountered in subtract a = op(a[slice1], a[slice2]) C:\\Users\\Ayrto\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\pyproj\\crs\\crs.py:543: UserWarning: You will likely lose important projection information when converting to a PROJ string from another format. See: https://proj.org/faq.html#what-is-the-best-format-for-describing-coordinate-reference-systems proj_string = self.to_proj4() C:\\Users\\Ayrto\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\numpy\\lib\\function_base.py:1280: RuntimeWarning: invalid value encountered in subtract a = op(a[slice1], a[slice2]) C:\\Users\\Ayrto\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\pyproj\\crs\\crs.py:543: UserWarning: You will likely lose important projection information when converting to a PROJ string from another format. See: https://proj.org/faq.html#what-is-the-best-format-for-describing-coordinate-reference-systems proj_string = self.to_proj4() C:\\Users\\Ayrto\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\numpy\\lib\\function_base.py:1280: RuntimeWarning: invalid value encountered in subtract a = op(a[slice1], a[slice2]) C:\\Users\\Ayrto\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\pyproj\\crs\\crs.py:543: UserWarning: You will likely lose important projection information when converting to a PROJ string from another format. See: https://proj.org/faq.html#what-is-the-best-format-for-describing-coordinate-reference-systems proj_string = self.to_proj4() C:\\Users\\Ayrto\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\numpy\\lib\\function_base.py:1280: RuntimeWarning: invalid value encountered in subtract a = op(a[slice1], a[slice2]) C:\\Users\\Ayrto\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\pyproj\\crs\\crs.py:543: UserWarning: You will likely lose important projection information when converting to a PROJ string from another format. See: https://proj.org/faq.html#what-is-the-best-format-for-describing-coordinate-reference-systems proj_string = self.to_proj4() C:\\Users\\Ayrto\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\numpy\\lib\\function_base.py:1280: RuntimeWarning: invalid value encountered in subtract a = op(a[slice1], a[slice2]) C:\\Users\\Ayrto\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\pyproj\\crs\\crs.py:543: UserWarning: You will likely lose important projection information when converting to a PROJ string from another format. See: https://proj.org/faq.html#what-is-the-best-format-for-describing-coordinate-reference-systems proj_string = self.to_proj4() C:\\Users\\Ayrto\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\numpy\\lib\\function_base.py:1280: RuntimeWarning: invalid value encountered in subtract a = op(a[slice1], a[slice2]) C:\\Users\\Ayrto\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\pyproj\\crs\\crs.py:543: UserWarning: You will likely lose important projection information when converting to a PROJ string from another format. See: https://proj.org/faq.html#what-is-the-best-format-for-describing-coordinate-reference-systems proj_string = self.to_proj4() C:\\Users\\Ayrto\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\numpy\\lib\\function_base.py:1280: RuntimeWarning: invalid value encountered in subtract a = op(a[slice1], a[slice2]) C:\\Users\\Ayrto\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\pyproj\\crs\\crs.py:543: UserWarning: You will likely lose important projection information when converting to a PROJ string from another format. See: https://proj.org/faq.html#what-is-the-best-format-for-describing-coordinate-reference-systems proj_string = self.to_proj4() C:\\Users\\Ayrto\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\numpy\\lib\\function_base.py:1280: RuntimeWarning: invalid value encountered in subtract a = op(a[slice1], a[slice2]) C:\\Users\\Ayrto\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\pyproj\\crs\\crs.py:543: UserWarning: You will likely lose important projection information when converting to a PROJ string from another format. See: https://proj.org/faq.html#what-is-the-best-format-for-describing-coordinate-reference-systems proj_string = self.to_proj4() C:\\Users\\Ayrto\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\numpy\\lib\\function_base.py:1280: RuntimeWarning: invalid value encountered in subtract a = op(a[slice1], a[slice2]) C:\\Users\\Ayrto\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\pyproj\\crs\\crs.py:543: UserWarning: You will likely lose important projection information when converting to a PROJ string from another format. See: https://proj.org/faq.html#what-is-the-best-format-for-describing-coordinate-reference-systems proj_string = self.to_proj4() C:\\Users\\Ayrto\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\numpy\\lib\\function_base.py:1280: RuntimeWarning: invalid value encountered in subtract a = op(a[slice1], a[slice2]) <ipython-input-42-4aa2b08f07bf>:5: UserWarning: The default value for the *approx* keyword argument to TransverseMercator will change from True to False after 0.18. ax = plt.axes(projection=ccrs.TransverseMercator()) C:\\Users\\Ayrto\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\dask\\core.py:121: RuntimeWarning: invalid value encountered in sin return func(*(_execute_task(a, cache) for a in args)) C:\\Users\\Ayrto\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\dask\\core.py:121: RuntimeWarning: invalid value encountered in cos return func(*(_execute_task(a, cache) for a in args)) <cartopy.mpl.feature_artist.FeatureArtist at 0x19102ac7430>","title":"Comparing Reprojection Libraries"},{"location":"03_zarr/","text":"Zarr \u00b6 C:\\Users\\Ayrto\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\google\\auth\\_default.py:69: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK without a quota project. You might receive a \"quota exceeded\" or \"API not enabled\" error. We recommend you rerun `gcloud auth application-default login` and make sure a quota project is added. Or you can use service accounts instead. For more information about service accounts, see https://cloud.google.com/docs/authentication/ warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING) Downloading: 100%|\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6| 1/1 [00:00<00:00, 3.31rows/s] import os import dotenv import matplotlib.pyplot as plt import cartopy.crs as ccrs User Inputs \u00b6 data_dir = '../data/raw' metadata_db_fp = '../data/EUMETSAT_metadata.db' debug_fp = '../logs/EUMETSAT_download.txt' new_grid_fp = '../data/intermediate/new_grid_4km_TM.json' new_coords_fp = '../data/intermediate/reproj_coords_TM_4km.csv' in_zarr_bucket = 'solar-pv-nowcasting-data/satellite/EUMETSAT/SEVIRI_RSS/OSGB36/all_zarr' out_zarr_bucket = 'solar-pv-nowcasting-data/satellite/EUMETSAT/SEVIRI_RSS/full_extent_TM_int16' Loading Environment Variables \u00b6 dotenv . load_dotenv ( '../.env' ) user_key = os . environ . get ( 'USER_KEY' ) user_secret = os . environ . get ( 'USER_SECRET' ) slack_id = os . environ . get ( 'SLACK_ID' ) slack_webhook_url = os . environ . get ( 'SLACK_WEBHOOK_URL' ) Preparing Data to Save to Zarr \u00b6 We'll start by loading in one of the datasets we've just downloaded, in this instance we'll take the most recent one by identifying it from the metadata db. dm = eumetsat.DownloadManager(user_key, user_secret, data_dir, metadata_db_fp, debug_fp) df_metadata = dm.get_df_metadata() df_metadata.tail() We'll then load in the file filename = df_metadata . loc [ df_metadata . index [ - 2 ], 'file_name' ] native_fp = f ' { data_dir } / { filename } .nat' severi_area_def = reproj . get_seviri_area_def ( native_fp ) seviri_crs = severi_area_def . to_cartopy_crs () scene = reproj . load_scene ( native_fp ) scene . load ([ 'HRV' ]) C:\\Users\\Ayrto\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\pyproj\\crs\\crs.py:543: UserWarning: You will likely lose important projection information when converting to a PROJ string from another format. See: https://proj.org/faq.html#what-is-the-best-format-for-describing-coordinate-reference-systems proj_string = self.to_proj4() C:\\Users\\Ayrto\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\pyproj\\crs\\crs.py:543: UserWarning: You will likely lose important projection information when converting to a PROJ string from another format. See: https://proj.org/faq.html#what-is-the-best-format-for-describing-coordinate-reference-systems proj_string = self.to_proj4() And visualise it to test that everything is working fig = plt . figure ( dpi = 250 , figsize = ( 10 , 10 )) ax = plt . axes ( projection = seviri_crs ) scene [ 'HRV' ] . plot . imshow ( ax = ax , cmap = 'magma' , vmin = 0 , vmax = 50 ) ax . coastlines ( resolution = '50m' , alpha = 0.8 , color = 'white' ) <cartopy.mpl.feature_artist.FeatureArtist at 0x292182c9a30> We now need to reproject it %% capture -- no - stdout %% time reprojector = reproj . Reprojector ( new_coords_fp , new_grid_fp ) ds_reproj = reprojector . reproject ( native_fp , reproj_library = 'pyresample' ) Wall time: 4.59 s Which again we'll check through visualisation fig = plt . figure ( dpi = 250 , figsize = ( 10 , 10 )) ax = plt . axes ( projection = ccrs . TransverseMercator ()) ds_reproj [ 'stacked_eumetsat_data' ] . sel ( variable = 'HRV' ) . plot . imshow ( ax = ax , cmap = 'magma' , vmin = 0 , vmax = 50 ) ax . coastlines ( resolution = '50m' , alpha = 0.8 , color = 'white' ) <ipython-input-67-53f177b0781d>:2: UserWarning: The default value for the *approx* keyword argument to TransverseMercator will change from True to False after 0.18. ax = plt.axes(projection=ccrs.TransverseMercator()) C:\\Users\\Ayrto\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\dask\\core.py:121: RuntimeWarning: invalid value encountered in cos return func(*(_execute_task(a, cache) for a in args)) C:\\Users\\Ayrto\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\dask\\core.py:121: RuntimeWarning: invalid value encountered in sin return func(*(_execute_task(a, cache) for a in args)) <cartopy.mpl.feature_artist.FeatureArtist at 0x292178ed7f0> Compressing \u00b6 We'll now develop our compressor class that will reduce the size of the datasets that we save to Zarr, in this instance we'll normalize the data and transform it to Int16. This has been found to reduce the size by ~50%. #exports def add_constant_coord_to_da ( da , coord_name , coord_val ): \"\"\" Adds a new coordinate with a constant value to the DataArray Parameters ---------- da : xr.DataArray DataArrray which will have the new coords added to it coord_name : str Name for the new coordinate dimensions coord_val Value that will be assigned to the new coordinates Returns ------- da : xr.DataArray DataArrray with the new coords added to it \"\"\" da = ( da . assign_coords ({ coord_name : coord_val }) . expand_dims ( coord_name ) ) return da class Compressor : def __init__ ( self , bits_per_pixel = 10 , mins = np . array ([ - 1.2278595 , - 2.5118103 , - 64.83977 , 63.404694 , 2.844452 , 199.10002 , - 17.254883 , - 26.29155 , - 1.1009827 , - 2.4184198 , 199.57048 , 198.95093 ]), maxs = np . array ([ 103.90016 , 69.60857 , 339.15588 , 340.26526 , 317.86752 , 313.2767 , 315.99194 , 274.82297 , 93.786545 , 101.34922 , 249.91806 , 286.96323 ]), variable_order = [ 'HRV' , 'IR_016' , 'IR_039' , 'IR_087' , 'IR_097' , 'IR_108' , 'IR_120' , 'IR_134' , 'VIS006' , 'VIS008' , 'WV_062' , 'WV_073' ] ): locals_ = locals () attrs_to_add = [ 'bits_per_pixel' , 'mins' , 'maxs' , 'variable_order' ] for attr in attrs_to_add : setattr ( self , attr , locals_ [ attr ]) return def fit ( self , da , dims = [ 'time' , 'y' , 'x' ]): self . mins = da . min ( dims ) . compute () self . maxs = da . max ( dims ) . compute () self . variable_order = da . coords [ 'variable' ] . values print ( f 'The mins are: { self . mins } ' ) print ( f 'The maxs are: { self . maxs } ' ) print ( f 'The variable order is: { self . variable_order } ' ) return def compress ( self , da ): da_meta = da . attrs for attr in [ 'mins' , 'maxs' ]: assert getattr ( self , attr ) is not None , f ' { attr } must be set in initialisation or through `fit`' if 'time' not in da . dims : time = pd . to_datetime ( da_meta [ 'end_time' ]) da = add_constant_coord_to_da ( da , 'time' , time ) da = ( da . reindex ({ 'variable' : self . variable_order }) . transpose ( 'time' , 'y' , 'x' , 'variable' ) ) upper_bound = ( 2 ** self . bits_per_pixel ) - 1 new_max = self . maxs - self . mins da -= self . mins da /= new_max da *= upper_bound da = ( da . fillna ( - 1 ) . round () . astype ( np . int16 ) ) da . attrs = { 'meta' : str ( da_meta )} # Must be serialisable return da %% time compressor = Compressor () da_compressed = compressor . compress ( ds_reproj [ 'stacked_eumetsat_data' ]) 2020-12-15 12:20:10.182808 Wall time: 19 ms Saving to Zarr \u00b6 We'll now create a helper function for saving the data-array to a zarr database # exports get_time_as_unix = lambda da : pd . Series (( pd . to_datetime ( da . time . values ) - pd . Timestamp ( '1970-01-01' )) . total_seconds ()) . astype ( int ) . values def save_da_to_zarr ( da , zarr_bucket , dim_order = [ 'time' , 'x' , 'y' , 'variable' ], zarr_mode = 'a' ): da = da . transpose ( * dim_order ) da [ 'time' ] = get_time_as_unix ( da ) _ , y_size , x_size , _ = da . shape out_store = gcsfs . GCSMap ( root = zarr_bucket , gcs = gcsfs . GCSFileSystem ()) chunks = ( 36 , y_size , x_size , 1 ) ds = xr . Dataset ({ 'stacked_eumetsat_data' : da . chunk ( chunks )}) zarr_mode_to_extra_kwargs = { 'a' : { 'append_dim' : 'time' }, 'w' : { 'encoding' : { 'stacked_eumetsat_data' : { 'compressor' : numcodecs . Blosc ( cname = 'zstd' , clevel = 5 ), 'chunks' : chunks } } } } assert zarr_mode in [ 'a' , 'w' ], '`zarr_mode` must be one of: `a`, `w`' extra_kwargs = zarr_mode_to_extra_kwargs [ zarr_mode ] ds . to_zarr ( out_store , mode = zarr_mode , consolidated = True , ** extra_kwargs ) return ds Now we can save it! out_zarr_bucket = 'solar-pv-nowcasting-data/satellite/EUMETSAT/SEVIRI_RSS/full_extent_TM_int16' ds = save_da_to_zarr ( da_compressed , out_zarr_bucket , zarr_mode = 'w' ) C:\\Users\\Ayrto\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\dask\\core.py:121: RuntimeWarning: invalid value encountered in sin return func(*(_execute_task(a, cache) for a in args)) C:\\Users\\Ayrto\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\dask\\core.py:121: RuntimeWarning: invalid value encountered in cos return func(*(_execute_task(a, cache) for a in args)) Loading Zarr Data \u00b6 We'll start by defining a loading function and a replacement for the standard gcsfs.utils is_retriable function We'll now read it in %% time loaded_xarray = load_from_zarr_bucket ( out_zarr_bucket ) loaded_xarray . time . compute () Wall time: 2.27 s /* CSS stylesheet for displaying xarray objects in jupyterlab. * */ :root { --xr-font-color0: var(--jp-content-font-color0, rgba(0, 0, 0, 1)); --xr-font-color2: var(--jp-content-font-color2, rgba(0, 0, 0, 0.54)); --xr-font-color3: var(--jp-content-font-color3, rgba(0, 0, 0, 0.38)); --xr-border-color: var(--jp-border-color2, #e0e0e0); --xr-disabled-color: var(--jp-layout-color3, #bdbdbd); --xr-background-color: var(--jp-layout-color0, white); --xr-background-color-row-even: var(--jp-layout-color1, white); --xr-background-color-row-odd: var(--jp-layout-color2, #eeeeee); } html[theme=dark], body.vscode-dark { --xr-font-color0: rgba(255, 255, 255, 1); --xr-font-color2: rgba(255, 255, 255, 0.54); --xr-font-color3: rgba(255, 255, 255, 0.38); --xr-border-color: #1F1F1F; --xr-disabled-color: #515151; --xr-background-color: #111111; --xr-background-color-row-even: #111111; --xr-background-color-row-odd: #313131; } .xr-wrap { display: block; min-width: 300px; max-width: 700px; } .xr-text-repr-fallback { /* fallback to plain text repr when CSS is not injected (untrusted notebook) */ display: none; } .xr-header { padding-top: 6px; padding-bottom: 6px; margin-bottom: 4px; border-bottom: solid 1px var(--xr-border-color); } .xr-header > div, .xr-header > ul { display: inline; margin-top: 0; margin-bottom: 0; } .xr-obj-type, .xr-array-name { margin-left: 2px; margin-right: 10px; } .xr-obj-type { color: var(--xr-font-color2); } .xr-sections { padding-left: 0 !important; display: grid; grid-template-columns: 150px auto auto 1fr 20px 20px; } .xr-section-item { display: contents; } .xr-section-item input { display: none; } .xr-section-item input + label { color: var(--xr-disabled-color); } .xr-section-item input:enabled + label { cursor: pointer; color: var(--xr-font-color2); } .xr-section-item input:enabled + label:hover { color: var(--xr-font-color0); } .xr-section-summary { grid-column: 1; color: var(--xr-font-color2); font-weight: 500; } .xr-section-summary > span { display: inline-block; padding-left: 0.5em; } .xr-section-summary-in:disabled + label { color: var(--xr-font-color2); } .xr-section-summary-in + label:before { display: inline-block; content: '\u00e2\u2013\u00ba'; font-size: 11px; width: 15px; text-align: center; } .xr-section-summary-in:disabled + label:before { color: var(--xr-disabled-color); } .xr-section-summary-in:checked + label:before { content: '\u00e2\u2013\u00bc'; } .xr-section-summary-in:checked + label > span { display: none; } .xr-section-summary, .xr-section-inline-details { padding-top: 4px; padding-bottom: 4px; } .xr-section-inline-details { grid-column: 2 / -1; } .xr-section-details { display: none; grid-column: 1 / -1; margin-bottom: 5px; } .xr-section-summary-in:checked ~ .xr-section-details { display: contents; } .xr-array-wrap { grid-column: 1 / -1; display: grid; grid-template-columns: 20px auto; } .xr-array-wrap > label { grid-column: 1; vertical-align: top; } .xr-preview { color: var(--xr-font-color3); } .xr-array-preview, .xr-array-data { padding: 0 5px !important; grid-column: 2; } .xr-array-data, .xr-array-in:checked ~ .xr-array-preview { display: none; } .xr-array-in:checked ~ .xr-array-data, .xr-array-preview { display: inline-block; } .xr-dim-list { display: inline-block !important; list-style: none; padding: 0 !important; margin: 0; } .xr-dim-list li { display: inline-block; padding: 0; margin: 0; } .xr-dim-list:before { content: '('; } .xr-dim-list:after { content: ')'; } .xr-dim-list li:not(:last-child):after { content: ','; padding-right: 5px; } .xr-has-index { font-weight: bold; } .xr-var-list, .xr-var-item { display: contents; } .xr-var-item > div, .xr-var-item label, .xr-var-item > .xr-var-name span { background-color: var(--xr-background-color-row-even); margin-bottom: 0; } .xr-var-item > .xr-var-name:hover span { padding-right: 5px; } .xr-var-list > li:nth-child(odd) > div, .xr-var-list > li:nth-child(odd) > label, .xr-var-list > li:nth-child(odd) > .xr-var-name span { background-color: var(--xr-background-color-row-odd); } .xr-var-name { grid-column: 1; } .xr-var-dims { grid-column: 2; } .xr-var-dtype { grid-column: 3; text-align: right; color: var(--xr-font-color2); } .xr-var-preview { grid-column: 4; } .xr-var-name, .xr-var-dims, .xr-var-dtype, .xr-preview, .xr-attrs dt { white-space: nowrap; overflow: hidden; text-overflow: ellipsis; padding-right: 10px; } .xr-var-name:hover, .xr-var-dims:hover, .xr-var-dtype:hover, .xr-attrs dt:hover { overflow: visible; width: auto; z-index: 1; } .xr-var-attrs, .xr-var-data { display: none; background-color: var(--xr-background-color) !important; padding-bottom: 5px !important; } .xr-var-attrs-in:checked ~ .xr-var-attrs, .xr-var-data-in:checked ~ .xr-var-data { display: block; } .xr-var-data > table { float: right; } .xr-var-name span, .xr-var-data, .xr-attrs { padding-left: 25px !important; } .xr-attrs, .xr-var-attrs, .xr-var-data { grid-column: 1 / -1; } dl.xr-attrs { padding: 0; margin: 0; display: grid; grid-template-columns: 125px auto; } .xr-attrs dt, .xr-attrs dd { padding: 0; margin: 0; float: left; padding-right: 10px; width: auto; } .xr-attrs dt { font-weight: normal; grid-column: 1; } .xr-attrs dt:hover span { display: inline-block; background: var(--xr-background-color); padding-right: 10px; } .xr-attrs dd { grid-column: 2; white-space: pre-wrap; word-break: break-all; } .xr-icon-database, .xr-icon-file-text2 { display: inline-block; vertical-align: middle; width: 1em; height: 1.5em !important; stroke-width: 0; stroke: currentColor; fill: currentColor; } <xarray.DataArray 'time' (time: 26)> array(['2020-12-16T15:19:15.000000000', '2020-12-16T15:24:16.000000000', '2020-12-16T15:29:17.000000000', '2020-12-16T15:34:18.000000000', '2020-12-16T15:39:18.000000000', '2020-12-16T15:44:18.000000000', '2020-12-16T15:49:18.000000000', '2020-12-16T15:54:16.000000000', '2020-12-16T15:59:15.000000000', '2020-12-16T16:04:13.000000000', '2020-12-16T16:14:13.000000000', '2020-12-16T16:19:13.000000000', '2020-12-16T16:24:14.000000000', '2020-12-16T16:29:15.000000000', '2020-12-16T16:39:16.000000000', '2020-12-16T16:44:16.000000000', '2020-12-16T16:49:16.000000000', '2020-12-16T16:54:16.000000000', '2020-12-16T16:59:15.000000000', '2020-12-16T17:04:15.000000000', '2020-12-16T17:09:15.000000000', '2020-12-16T17:19:15.000000000', '2020-12-16T17:24:16.000000000', '2020-12-16T17:29:17.000000000', '2020-12-16T17:34:18.000000000', '2020-12-16T17:39:18.000000000'], dtype='datetime64[ns]') Coordinates: * time (time) datetime64[ns] 2020-12-16T15:19:15 ... 2020-12-16T17:39:18 xarray.DataArray 'time' time : 26 2020-12-16T15:19:15 2020-12-16T15:24:16 ... 2020-12-16T17:39:18 array(['2020-12-16T15:19:15.000000000', '2020-12-16T15:24:16.000000000', '2020-12-16T15:29:17.000000000', '2020-12-16T15:34:18.000000000', '2020-12-16T15:39:18.000000000', '2020-12-16T15:44:18.000000000', '2020-12-16T15:49:18.000000000', '2020-12-16T15:54:16.000000000', '2020-12-16T15:59:15.000000000', '2020-12-16T16:04:13.000000000', '2020-12-16T16:14:13.000000000', '2020-12-16T16:19:13.000000000', '2020-12-16T16:24:14.000000000', '2020-12-16T16:29:15.000000000', '2020-12-16T16:39:16.000000000', '2020-12-16T16:44:16.000000000', '2020-12-16T16:49:16.000000000', '2020-12-16T16:54:16.000000000', '2020-12-16T16:59:15.000000000', '2020-12-16T17:04:15.000000000', '2020-12-16T17:09:15.000000000', '2020-12-16T17:19:15.000000000', '2020-12-16T17:24:16.000000000', '2020-12-16T17:29:17.000000000', '2020-12-16T17:34:18.000000000', '2020-12-16T17:39:18.000000000'], dtype='datetime64[ns]') Coordinates: (1) time (time) datetime64[ns] 2020-12-16T15:19:15 ... 2020-12-... array(['2020-12-16T15:19:15.000000000', '2020-12-16T15:24:16.000000000', '2020-12-16T15:29:17.000000000', '2020-12-16T15:34:18.000000000', '2020-12-16T15:39:18.000000000', '2020-12-16T15:44:18.000000000', '2020-12-16T15:49:18.000000000', '2020-12-16T15:54:16.000000000', '2020-12-16T15:59:15.000000000', '2020-12-16T16:04:13.000000000', '2020-12-16T16:14:13.000000000', '2020-12-16T16:19:13.000000000', '2020-12-16T16:24:14.000000000', '2020-12-16T16:29:15.000000000', '2020-12-16T16:39:16.000000000', '2020-12-16T16:44:16.000000000', '2020-12-16T16:49:16.000000000', '2020-12-16T16:54:16.000000000', '2020-12-16T16:59:15.000000000', '2020-12-16T17:04:15.000000000', '2020-12-16T17:09:15.000000000', '2020-12-16T17:19:15.000000000', '2020-12-16T17:24:16.000000000', '2020-12-16T17:29:17.000000000', '2020-12-16T17:34:18.000000000', '2020-12-16T17:39:18.000000000'], dtype='datetime64[ns]') Attributes: (0) fig = plt . figure ( dpi = 250 , figsize = ( 10 , 10 )) ax = plt . axes ( projection = ccrs . TransverseMercator ()) loaded_xarray [ 'stacked_eumetsat_data' ] . isel ( variable = 0 , time = 0 ) . T . plot ( ax = ax , cmap = 'magma' , vmin =- 200 , vmax = 400 ) ax . coastlines ( resolution = '50m' , alpha = 0.8 , color = 'white' ) <ipython-input-82-f7e189d5f897>:2: UserWarning: The default value for the *approx* keyword argument to TransverseMercator will change from True to False after 0.18. ax = plt.axes(projection=ccrs.TransverseMercator()) <cartopy.mpl.feature_artist.FeatureArtist at 0x2925b42afa0> # Disk space is priority, then look at io speeds # # Priority order # * notebook examples # * airflow # * end-to-end pipeline with benchmarking # * publish documentation # * add more tests Brief - [x] Tweak Zarr data types to minimise storage space (e.g. re-scale all channels to [0, 1023] and store as int16s, with missing values encoded as -1; and compressed using zstd level 5). (To re-scale to [0, 1023], you'll need to know the min and max values! I've computed the mins and maxes for as 12 channels from James Fulton's 'all_zarr' array. The mins and maxes are listed here. Not sure if these mins and maxes will be the same after you've re-projected the entire geographical scope though. - [x] Each Zarr chunk could be something like 32 timesteps, 512 pixels wide, 512 pixels high, and 1 channel. (We're not sure how many channels will be useful for PV nowcasting. So, it's probably best to keep the channels in separate chunks.) - [x] Would be great to use all the CPU cores on the VM. If the reprojection library doesn't do this itself, then maybe consider spinning up multiple processes, maybe using concurrent.futures.ProcessPoolExecutor.map(worker_func, [list of native files to re-projected]); or using dask.delayed. Zarr supports parallel writes as long as the writes are to different chunks (see Zarr's docs on parallel processing). - [x] One challenge will be that, if each Zarr chunk is multiple timesteps, then the conversion script may have to load multiple timesteps in memory, or in temporary files on the VM, before concatenating multiple timesteps into one Zarr chunk. - [ ] When the script is finished processing a Native file, maybe we should move the native file to a cheaper storage class (e.g. coldline storage or archive storage). WMS Notes \u00b6 https://en.wikipedia.org/wiki/Tiled_web_map Conventions: * Tiles are 256x256 pixels * At the outer most zoom level, 0, the entire world can be rendered in a single map tile. * Each zoom level doubles in both dimensions, so a single tile is replaced by 4 tiles when zooming in. This means that about 22 zoom levels are sufficient for most practical purposes. * The Web Mercator projection is used, with latitude limits of around 85 degrees. * An X and Y numbering scheme * PNG images for tiles * Images are served through a Web server, with a URL like http://.../Z/X/Y.png, where Z is the zoom level, and X and Y identify the tile. * Tile Map Service: (0 to 2zoom-1, 2zoom-1 to 0) for the range (-180, +85.0511) - (+180, -85.0511).","title":"Saving with Zarr"},{"location":"03_zarr/#zarr","text":"C:\\Users\\Ayrto\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\google\\auth\\_default.py:69: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK without a quota project. You might receive a \"quota exceeded\" or \"API not enabled\" error. We recommend you rerun `gcloud auth application-default login` and make sure a quota project is added. Or you can use service accounts instead. For more information about service accounts, see https://cloud.google.com/docs/authentication/ warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING) Downloading: 100%|\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6| 1/1 [00:00<00:00, 3.31rows/s] import os import dotenv import matplotlib.pyplot as plt import cartopy.crs as ccrs","title":"Zarr"},{"location":"03_zarr/#user-inputs","text":"data_dir = '../data/raw' metadata_db_fp = '../data/EUMETSAT_metadata.db' debug_fp = '../logs/EUMETSAT_download.txt' new_grid_fp = '../data/intermediate/new_grid_4km_TM.json' new_coords_fp = '../data/intermediate/reproj_coords_TM_4km.csv' in_zarr_bucket = 'solar-pv-nowcasting-data/satellite/EUMETSAT/SEVIRI_RSS/OSGB36/all_zarr' out_zarr_bucket = 'solar-pv-nowcasting-data/satellite/EUMETSAT/SEVIRI_RSS/full_extent_TM_int16'","title":"User Inputs"},{"location":"03_zarr/#loading-environment-variables","text":"dotenv . load_dotenv ( '../.env' ) user_key = os . environ . get ( 'USER_KEY' ) user_secret = os . environ . get ( 'USER_SECRET' ) slack_id = os . environ . get ( 'SLACK_ID' ) slack_webhook_url = os . environ . get ( 'SLACK_WEBHOOK_URL' )","title":"Loading Environment Variables"},{"location":"03_zarr/#preparing-data-to-save-to-zarr","text":"We'll start by loading in one of the datasets we've just downloaded, in this instance we'll take the most recent one by identifying it from the metadata db. dm = eumetsat.DownloadManager(user_key, user_secret, data_dir, metadata_db_fp, debug_fp) df_metadata = dm.get_df_metadata() df_metadata.tail() We'll then load in the file filename = df_metadata . loc [ df_metadata . index [ - 2 ], 'file_name' ] native_fp = f ' { data_dir } / { filename } .nat' severi_area_def = reproj . get_seviri_area_def ( native_fp ) seviri_crs = severi_area_def . to_cartopy_crs () scene = reproj . load_scene ( native_fp ) scene . load ([ 'HRV' ]) C:\\Users\\Ayrto\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\pyproj\\crs\\crs.py:543: UserWarning: You will likely lose important projection information when converting to a PROJ string from another format. See: https://proj.org/faq.html#what-is-the-best-format-for-describing-coordinate-reference-systems proj_string = self.to_proj4() C:\\Users\\Ayrto\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\pyproj\\crs\\crs.py:543: UserWarning: You will likely lose important projection information when converting to a PROJ string from another format. See: https://proj.org/faq.html#what-is-the-best-format-for-describing-coordinate-reference-systems proj_string = self.to_proj4() And visualise it to test that everything is working fig = plt . figure ( dpi = 250 , figsize = ( 10 , 10 )) ax = plt . axes ( projection = seviri_crs ) scene [ 'HRV' ] . plot . imshow ( ax = ax , cmap = 'magma' , vmin = 0 , vmax = 50 ) ax . coastlines ( resolution = '50m' , alpha = 0.8 , color = 'white' ) <cartopy.mpl.feature_artist.FeatureArtist at 0x292182c9a30> We now need to reproject it %% capture -- no - stdout %% time reprojector = reproj . Reprojector ( new_coords_fp , new_grid_fp ) ds_reproj = reprojector . reproject ( native_fp , reproj_library = 'pyresample' ) Wall time: 4.59 s Which again we'll check through visualisation fig = plt . figure ( dpi = 250 , figsize = ( 10 , 10 )) ax = plt . axes ( projection = ccrs . TransverseMercator ()) ds_reproj [ 'stacked_eumetsat_data' ] . sel ( variable = 'HRV' ) . plot . imshow ( ax = ax , cmap = 'magma' , vmin = 0 , vmax = 50 ) ax . coastlines ( resolution = '50m' , alpha = 0.8 , color = 'white' ) <ipython-input-67-53f177b0781d>:2: UserWarning: The default value for the *approx* keyword argument to TransverseMercator will change from True to False after 0.18. ax = plt.axes(projection=ccrs.TransverseMercator()) C:\\Users\\Ayrto\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\dask\\core.py:121: RuntimeWarning: invalid value encountered in cos return func(*(_execute_task(a, cache) for a in args)) C:\\Users\\Ayrto\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\dask\\core.py:121: RuntimeWarning: invalid value encountered in sin return func(*(_execute_task(a, cache) for a in args)) <cartopy.mpl.feature_artist.FeatureArtist at 0x292178ed7f0>","title":"Preparing Data to Save to Zarr"},{"location":"03_zarr/#compressing","text":"We'll now develop our compressor class that will reduce the size of the datasets that we save to Zarr, in this instance we'll normalize the data and transform it to Int16. This has been found to reduce the size by ~50%. #exports def add_constant_coord_to_da ( da , coord_name , coord_val ): \"\"\" Adds a new coordinate with a constant value to the DataArray Parameters ---------- da : xr.DataArray DataArrray which will have the new coords added to it coord_name : str Name for the new coordinate dimensions coord_val Value that will be assigned to the new coordinates Returns ------- da : xr.DataArray DataArrray with the new coords added to it \"\"\" da = ( da . assign_coords ({ coord_name : coord_val }) . expand_dims ( coord_name ) ) return da class Compressor : def __init__ ( self , bits_per_pixel = 10 , mins = np . array ([ - 1.2278595 , - 2.5118103 , - 64.83977 , 63.404694 , 2.844452 , 199.10002 , - 17.254883 , - 26.29155 , - 1.1009827 , - 2.4184198 , 199.57048 , 198.95093 ]), maxs = np . array ([ 103.90016 , 69.60857 , 339.15588 , 340.26526 , 317.86752 , 313.2767 , 315.99194 , 274.82297 , 93.786545 , 101.34922 , 249.91806 , 286.96323 ]), variable_order = [ 'HRV' , 'IR_016' , 'IR_039' , 'IR_087' , 'IR_097' , 'IR_108' , 'IR_120' , 'IR_134' , 'VIS006' , 'VIS008' , 'WV_062' , 'WV_073' ] ): locals_ = locals () attrs_to_add = [ 'bits_per_pixel' , 'mins' , 'maxs' , 'variable_order' ] for attr in attrs_to_add : setattr ( self , attr , locals_ [ attr ]) return def fit ( self , da , dims = [ 'time' , 'y' , 'x' ]): self . mins = da . min ( dims ) . compute () self . maxs = da . max ( dims ) . compute () self . variable_order = da . coords [ 'variable' ] . values print ( f 'The mins are: { self . mins } ' ) print ( f 'The maxs are: { self . maxs } ' ) print ( f 'The variable order is: { self . variable_order } ' ) return def compress ( self , da ): da_meta = da . attrs for attr in [ 'mins' , 'maxs' ]: assert getattr ( self , attr ) is not None , f ' { attr } must be set in initialisation or through `fit`' if 'time' not in da . dims : time = pd . to_datetime ( da_meta [ 'end_time' ]) da = add_constant_coord_to_da ( da , 'time' , time ) da = ( da . reindex ({ 'variable' : self . variable_order }) . transpose ( 'time' , 'y' , 'x' , 'variable' ) ) upper_bound = ( 2 ** self . bits_per_pixel ) - 1 new_max = self . maxs - self . mins da -= self . mins da /= new_max da *= upper_bound da = ( da . fillna ( - 1 ) . round () . astype ( np . int16 ) ) da . attrs = { 'meta' : str ( da_meta )} # Must be serialisable return da %% time compressor = Compressor () da_compressed = compressor . compress ( ds_reproj [ 'stacked_eumetsat_data' ]) 2020-12-15 12:20:10.182808 Wall time: 19 ms","title":"Compressing"},{"location":"03_zarr/#saving-to-zarr","text":"We'll now create a helper function for saving the data-array to a zarr database # exports get_time_as_unix = lambda da : pd . Series (( pd . to_datetime ( da . time . values ) - pd . Timestamp ( '1970-01-01' )) . total_seconds ()) . astype ( int ) . values def save_da_to_zarr ( da , zarr_bucket , dim_order = [ 'time' , 'x' , 'y' , 'variable' ], zarr_mode = 'a' ): da = da . transpose ( * dim_order ) da [ 'time' ] = get_time_as_unix ( da ) _ , y_size , x_size , _ = da . shape out_store = gcsfs . GCSMap ( root = zarr_bucket , gcs = gcsfs . GCSFileSystem ()) chunks = ( 36 , y_size , x_size , 1 ) ds = xr . Dataset ({ 'stacked_eumetsat_data' : da . chunk ( chunks )}) zarr_mode_to_extra_kwargs = { 'a' : { 'append_dim' : 'time' }, 'w' : { 'encoding' : { 'stacked_eumetsat_data' : { 'compressor' : numcodecs . Blosc ( cname = 'zstd' , clevel = 5 ), 'chunks' : chunks } } } } assert zarr_mode in [ 'a' , 'w' ], '`zarr_mode` must be one of: `a`, `w`' extra_kwargs = zarr_mode_to_extra_kwargs [ zarr_mode ] ds . to_zarr ( out_store , mode = zarr_mode , consolidated = True , ** extra_kwargs ) return ds Now we can save it! out_zarr_bucket = 'solar-pv-nowcasting-data/satellite/EUMETSAT/SEVIRI_RSS/full_extent_TM_int16' ds = save_da_to_zarr ( da_compressed , out_zarr_bucket , zarr_mode = 'w' ) C:\\Users\\Ayrto\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\dask\\core.py:121: RuntimeWarning: invalid value encountered in sin return func(*(_execute_task(a, cache) for a in args)) C:\\Users\\Ayrto\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\dask\\core.py:121: RuntimeWarning: invalid value encountered in cos return func(*(_execute_task(a, cache) for a in args))","title":"Saving to Zarr"},{"location":"03_zarr/#loading-zarr-data","text":"We'll start by defining a loading function and a replacement for the standard gcsfs.utils is_retriable function We'll now read it in %% time loaded_xarray = load_from_zarr_bucket ( out_zarr_bucket ) loaded_xarray . time . compute () Wall time: 2.27 s /* CSS stylesheet for displaying xarray objects in jupyterlab. * */ :root { --xr-font-color0: var(--jp-content-font-color0, rgba(0, 0, 0, 1)); --xr-font-color2: var(--jp-content-font-color2, rgba(0, 0, 0, 0.54)); --xr-font-color3: var(--jp-content-font-color3, rgba(0, 0, 0, 0.38)); --xr-border-color: var(--jp-border-color2, #e0e0e0); --xr-disabled-color: var(--jp-layout-color3, #bdbdbd); --xr-background-color: var(--jp-layout-color0, white); --xr-background-color-row-even: var(--jp-layout-color1, white); --xr-background-color-row-odd: var(--jp-layout-color2, #eeeeee); } html[theme=dark], body.vscode-dark { --xr-font-color0: rgba(255, 255, 255, 1); --xr-font-color2: rgba(255, 255, 255, 0.54); --xr-font-color3: rgba(255, 255, 255, 0.38); --xr-border-color: #1F1F1F; --xr-disabled-color: #515151; --xr-background-color: #111111; --xr-background-color-row-even: #111111; --xr-background-color-row-odd: #313131; } .xr-wrap { display: block; min-width: 300px; max-width: 700px; } .xr-text-repr-fallback { /* fallback to plain text repr when CSS is not injected (untrusted notebook) */ display: none; } .xr-header { padding-top: 6px; padding-bottom: 6px; margin-bottom: 4px; border-bottom: solid 1px var(--xr-border-color); } .xr-header > div, .xr-header > ul { display: inline; margin-top: 0; margin-bottom: 0; } .xr-obj-type, .xr-array-name { margin-left: 2px; margin-right: 10px; } .xr-obj-type { color: var(--xr-font-color2); } .xr-sections { padding-left: 0 !important; display: grid; grid-template-columns: 150px auto auto 1fr 20px 20px; } .xr-section-item { display: contents; } .xr-section-item input { display: none; } .xr-section-item input + label { color: var(--xr-disabled-color); } .xr-section-item input:enabled + label { cursor: pointer; color: var(--xr-font-color2); } .xr-section-item input:enabled + label:hover { color: var(--xr-font-color0); } .xr-section-summary { grid-column: 1; color: var(--xr-font-color2); font-weight: 500; } .xr-section-summary > span { display: inline-block; padding-left: 0.5em; } .xr-section-summary-in:disabled + label { color: var(--xr-font-color2); } .xr-section-summary-in + label:before { display: inline-block; content: '\u00e2\u2013\u00ba'; font-size: 11px; width: 15px; text-align: center; } .xr-section-summary-in:disabled + label:before { color: var(--xr-disabled-color); } .xr-section-summary-in:checked + label:before { content: '\u00e2\u2013\u00bc'; } .xr-section-summary-in:checked + label > span { display: none; } .xr-section-summary, .xr-section-inline-details { padding-top: 4px; padding-bottom: 4px; } .xr-section-inline-details { grid-column: 2 / -1; } .xr-section-details { display: none; grid-column: 1 / -1; margin-bottom: 5px; } .xr-section-summary-in:checked ~ .xr-section-details { display: contents; } .xr-array-wrap { grid-column: 1 / -1; display: grid; grid-template-columns: 20px auto; } .xr-array-wrap > label { grid-column: 1; vertical-align: top; } .xr-preview { color: var(--xr-font-color3); } .xr-array-preview, .xr-array-data { padding: 0 5px !important; grid-column: 2; } .xr-array-data, .xr-array-in:checked ~ .xr-array-preview { display: none; } .xr-array-in:checked ~ .xr-array-data, .xr-array-preview { display: inline-block; } .xr-dim-list { display: inline-block !important; list-style: none; padding: 0 !important; margin: 0; } .xr-dim-list li { display: inline-block; padding: 0; margin: 0; } .xr-dim-list:before { content: '('; } .xr-dim-list:after { content: ')'; } .xr-dim-list li:not(:last-child):after { content: ','; padding-right: 5px; } .xr-has-index { font-weight: bold; } .xr-var-list, .xr-var-item { display: contents; } .xr-var-item > div, .xr-var-item label, .xr-var-item > .xr-var-name span { background-color: var(--xr-background-color-row-even); margin-bottom: 0; } .xr-var-item > .xr-var-name:hover span { padding-right: 5px; } .xr-var-list > li:nth-child(odd) > div, .xr-var-list > li:nth-child(odd) > label, .xr-var-list > li:nth-child(odd) > .xr-var-name span { background-color: var(--xr-background-color-row-odd); } .xr-var-name { grid-column: 1; } .xr-var-dims { grid-column: 2; } .xr-var-dtype { grid-column: 3; text-align: right; color: var(--xr-font-color2); } .xr-var-preview { grid-column: 4; } .xr-var-name, .xr-var-dims, .xr-var-dtype, .xr-preview, .xr-attrs dt { white-space: nowrap; overflow: hidden; text-overflow: ellipsis; padding-right: 10px; } .xr-var-name:hover, .xr-var-dims:hover, .xr-var-dtype:hover, .xr-attrs dt:hover { overflow: visible; width: auto; z-index: 1; } .xr-var-attrs, .xr-var-data { display: none; background-color: var(--xr-background-color) !important; padding-bottom: 5px !important; } .xr-var-attrs-in:checked ~ .xr-var-attrs, .xr-var-data-in:checked ~ .xr-var-data { display: block; } .xr-var-data > table { float: right; } .xr-var-name span, .xr-var-data, .xr-attrs { padding-left: 25px !important; } .xr-attrs, .xr-var-attrs, .xr-var-data { grid-column: 1 / -1; } dl.xr-attrs { padding: 0; margin: 0; display: grid; grid-template-columns: 125px auto; } .xr-attrs dt, .xr-attrs dd { padding: 0; margin: 0; float: left; padding-right: 10px; width: auto; } .xr-attrs dt { font-weight: normal; grid-column: 1; } .xr-attrs dt:hover span { display: inline-block; background: var(--xr-background-color); padding-right: 10px; } .xr-attrs dd { grid-column: 2; white-space: pre-wrap; word-break: break-all; } .xr-icon-database, .xr-icon-file-text2 { display: inline-block; vertical-align: middle; width: 1em; height: 1.5em !important; stroke-width: 0; stroke: currentColor; fill: currentColor; } <xarray.DataArray 'time' (time: 26)> array(['2020-12-16T15:19:15.000000000', '2020-12-16T15:24:16.000000000', '2020-12-16T15:29:17.000000000', '2020-12-16T15:34:18.000000000', '2020-12-16T15:39:18.000000000', '2020-12-16T15:44:18.000000000', '2020-12-16T15:49:18.000000000', '2020-12-16T15:54:16.000000000', '2020-12-16T15:59:15.000000000', '2020-12-16T16:04:13.000000000', '2020-12-16T16:14:13.000000000', '2020-12-16T16:19:13.000000000', '2020-12-16T16:24:14.000000000', '2020-12-16T16:29:15.000000000', '2020-12-16T16:39:16.000000000', '2020-12-16T16:44:16.000000000', '2020-12-16T16:49:16.000000000', '2020-12-16T16:54:16.000000000', '2020-12-16T16:59:15.000000000', '2020-12-16T17:04:15.000000000', '2020-12-16T17:09:15.000000000', '2020-12-16T17:19:15.000000000', '2020-12-16T17:24:16.000000000', '2020-12-16T17:29:17.000000000', '2020-12-16T17:34:18.000000000', '2020-12-16T17:39:18.000000000'], dtype='datetime64[ns]') Coordinates: * time (time) datetime64[ns] 2020-12-16T15:19:15 ... 2020-12-16T17:39:18 xarray.DataArray 'time' time : 26 2020-12-16T15:19:15 2020-12-16T15:24:16 ... 2020-12-16T17:39:18 array(['2020-12-16T15:19:15.000000000', '2020-12-16T15:24:16.000000000', '2020-12-16T15:29:17.000000000', '2020-12-16T15:34:18.000000000', '2020-12-16T15:39:18.000000000', '2020-12-16T15:44:18.000000000', '2020-12-16T15:49:18.000000000', '2020-12-16T15:54:16.000000000', '2020-12-16T15:59:15.000000000', '2020-12-16T16:04:13.000000000', '2020-12-16T16:14:13.000000000', '2020-12-16T16:19:13.000000000', '2020-12-16T16:24:14.000000000', '2020-12-16T16:29:15.000000000', '2020-12-16T16:39:16.000000000', '2020-12-16T16:44:16.000000000', '2020-12-16T16:49:16.000000000', '2020-12-16T16:54:16.000000000', '2020-12-16T16:59:15.000000000', '2020-12-16T17:04:15.000000000', '2020-12-16T17:09:15.000000000', '2020-12-16T17:19:15.000000000', '2020-12-16T17:24:16.000000000', '2020-12-16T17:29:17.000000000', '2020-12-16T17:34:18.000000000', '2020-12-16T17:39:18.000000000'], dtype='datetime64[ns]') Coordinates: (1) time (time) datetime64[ns] 2020-12-16T15:19:15 ... 2020-12-... array(['2020-12-16T15:19:15.000000000', '2020-12-16T15:24:16.000000000', '2020-12-16T15:29:17.000000000', '2020-12-16T15:34:18.000000000', '2020-12-16T15:39:18.000000000', '2020-12-16T15:44:18.000000000', '2020-12-16T15:49:18.000000000', '2020-12-16T15:54:16.000000000', '2020-12-16T15:59:15.000000000', '2020-12-16T16:04:13.000000000', '2020-12-16T16:14:13.000000000', '2020-12-16T16:19:13.000000000', '2020-12-16T16:24:14.000000000', '2020-12-16T16:29:15.000000000', '2020-12-16T16:39:16.000000000', '2020-12-16T16:44:16.000000000', '2020-12-16T16:49:16.000000000', '2020-12-16T16:54:16.000000000', '2020-12-16T16:59:15.000000000', '2020-12-16T17:04:15.000000000', '2020-12-16T17:09:15.000000000', '2020-12-16T17:19:15.000000000', '2020-12-16T17:24:16.000000000', '2020-12-16T17:29:17.000000000', '2020-12-16T17:34:18.000000000', '2020-12-16T17:39:18.000000000'], dtype='datetime64[ns]') Attributes: (0) fig = plt . figure ( dpi = 250 , figsize = ( 10 , 10 )) ax = plt . axes ( projection = ccrs . TransverseMercator ()) loaded_xarray [ 'stacked_eumetsat_data' ] . isel ( variable = 0 , time = 0 ) . T . plot ( ax = ax , cmap = 'magma' , vmin =- 200 , vmax = 400 ) ax . coastlines ( resolution = '50m' , alpha = 0.8 , color = 'white' ) <ipython-input-82-f7e189d5f897>:2: UserWarning: The default value for the *approx* keyword argument to TransverseMercator will change from True to False after 0.18. ax = plt.axes(projection=ccrs.TransverseMercator()) <cartopy.mpl.feature_artist.FeatureArtist at 0x2925b42afa0> # Disk space is priority, then look at io speeds # # Priority order # * notebook examples # * airflow # * end-to-end pipeline with benchmarking # * publish documentation # * add more tests Brief - [x] Tweak Zarr data types to minimise storage space (e.g. re-scale all channels to [0, 1023] and store as int16s, with missing values encoded as -1; and compressed using zstd level 5). (To re-scale to [0, 1023], you'll need to know the min and max values! I've computed the mins and maxes for as 12 channels from James Fulton's 'all_zarr' array. The mins and maxes are listed here. Not sure if these mins and maxes will be the same after you've re-projected the entire geographical scope though. - [x] Each Zarr chunk could be something like 32 timesteps, 512 pixels wide, 512 pixels high, and 1 channel. (We're not sure how many channels will be useful for PV nowcasting. So, it's probably best to keep the channels in separate chunks.) - [x] Would be great to use all the CPU cores on the VM. If the reprojection library doesn't do this itself, then maybe consider spinning up multiple processes, maybe using concurrent.futures.ProcessPoolExecutor.map(worker_func, [list of native files to re-projected]); or using dask.delayed. Zarr supports parallel writes as long as the writes are to different chunks (see Zarr's docs on parallel processing). - [x] One challenge will be that, if each Zarr chunk is multiple timesteps, then the conversion script may have to load multiple timesteps in memory, or in temporary files on the VM, before concatenating multiple timesteps into one Zarr chunk. - [ ] When the script is finished processing a Native file, maybe we should move the native file to a cheaper storage class (e.g. coldline storage or archive storage).","title":"Loading Zarr Data"},{"location":"03_zarr/#wms-notes","text":"https://en.wikipedia.org/wiki/Tiled_web_map Conventions: * Tiles are 256x256 pixels * At the outer most zoom level, 0, the entire world can be rendered in a single map tile. * Each zoom level doubles in both dimensions, so a single tile is replaced by 4 tiles when zooming in. This means that about 22 zoom levels are sufficient for most practical purposes. * The Web Mercator projection is used, with latitude limits of around 85 degrees. * An X and Y numbering scheme * PNG images for tiles * Images are served through a Web server, with a URL like http://.../Z/X/Y.png, where Z is the zoom level, and X and Y identify the tile. * Tile Map Service: (0 to 2zoom-1, 2zoom-1 to 0) for the range (-180, +85.0511) - (+180, -85.0511).","title":"WMS Notes"},{"location":"04_gcp/","text":"EUMETSAT and GCP \u00b6 Setup \u00b6 Some GCP Helpers \u00b6 First need a couple of helper functions to work with Google Cloud Platform. Ideally the principles will transfer easily to other cloud providers if necessary. User input \u00b6 BUCKET_NAME = \"solar-pv-nowcasting-data\" PREFIX = \"satellite/EUMETSAT/SEVIRI_RSS/native/2020\" blobs = list_blobs_with_prefix ( BUCKET_NAME , prefix = PREFIX ) print ( f 'There are { len ( blobs ) } files' ) There are 2 files blobs [: 10 ] ['satellite/EUMETSAT/SEVIRI_RSS/native/2020/01/01/12/04/MSG3-SEVI-MSG15-0100-NA-20200101120418.186000000Z-NA.nat.bz2', 'satellite/EUMETSAT/SEVIRI_RSS/native/2020/01/01/12/09/MSG3-SEVI-MSG15-0100-NA-20200101120916.896000000Z-NA.nat.bz2'] storage_client = storage . Client () PREFIX = \"satellite/EUMETSAT/SEVIRI_RSS/native/2018/\" # Note: Client.list_blobs requires at least package version 1.17.0. blobs_ = storage_client . list_blobs ( BUCKET_NAME , prefix = PREFIX ) sizes = [] for blob in blobs_ : sizes . append ( blob . size ) --------------------------------------------------------------------------- KeyboardInterrupt Traceback (most recent call last) <ipython-input-20-8607175daf03> in <module> 8 sizes = [] 9 ---> 10 for blob in blobs_: 11 sizes.append(blob.size) ~/software/anaconda3/envs/satip_dev/lib/python3.8/site-packages/google/api_core/page_iterator.py in _items_iter(self) 210 def _items_iter(self): 211 \"\"\"Iterator for each item returned.\"\"\" --> 212 for page in self._page_iter(increment=False): 213 for item in page: 214 self.num_results += 1 ~/software/anaconda3/envs/satip_dev/lib/python3.8/site-packages/google/api_core/page_iterator.py in _page_iter(self, increment) 247 self.num_results += page.num_items 248 yield page --> 249 page = self._next_page() 250 251 @abc.abstractmethod ~/software/anaconda3/envs/satip_dev/lib/python3.8/site-packages/google/api_core/page_iterator.py in _next_page(self) 367 \"\"\" 368 if self._has_next_page(): --> 369 response = self._get_next_page_response() 370 items = response.get(self._items_key, ()) 371 page = Page(self, items, self.item_to_value, raw_page=response) ~/software/anaconda3/envs/satip_dev/lib/python3.8/site-packages/google/api_core/page_iterator.py in _get_next_page_response(self) 416 params = self._get_query_params() 417 if self._HTTP_METHOD == \"GET\": --> 418 return self.api_request( 419 method=self._HTTP_METHOD, path=self.path, query_params=params 420 ) ~/software/anaconda3/envs/satip_dev/lib/python3.8/site-packages/google/cloud/storage/_http.py in api_request(self, *args, **kwargs) 61 if retry: 62 call = retry(call) ---> 63 return call() ~/software/anaconda3/envs/satip_dev/lib/python3.8/site-packages/google/api_core/retry.py in retry_wrapped_func(*args, **kwargs) 279 self._initial, self._maximum, multiplier=self._multiplier 280 ) --> 281 return retry_target( 282 target, 283 self._predicate, ~/software/anaconda3/envs/satip_dev/lib/python3.8/site-packages/google/api_core/retry.py in retry_target(target, predicate, sleep_generator, deadline, on_error) 182 for sleep in sleep_generator: 183 try: --> 184 return target() 185 186 # pylint: disable=broad-except ~/software/anaconda3/envs/satip_dev/lib/python3.8/site-packages/google/cloud/_http.py in api_request(self, method, path, query_params, data, content_type, headers, api_base_url, api_version, expect_json, _target_object, timeout) 422 content_type = \"application/json\" 423 --> 424 response = self._make_request( 425 method=method, 426 url=url, ~/software/anaconda3/envs/satip_dev/lib/python3.8/site-packages/google/cloud/_http.py in _make_request(self, method, url, data, content_type, headers, target_object, timeout) 286 headers[\"User-Agent\"] = self.user_agent 287 --> 288 return self._do_request( 289 method, url, headers, data, target_object, timeout=timeout 290 ) ~/software/anaconda3/envs/satip_dev/lib/python3.8/site-packages/google/cloud/_http.py in _do_request(self, method, url, headers, data, target_object, timeout) 324 :returns: The HTTP response. 325 \"\"\" --> 326 return self.http.request( 327 url=url, method=method, headers=headers, data=data, timeout=timeout 328 ) ~/software/anaconda3/envs/satip_dev/lib/python3.8/site-packages/google/auth/transport/requests.py in request(self, method, url, data, headers, max_allowed_time, timeout, **kwargs) 462 463 with TimeoutGuard(remaining_time) as guard: --> 464 response = super(AuthorizedSession, self).request( 465 method, 466 url, ~/software/anaconda3/envs/satip_dev/lib/python3.8/site-packages/requests/sessions.py in request(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json) 540 } 541 send_kwargs.update(settings) --> 542 resp = self.send(prep, **send_kwargs) 543 544 return resp ~/software/anaconda3/envs/satip_dev/lib/python3.8/site-packages/requests/sessions.py in send(self, request, **kwargs) 695 696 if not stream: --> 697 r.content 698 699 return r ~/software/anaconda3/envs/satip_dev/lib/python3.8/site-packages/requests/models.py in content(self) 829 self._content = None 830 else: --> 831 self._content = b''.join(self.iter_content(CONTENT_CHUNK_SIZE)) or b'' 832 833 self._content_consumed = True ~/software/anaconda3/envs/satip_dev/lib/python3.8/site-packages/requests/models.py in generate() 751 if hasattr(self.raw, 'stream'): 752 try: --> 753 for chunk in self.raw.stream(chunk_size, decode_content=True): 754 yield chunk 755 except ProtocolError as e: ~/software/anaconda3/envs/satip_dev/lib/python3.8/site-packages/urllib3/response.py in stream(self, amt, decode_content) 573 else: 574 while not is_fp_closed(self._fp): --> 575 data = self.read(amt=amt, decode_content=decode_content) 576 577 if data: ~/software/anaconda3/envs/satip_dev/lib/python3.8/site-packages/urllib3/response.py in read(self, amt, decode_content, cache_content) 516 else: 517 cache_content = False --> 518 data = self._fp.read(amt) if not fp_closed else b\"\" 519 if ( 520 amt != 0 and not data ~/software/anaconda3/envs/satip_dev/lib/python3.8/http/client.py in read(self, amt) 456 # Amount is given, implement using readinto 457 b = bytearray(amt) --> 458 n = self.readinto(b) 459 return memoryview(b)[:n].tobytes() 460 else: ~/software/anaconda3/envs/satip_dev/lib/python3.8/http/client.py in readinto(self, b) 500 # connection, and the user is reading more bytes than will be provided 501 # (for example, reading in 1k chunks) --> 502 n = self.fp.readinto(b) 503 if not n and b: 504 # Ideally, we would raise IncompleteRead if the content-length ~/software/anaconda3/envs/satip_dev/lib/python3.8/socket.py in readinto(self, b) 667 while True: 668 try: --> 669 return self._sock.recv_into(b) 670 except timeout: 671 self._timeout_occurred = True ~/software/anaconda3/envs/satip_dev/lib/python3.8/ssl.py in recv_into(self, buffer, nbytes, flags) 1239 \"non-zero flags not allowed in calls to recv_into() on %s\" % 1240 self.__class__) -> 1241 return self.read(nbytes, buffer) 1242 else: 1243 return super().recv_into(buffer, nbytes, flags) ~/software/anaconda3/envs/satip_dev/lib/python3.8/ssl.py in read(self, len, buffer) 1097 try: 1098 if buffer is not None: -> 1099 return self._sslobj.read(len, buffer) 1100 else: 1101 return self._sslobj.read(len) KeyboardInterrupt: sum ( sizes ) / 1e9 2018 contains 2.4TB of data Note that using the storage client to return blobs returns an iterable of blob metadata objects. From those we've extracted the names. We can go backwards from the names to interact with the blobs. df = pd . DataFrame ( blobs , columns = [ 'blobs' ]) df = df [ df [ 'blobs' ] . str . endswith ( '.nat.bz2' )] # only compressed data files df [ 'datetime' ] = pd . to_datetime ( df [ 'blobs' ] . str . slice ( start = 37 , stop = 53 ), format = \"%Y/%m/ %d /%H/%M\" ) months_in_order = [ 'January' , 'February' , 'March' , 'April' , 'May' , 'June' , 'July' , 'August' , 'September' , 'October' , 'November' , 'December' ] blobs_by_month = df \\ . assign ( year = lambda x : x [ 'datetime' ] . dt . year ) \\ . assign ( month = lambda x : x [ 'datetime' ] . dt . month_name ()) \\ . groupby ([ 'month' , 'year' ]) . count ()[ 'blobs' ] . to_frame () \\ . reset_index () \\ . pivot ( index = 'month' , columns = 'year' , values = 'blobs' ) \\ . reindex ( months_in_order ) blobs_by_month .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } year 2020 month January 2.0 February NaN March NaN April NaN May NaN June NaN July NaN August NaN September NaN October NaN November NaN December NaN # credit: https://dfrieds.com/data-visualizations/visualize-historical-time-comparisons.html figure , axes = plt . subplots ( figsize = ( 10 , 11 )) sns . heatmap ( blobs_by_month , annot = True , linewidths =. 5 , ax = axes , cmap = \"Greys\" ) axes . axes . set_title ( \"Count of .nat.bz files in Storage by Month and Year\" , fontsize = 20 , y = 1.01 ) axes . axes . set_ylabel ( \"month\" , labelpad = 50 , rotation = 0 ) axes . axes . set_xlabel ( \"year\" , labelpad = 16 ); plt . yticks ( rotation = 0 ); filenames = df [ 'blobs' ] . str . split ( '/' ) . str [ - 1 ] . str . replace ( '.bz2' , '' ) filenames 1 MSG3-SEVI-MSG15-0100-NA-20180531000417.2340000... 2 MSG3-SEVI-MSG15-0100-NA-20180531000917.4680000... 3 MSG3-SEVI-MSG15-0100-NA-20180531001417.7010000... 4 MSG3-SEVI-MSG15-0100-NA-20180531001917.9350000... 5 MSG3-SEVI-MSG15-0100-NA-20180531002416.3640000... ... 170664 MSG3-SEVI-MSG15-0100-NA-20191231233418.2450000... 170665 MSG3-SEVI-MSG15-0100-NA-20191231233916.9530000... 170666 MSG3-SEVI-MSG15-0100-NA-20191231234415.6620000... 170667 MSG3-SEVI-MSG15-0100-NA-20191231234914.3700000... 170668 MSG3-SEVI-MSG15-0100-NA-20191231235414.2810000... Name: blobs, Length: 170667, dtype: object PREFIX = \"satellite/EUMETSAT/SEVIRI_RSS/native/2019/10/01\" filenames = get_eumetsat_filenames ( BUCKET_NAME , prefix = PREFIX ) len ( filenames ) 68202 Write metadata to bigquery \u00b6 For cloud storage functions, storing metadata in a RDBS seems useful. BigQuery is a low hassle way to achieve this and can scale to lots of data with ease. Downsides are rather inflexible migrations and updates. # write_metadata_to_gcp(df, 'test', 'solar-pv-nowcasting') --------------------------------------------------------------------------- NotFoundException Traceback (most recent call last) <ipython-input-29-4f7fbf24d98f> in <module> ----> 1 write_metadata_to_gcp(df, 'test', 'solar-pv-nowcasting') <ipython-input-28-9e358fc4b48b> in write_metadata_to_gcp(df, table_id, project_id, credentials, append) 13 ) 14 else: ---> 15 pandas_gbq.to_gbq( 16 df, 17 table_id, ~/software/anaconda3/envs/satip_dev/lib/python3.8/site-packages/pandas_gbq/gbq.py in to_gbq(dataframe, destination_table, project_id, chunksize, reauth, if_exists, auth_local_webserver, table_schema, location, progress_bar, credentials, verbose, private_key) 1133 1134 if \".\" not in destination_table: -> 1135 raise NotFoundException( 1136 \"Invalid Table Name. Should be of the form 'datasetId.tableId' \" 1137 ) NotFoundException: Invalid Table Name. Should be of the form 'datasetId.tableId' Downloading: 100%|\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6| 1/1 [00:00<00:00, 1.30rows/s] '2020-12-03 19:14'","title":"GCP Analytics"},{"location":"04_gcp/#eumetsat-and-gcp","text":"","title":"EUMETSAT and GCP"},{"location":"04_gcp/#setup","text":"","title":"Setup"},{"location":"04_gcp/#some-gcp-helpers","text":"First need a couple of helper functions to work with Google Cloud Platform. Ideally the principles will transfer easily to other cloud providers if necessary.","title":"Some GCP Helpers"},{"location":"04_gcp/#user-input","text":"BUCKET_NAME = \"solar-pv-nowcasting-data\" PREFIX = \"satellite/EUMETSAT/SEVIRI_RSS/native/2020\" blobs = list_blobs_with_prefix ( BUCKET_NAME , prefix = PREFIX ) print ( f 'There are { len ( blobs ) } files' ) There are 2 files blobs [: 10 ] ['satellite/EUMETSAT/SEVIRI_RSS/native/2020/01/01/12/04/MSG3-SEVI-MSG15-0100-NA-20200101120418.186000000Z-NA.nat.bz2', 'satellite/EUMETSAT/SEVIRI_RSS/native/2020/01/01/12/09/MSG3-SEVI-MSG15-0100-NA-20200101120916.896000000Z-NA.nat.bz2'] storage_client = storage . Client () PREFIX = \"satellite/EUMETSAT/SEVIRI_RSS/native/2018/\" # Note: Client.list_blobs requires at least package version 1.17.0. blobs_ = storage_client . list_blobs ( BUCKET_NAME , prefix = PREFIX ) sizes = [] for blob in blobs_ : sizes . append ( blob . size ) --------------------------------------------------------------------------- KeyboardInterrupt Traceback (most recent call last) <ipython-input-20-8607175daf03> in <module> 8 sizes = [] 9 ---> 10 for blob in blobs_: 11 sizes.append(blob.size) ~/software/anaconda3/envs/satip_dev/lib/python3.8/site-packages/google/api_core/page_iterator.py in _items_iter(self) 210 def _items_iter(self): 211 \"\"\"Iterator for each item returned.\"\"\" --> 212 for page in self._page_iter(increment=False): 213 for item in page: 214 self.num_results += 1 ~/software/anaconda3/envs/satip_dev/lib/python3.8/site-packages/google/api_core/page_iterator.py in _page_iter(self, increment) 247 self.num_results += page.num_items 248 yield page --> 249 page = self._next_page() 250 251 @abc.abstractmethod ~/software/anaconda3/envs/satip_dev/lib/python3.8/site-packages/google/api_core/page_iterator.py in _next_page(self) 367 \"\"\" 368 if self._has_next_page(): --> 369 response = self._get_next_page_response() 370 items = response.get(self._items_key, ()) 371 page = Page(self, items, self.item_to_value, raw_page=response) ~/software/anaconda3/envs/satip_dev/lib/python3.8/site-packages/google/api_core/page_iterator.py in _get_next_page_response(self) 416 params = self._get_query_params() 417 if self._HTTP_METHOD == \"GET\": --> 418 return self.api_request( 419 method=self._HTTP_METHOD, path=self.path, query_params=params 420 ) ~/software/anaconda3/envs/satip_dev/lib/python3.8/site-packages/google/cloud/storage/_http.py in api_request(self, *args, **kwargs) 61 if retry: 62 call = retry(call) ---> 63 return call() ~/software/anaconda3/envs/satip_dev/lib/python3.8/site-packages/google/api_core/retry.py in retry_wrapped_func(*args, **kwargs) 279 self._initial, self._maximum, multiplier=self._multiplier 280 ) --> 281 return retry_target( 282 target, 283 self._predicate, ~/software/anaconda3/envs/satip_dev/lib/python3.8/site-packages/google/api_core/retry.py in retry_target(target, predicate, sleep_generator, deadline, on_error) 182 for sleep in sleep_generator: 183 try: --> 184 return target() 185 186 # pylint: disable=broad-except ~/software/anaconda3/envs/satip_dev/lib/python3.8/site-packages/google/cloud/_http.py in api_request(self, method, path, query_params, data, content_type, headers, api_base_url, api_version, expect_json, _target_object, timeout) 422 content_type = \"application/json\" 423 --> 424 response = self._make_request( 425 method=method, 426 url=url, ~/software/anaconda3/envs/satip_dev/lib/python3.8/site-packages/google/cloud/_http.py in _make_request(self, method, url, data, content_type, headers, target_object, timeout) 286 headers[\"User-Agent\"] = self.user_agent 287 --> 288 return self._do_request( 289 method, url, headers, data, target_object, timeout=timeout 290 ) ~/software/anaconda3/envs/satip_dev/lib/python3.8/site-packages/google/cloud/_http.py in _do_request(self, method, url, headers, data, target_object, timeout) 324 :returns: The HTTP response. 325 \"\"\" --> 326 return self.http.request( 327 url=url, method=method, headers=headers, data=data, timeout=timeout 328 ) ~/software/anaconda3/envs/satip_dev/lib/python3.8/site-packages/google/auth/transport/requests.py in request(self, method, url, data, headers, max_allowed_time, timeout, **kwargs) 462 463 with TimeoutGuard(remaining_time) as guard: --> 464 response = super(AuthorizedSession, self).request( 465 method, 466 url, ~/software/anaconda3/envs/satip_dev/lib/python3.8/site-packages/requests/sessions.py in request(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json) 540 } 541 send_kwargs.update(settings) --> 542 resp = self.send(prep, **send_kwargs) 543 544 return resp ~/software/anaconda3/envs/satip_dev/lib/python3.8/site-packages/requests/sessions.py in send(self, request, **kwargs) 695 696 if not stream: --> 697 r.content 698 699 return r ~/software/anaconda3/envs/satip_dev/lib/python3.8/site-packages/requests/models.py in content(self) 829 self._content = None 830 else: --> 831 self._content = b''.join(self.iter_content(CONTENT_CHUNK_SIZE)) or b'' 832 833 self._content_consumed = True ~/software/anaconda3/envs/satip_dev/lib/python3.8/site-packages/requests/models.py in generate() 751 if hasattr(self.raw, 'stream'): 752 try: --> 753 for chunk in self.raw.stream(chunk_size, decode_content=True): 754 yield chunk 755 except ProtocolError as e: ~/software/anaconda3/envs/satip_dev/lib/python3.8/site-packages/urllib3/response.py in stream(self, amt, decode_content) 573 else: 574 while not is_fp_closed(self._fp): --> 575 data = self.read(amt=amt, decode_content=decode_content) 576 577 if data: ~/software/anaconda3/envs/satip_dev/lib/python3.8/site-packages/urllib3/response.py in read(self, amt, decode_content, cache_content) 516 else: 517 cache_content = False --> 518 data = self._fp.read(amt) if not fp_closed else b\"\" 519 if ( 520 amt != 0 and not data ~/software/anaconda3/envs/satip_dev/lib/python3.8/http/client.py in read(self, amt) 456 # Amount is given, implement using readinto 457 b = bytearray(amt) --> 458 n = self.readinto(b) 459 return memoryview(b)[:n].tobytes() 460 else: ~/software/anaconda3/envs/satip_dev/lib/python3.8/http/client.py in readinto(self, b) 500 # connection, and the user is reading more bytes than will be provided 501 # (for example, reading in 1k chunks) --> 502 n = self.fp.readinto(b) 503 if not n and b: 504 # Ideally, we would raise IncompleteRead if the content-length ~/software/anaconda3/envs/satip_dev/lib/python3.8/socket.py in readinto(self, b) 667 while True: 668 try: --> 669 return self._sock.recv_into(b) 670 except timeout: 671 self._timeout_occurred = True ~/software/anaconda3/envs/satip_dev/lib/python3.8/ssl.py in recv_into(self, buffer, nbytes, flags) 1239 \"non-zero flags not allowed in calls to recv_into() on %s\" % 1240 self.__class__) -> 1241 return self.read(nbytes, buffer) 1242 else: 1243 return super().recv_into(buffer, nbytes, flags) ~/software/anaconda3/envs/satip_dev/lib/python3.8/ssl.py in read(self, len, buffer) 1097 try: 1098 if buffer is not None: -> 1099 return self._sslobj.read(len, buffer) 1100 else: 1101 return self._sslobj.read(len) KeyboardInterrupt: sum ( sizes ) / 1e9 2018 contains 2.4TB of data Note that using the storage client to return blobs returns an iterable of blob metadata objects. From those we've extracted the names. We can go backwards from the names to interact with the blobs. df = pd . DataFrame ( blobs , columns = [ 'blobs' ]) df = df [ df [ 'blobs' ] . str . endswith ( '.nat.bz2' )] # only compressed data files df [ 'datetime' ] = pd . to_datetime ( df [ 'blobs' ] . str . slice ( start = 37 , stop = 53 ), format = \"%Y/%m/ %d /%H/%M\" ) months_in_order = [ 'January' , 'February' , 'March' , 'April' , 'May' , 'June' , 'July' , 'August' , 'September' , 'October' , 'November' , 'December' ] blobs_by_month = df \\ . assign ( year = lambda x : x [ 'datetime' ] . dt . year ) \\ . assign ( month = lambda x : x [ 'datetime' ] . dt . month_name ()) \\ . groupby ([ 'month' , 'year' ]) . count ()[ 'blobs' ] . to_frame () \\ . reset_index () \\ . pivot ( index = 'month' , columns = 'year' , values = 'blobs' ) \\ . reindex ( months_in_order ) blobs_by_month .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } year 2020 month January 2.0 February NaN March NaN April NaN May NaN June NaN July NaN August NaN September NaN October NaN November NaN December NaN # credit: https://dfrieds.com/data-visualizations/visualize-historical-time-comparisons.html figure , axes = plt . subplots ( figsize = ( 10 , 11 )) sns . heatmap ( blobs_by_month , annot = True , linewidths =. 5 , ax = axes , cmap = \"Greys\" ) axes . axes . set_title ( \"Count of .nat.bz files in Storage by Month and Year\" , fontsize = 20 , y = 1.01 ) axes . axes . set_ylabel ( \"month\" , labelpad = 50 , rotation = 0 ) axes . axes . set_xlabel ( \"year\" , labelpad = 16 ); plt . yticks ( rotation = 0 ); filenames = df [ 'blobs' ] . str . split ( '/' ) . str [ - 1 ] . str . replace ( '.bz2' , '' ) filenames 1 MSG3-SEVI-MSG15-0100-NA-20180531000417.2340000... 2 MSG3-SEVI-MSG15-0100-NA-20180531000917.4680000... 3 MSG3-SEVI-MSG15-0100-NA-20180531001417.7010000... 4 MSG3-SEVI-MSG15-0100-NA-20180531001917.9350000... 5 MSG3-SEVI-MSG15-0100-NA-20180531002416.3640000... ... 170664 MSG3-SEVI-MSG15-0100-NA-20191231233418.2450000... 170665 MSG3-SEVI-MSG15-0100-NA-20191231233916.9530000... 170666 MSG3-SEVI-MSG15-0100-NA-20191231234415.6620000... 170667 MSG3-SEVI-MSG15-0100-NA-20191231234914.3700000... 170668 MSG3-SEVI-MSG15-0100-NA-20191231235414.2810000... Name: blobs, Length: 170667, dtype: object PREFIX = \"satellite/EUMETSAT/SEVIRI_RSS/native/2019/10/01\" filenames = get_eumetsat_filenames ( BUCKET_NAME , prefix = PREFIX ) len ( filenames ) 68202","title":"User input"},{"location":"04_gcp/#write-metadata-to-bigquery","text":"For cloud storage functions, storing metadata in a RDBS seems useful. BigQuery is a low hassle way to achieve this and can scale to lots of data with ease. Downsides are rather inflexible migrations and updates. # write_metadata_to_gcp(df, 'test', 'solar-pv-nowcasting') --------------------------------------------------------------------------- NotFoundException Traceback (most recent call last) <ipython-input-29-4f7fbf24d98f> in <module> ----> 1 write_metadata_to_gcp(df, 'test', 'solar-pv-nowcasting') <ipython-input-28-9e358fc4b48b> in write_metadata_to_gcp(df, table_id, project_id, credentials, append) 13 ) 14 else: ---> 15 pandas_gbq.to_gbq( 16 df, 17 table_id, ~/software/anaconda3/envs/satip_dev/lib/python3.8/site-packages/pandas_gbq/gbq.py in to_gbq(dataframe, destination_table, project_id, chunksize, reauth, if_exists, auth_local_webserver, table_schema, location, progress_bar, credentials, verbose, private_key) 1133 1134 if \".\" not in destination_table: -> 1135 raise NotFoundException( 1136 \"Invalid Table Name. Should be of the form 'datasetId.tableId' \" 1137 ) NotFoundException: Invalid Table Name. Should be of the form 'datasetId.tableId' Downloading: 100%|\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6| 1/1 [00:00<00:00, 1.30rows/s] '2020-12-03 19:14'","title":"Write metadata to bigquery"},{"location":"05_pipeline/","text":"End-to-End Pipeline \u00b6 #exports import pandas as pd import xarray as xr from satip import eumetsat , reproj , io , gcp_helpers from dagster import execute_pipeline , pipeline , solid , Field import os import dotenv C:\\Users\\Ayrto\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\google\\auth\\_default.py:69: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK without a quota project. You might receive a \"quota exceeded\" or \"API not enabled\" error. We recommend you rerun `gcloud auth application-default login` and make sure a quota project is added. Or you can use service accounts instead. For more information about service accounts, see https://cloud.google.com/docs/authentication/ warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING) Downloading: 100%|\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6| 1/1 [00:00<00:00, 3.21rows/s] User Inputs \u00b6 data_dir = '../data/raw' sorted_dir = '../data/sorted' debug_fp = '../logs/EUMETSAT_download.txt' env_vars_fp = '../.env' metadata_db_fp = '../data/EUMETSAT_metadata.db' new_grid_fp = '../data/intermediate/new_grid_4km_TM.json' new_coords_fp = '../data/intermediate/reproj_coords_TM_4km.csv' zarr_bucket = 'solar-pv-nowcasting-data/satellite/EUMETSAT/SEVIRI_RSS/full_extent_TM_int16' Loading Environment Variables \u00b6 #exports user_key = os . environ . get ( 'USER_KEY' ) user_secret = os . environ . get ( 'USER_SECRET' ) slack_id = os . environ . get ( 'SLACK_ID' ) slack_webhook_url = os . environ . get ( 'SLACK_WEBHOOK_URL' ) Dagster Pipeline \u00b6 We're now going to combine these steps into a pipeline using dagster #exports @solid ( config_schema = { 'user_key' : Field ( str , default_value = user_key , is_required = False ), 'user_secret' : Field ( str , default_value = user_secret , is_required = False ), 'slack_webhook_url' : Field ( str , default_value = slack_webhook_url , is_required = False ), 'slack_id' : Field ( str , default_value = slack_id , is_required = False ) } ) def download_latest_eumetsat_files ( context , data_dir : str , metadata_db_fp : str , debug_fp : str , table_id : str , project_id : str , start_date : str = '' ): if start_date == '' : sql_query = f 'select * from { table_id } where result_time = (select max(result_time) from { table_id } )' start_date = gcp_helpers . query ( sql_query , project_id )[ 'result_time' ] . iloc [ 0 ] end_date = pd . Timestamp . now () . strftime ( '%Y-%m- %d %H:%M' ) dm = eumetsat . DownloadManager ( context . solid_config [ 'user_key' ], context . solid_config [ 'user_secret' ], data_dir , metadata_db_fp , debug_fp , slack_webhook_url = context . solid_config [ 'slack_webhook_url' ], slack_id = context . solid_config [ 'slack_id' ]) df_new_metadata = dm . download_datasets ( start_date , end_date ) if df_new_metadata is None : df_new_metadata = pd . DataFrame ( columns = [ 'result_time' , 'file_name' ]) else : df_new_metadata = df_new_metadata . iloc [ 1 :] # the first entry is the last one we downloaded return df_new_metadata @solid () def df_metadata_to_dt_to_fp_map ( _ , df_new_metadata , data_dir : str ) -> dict : \"\"\" Here we'll then identify downloaded files in the metadata dataframe and return a mapping between datetimes and filenames \"\"\" datetime_to_filename = ( df_new_metadata . set_index ( 'result_time' ) [ 'file_name' ] . drop_duplicates () . to_dict () ) datetime_to_filepath = { datetime : f \" { data_dir } / { filename } .nat\" for datetime , filename in datetime_to_filename . items () if filename != {} } return datetime_to_filepath @solid () def reproject_datasets ( _ , datetime_to_filepath : dict , new_coords_fp : str , new_grid_fp : str ): reprojector = reproj . Reprojector ( new_coords_fp , new_grid_fp ) reprojected_dss = [ ( reprojector . reproject ( filepath , reproj_library = 'pyresample' ) . pipe ( io . add_constant_coord_to_da , 'time' , pd . to_datetime ( datetime )) ) for datetime , filepath in datetime_to_filepath . items () ] if len ( reprojected_dss ) > 0 : ds_combined_reproj = xr . concat ( reprojected_dss , 'time' , coords = 'all' , data_vars = 'all' ) return ds_combined_reproj else : return xr . Dataset () @solid () def compress_and_save_datasets ( _ , ds_combined_reproj , zarr_bucket : str , var_name : str = 'stacked_eumetsat_data' ): # Handle case where no new data exists if len ( ds_combined_reproj . dims ) == 0 : return # Compressing the datasets compressor = io . Compressor () var_name = var_name da_compressed = compressor . compress ( ds_combined_reproj [ var_name ]) # Saving to Zarr ds_compressed = io . save_da_to_zarr ( da_compressed , zarr_bucket ) return ds_compressed @solid () def save_metadata ( _ , df_new_metadata , table_id : str , project_id : str ): gcp_helpers . write_metadata_to_gcp ( df_new_metadata , table_id , project_id , append = True ) #exports @pipeline def download_latest_data_pipeline (): # Retrieving data, reprojecting, compressing, and saving to GCP df_new_metadata = download_latest_eumetsat_files () datetime_to_filepath = df_metadata_to_dt_to_fp_map ( df_new_metadata ) ds_combined_reproj = reproject_datasets ( datetime_to_filepath ) ds_combined_compressed = compress_and_save_datasets ( ds_combined_reproj ) save_metadata ( df_new_metadata ) run_config = { 'solids' : { 'download_latest_eumetsat_files' : { 'inputs' : { 'data_dir' : \"../data/raw\" , 'metadata_db_fp' : \"../data/EUMETSAT_metadata.db\" , 'debug_fp' : \"../logs/EUMETSAT_download.txt\" , 'table_id' : \"eumetsat.metadata\" , 'project_id' : \"solar-pv-nowcasting\" , 'start_date' : \"2020-12-16 19:30\" }, }, 'df_metadata_to_dt_to_fp_map' : { 'inputs' : { 'data_dir' : \"../data/raw\" } }, 'reproject_datasets' : { 'inputs' : { 'new_coords_fp' : \"../data/intermediate/reproj_coords_TM_4km.csv\" , 'new_grid_fp' : \"../data/intermediate/new_grid_4km_TM.json\" } }, 'compress_and_save_datasets' : { 'inputs' : { 'zarr_bucket' : \"solar-pv-nowcasting-data/satellite/EUMETSAT/SEVIRI_RSS/full_extent_TM_int16\" , 'var_name' : \"stacked_eumetsat_data\" } }, 'save_metadata' : { 'inputs' : { 'table_id' : \"eumetsat.metadata\" , 'project_id' : \"solar-pv-nowcasting\" }, } } } execute_pipeline ( download_latest_data_pipeline , run_config = run_config ) 2020-12-16 20:00:18 - dagster - DEBUG - download_latest_data_pipeline - 843966a6-a689-430d-97e0-7c937aefc85e - 20040 - ENGINE_EVENT - Starting initialization of resources [asset_store]. 2020-12-16 20:00:18 - dagster - DEBUG - download_latest_data_pipeline - 843966a6-a689-430d-97e0-7c937aefc85e - 20040 - ENGINE_EVENT - Finished initialization of resources [asset_store]. 2020-12-16 20:00:18 - dagster - DEBUG - download_latest_data_pipeline - 843966a6-a689-430d-97e0-7c937aefc85e - 20040 - PIPELINE_START - Started execution of pipeline \"download_latest_data_pipeline\". 2020-12-16 20:00:18 - dagster - DEBUG - download_latest_data_pipeline - 843966a6-a689-430d-97e0-7c937aefc85e - 20040 - ENGINE_EVENT - Executing steps in process (pid: 20040) 2020-12-16 20:00:18 - dagster - DEBUG - download_latest_data_pipeline - 843966a6-a689-430d-97e0-7c937aefc85e - 20040 - download_latest_eumetsat_files.compute - STEP_START - Started execution of step \"download_latest_eumetsat_files.compute\". 2020-12-16 20:00:18 - dagster - DEBUG - download_latest_data_pipeline - 843966a6-a689-430d-97e0-7c937aefc85e - 20040 - download_latest_eumetsat_files.compute - STEP_INPUT - Got input \"data_dir\" of type \"String\". (Type check passed). 2020-12-16 20:00:18 - dagster - DEBUG - download_latest_data_pipeline - 843966a6-a689-430d-97e0-7c937aefc85e - 20040 - download_latest_eumetsat_files.compute - STEP_INPUT - Got input \"metadata_db_fp\" of type \"String\". (Type check passed). 2020-12-16 20:00:18 - dagster - DEBUG - download_latest_data_pipeline - 843966a6-a689-430d-97e0-7c937aefc85e - 20040 - download_latest_eumetsat_files.compute - STEP_INPUT - Got input \"debug_fp\" of type \"String\". (Type check passed). 2020-12-16 20:00:18 - dagster - DEBUG - download_latest_data_pipeline - 843966a6-a689-430d-97e0-7c937aefc85e - 20040 - download_latest_eumetsat_files.compute - STEP_INPUT - Got input \"start_date\" of type \"String\". (Type check passed). 2020-12-16 20:00:19 - dagster - ERROR - download_latest_data_pipeline - 843966a6-a689-430d-97e0-7c937aefc85e - 20040 - download_latest_eumetsat_files.compute - STEP_FAILURE - Execution of step \"download_latest_eumetsat_files.compute\" failed. AttributeError: 'DagsterLogManager' object has no attribute 'setLevel' File \"C:\\Users\\Ayrto\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\dagster\\core\\errors.py\", line 180, in user_code_error_boundary yield File \"C:\\Users\\Ayrto\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\dagster\\core\\execution\\plan\\execute_step.py\", line 475, in _user_event_sequence_for_step_compute_fn for event in iterate_with_context(raise_interrupts_immediately, gen): File \"C:\\Users\\Ayrto\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\dagster\\utils\\__init__.py\", line 443, in iterate_with_context next_output = next(iterator) File \"C:\\Users\\Ayrto\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\dagster\\core\\execution\\plan\\compute.py\", line 105, in _execute_core_compute for step_output in _yield_compute_results(compute_context, inputs, compute_fn): File \"C:\\Users\\Ayrto\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\dagster\\core\\execution\\plan\\compute.py\", line 76, in _yield_compute_results for event in user_event_sequence: File \"C:\\Users\\Ayrto\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\dagster\\core\\definitions\\decorators\\solid.py\", line 227, in compute result = fn(context, **kwargs) File \"<ipython-input-5-968917ba6d74>\", line 16, in download_latest_eumetsat_files dm = eumetsat.DownloadManager(context.solid_config['user_key'], context.solid_config['user_secret'], data_dir, metadata_db_fp, debug_fp, slack_webhook_url=context.solid_config['slack_webhook_url'], slack_id=context.solid_config['slack_id'], logger_name=context.log) File \"c:\\users\\ayrto\\desktop\\freelance work\\fea\\work\\ocf\\satip\\satip\\eumetsat.py\", line 312, in __init__ self.logger = utils.set_up_logging(logger_name, log_fp, File \"c:\\users\\ayrto\\desktop\\freelance work\\fea\\work\\ocf\\satip\\satip\\utils.py\", line 133, in set_up_logging logger.setLevel(getattr(logging, main_logging_level)) 2020-12-16 20:00:19 - dagster - ERROR - download_latest_data_pipeline - 843966a6-a689-430d-97e0-7c937aefc85e - 20040 - PIPELINE_FAILURE - Execution of pipeline \"download_latest_data_pipeline\" failed. An exception was thrown during execution. AttributeError: 'DagsterLogManager' object has no attribute 'setLevel' File \"C:\\Users\\Ayrto\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\dagster\\core\\execution\\api.py\", line 665, in _pipeline_execution_iterator for event in pipeline_context.executor.execute(pipeline_context, execution_plan): File \"C:\\Users\\Ayrto\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\dagster\\core\\executor\\in_process.py\", line 36, in execute for event in inner_plan_execution_iterator(pipeline_context, execution_plan): File \"C:\\Users\\Ayrto\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\dagster\\core\\execution\\plan\\execute_plan.py\", line 77, in inner_plan_execution_iterator for step_event in check.generator( File \"C:\\Users\\Ayrto\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\dagster\\core\\execution\\plan\\execute_plan.py\", line 272, in _dagster_event_sequence_for_step raise dagster_user_error.user_exception File \"C:\\Users\\Ayrto\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\dagster\\core\\errors.py\", line 180, in user_code_error_boundary yield File \"C:\\Users\\Ayrto\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\dagster\\core\\execution\\plan\\execute_step.py\", line 475, in _user_event_sequence_for_step_compute_fn for event in iterate_with_context(raise_interrupts_immediately, gen): File \"C:\\Users\\Ayrto\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\dagster\\utils\\__init__.py\", line 443, in iterate_with_context next_output = next(iterator) File \"C:\\Users\\Ayrto\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\dagster\\core\\execution\\plan\\compute.py\", line 105, in _execute_core_compute for step_output in _yield_compute_results(compute_context, inputs, compute_fn): File \"C:\\Users\\Ayrto\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\dagster\\core\\execution\\plan\\compute.py\", line 76, in _yield_compute_results for event in user_event_sequence: File \"C:\\Users\\Ayrto\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\dagster\\core\\definitions\\decorators\\solid.py\", line 227, in compute result = fn(context, **kwargs) File \"<ipython-input-5-968917ba6d74>\", line 16, in download_latest_eumetsat_files dm = eumetsat.DownloadManager(context.solid_config['user_key'], context.solid_config['user_secret'], data_dir, metadata_db_fp, debug_fp, slack_webhook_url=context.solid_config['slack_webhook_url'], slack_id=context.solid_config['slack_id'], logger_name=context.log) File \"c:\\users\\ayrto\\desktop\\freelance work\\fea\\work\\ocf\\satip\\satip\\eumetsat.py\", line 312, in __init__ self.logger = utils.set_up_logging(logger_name, log_fp, File \"c:\\users\\ayrto\\desktop\\freelance work\\fea\\work\\ocf\\satip\\satip\\utils.py\", line 133, in set_up_logging logger.setLevel(getattr(logging, main_logging_level)) --------------------------------------------------------------------------- DagsterExecutionStepExecutionError Traceback (most recent call last) ~\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\dagster\\core\\execution\\plan\\execute_plan.py in _dagster_event_sequence_for_step(step_context, retries) 211 --> 212 for step_event in check.generator(step_events): 213 yield step_event ~\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\dagster\\core\\execution\\plan\\execute_step.py in core_dagster_event_sequence_for_step(step_context, prior_attempt_count) 285 # timer block above in order for time to be recorded accurately. --> 286 for user_event in check.generator( 287 _step_output_error_checked_user_event_sequence(step_context, user_event_sequence) ~\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\dagster\\core\\execution\\plan\\execute_step.py in _step_output_error_checked_user_event_sequence(step_context, user_event_sequence) 58 ---> 59 for user_event in user_event_sequence: 60 if not isinstance(user_event, Output): ~\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\dagster\\core\\execution\\plan\\execute_step.py in _user_event_sequence_for_step_compute_fn(step_context, evaluated_inputs) 475 for event in iterate_with_context(raise_interrupts_immediately, gen): --> 476 yield event 477 ~\\anaconda3\\envs\\satip_dev\\lib\\contextlib.py in __exit__(self, type, value, traceback) 130 try: --> 131 self.gen.throw(type, value, traceback) 132 except StopIteration as exc: ~\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\dagster\\core\\errors.py in user_code_error_boundary(error_cls, msg_fn, control_flow_exceptions, **kwargs) 189 # with the error reported further up the stack --> 190 raise_from( 191 error_cls(msg_fn(), user_exception=e, original_exc_info=sys.exc_info(), **kwargs), e ~\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\future\\utils\\__init__.py in raise_from(exc, cause) 402 execstr = \"raise __python_future_raise_from_exc from __python_future_raise_from_cause\" --> 403 exec(execstr, myglobals, mylocals) 404 ~\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\dagster\\core\\errors.py in <module> DagsterExecutionStepExecutionError: Error occurred during the execution of step: step key: \"download_latest_eumetsat_files.compute\" solid invocation: \"download_latest_eumetsat_files\" solid definition: \"download_latest_eumetsat_files\" During handling of the above exception, another exception occurred: AttributeError Traceback (most recent call last) [... skipping hidden 1 frame] <ipython-input-7-79fdcc951128> in <module> 36 ---> 37 execute_pipeline(download_latest_data_pipeline, run_config=run_config) ~\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\dagster\\core\\execution\\api.py in execute_pipeline(pipeline, run_config, mode, preset, tags, solid_selection, instance, raise_on_error) 323 with _ephemeral_instance_if_missing(instance) as execute_instance: --> 324 return _logged_execute_pipeline( 325 pipeline, ~\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\dagster\\core\\telemetry.py in wrap(*args, **kwargs) 88 log_action(instance=instance, action=f.__name__ + \"_started\", client_time=start_time) ---> 89 result = f(*args, **kwargs) 90 end_time = datetime.datetime.now() ~\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\dagster\\core\\execution\\api.py in _logged_execute_pipeline(pipeline, instance, run_config, mode, preset, tags, solid_selection, raise_on_error) 374 --> 375 return execute_run(pipeline, pipeline_run, instance, raise_on_error=raise_on_error) 376 ~\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\dagster\\core\\execution\\api.py in execute_run(pipeline, pipeline_run, instance, raise_on_error) 176 ) --> 177 event_list = list(_execute_run_iterable) 178 pipeline_context = _execute_run_iterable.pipeline_context ~\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\dagster\\core\\execution\\api.py in __iter__(self) 726 if self.pipeline_context: # False if we had a pipeline init failure --> 727 for event in self.iterator( 728 execution_plan=self.execution_plan, pipeline_context=self.pipeline_context, ~\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\dagster\\core\\execution\\api.py in _pipeline_execution_iterator(pipeline_context, execution_plan) 664 try: --> 665 for event in pipeline_context.executor.execute(pipeline_context, execution_plan): 666 if event.is_step_failure: ~\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\dagster\\core\\executor\\in_process.py in execute(self, pipeline_context, execution_plan) 35 with time_execution_scope() as timer_result: ---> 36 for event in inner_plan_execution_iterator(pipeline_context, execution_plan): 37 yield event ~\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\dagster\\core\\execution\\plan\\execute_plan.py in inner_plan_execution_iterator(pipeline_context, execution_plan) 76 else: ---> 77 for step_event in check.generator( 78 _dagster_event_sequence_for_step(step_context, retries) ~\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\dagster\\core\\execution\\plan\\execute_plan.py in _dagster_event_sequence_for_step(step_context, retries) 271 if step_context.raise_on_error: --> 272 raise dagster_user_error.user_exception 273 ~\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\dagster\\core\\errors.py in user_code_error_boundary(error_cls, msg_fn, control_flow_exceptions, **kwargs) 179 try: --> 180 yield 181 except control_flow_exceptions as cf: ~\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\dagster\\core\\execution\\plan\\execute_step.py in _user_event_sequence_for_step_compute_fn(step_context, evaluated_inputs) 474 # Allow interrupts again during each step of the execution --> 475 for event in iterate_with_context(raise_interrupts_immediately, gen): 476 yield event ~\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\dagster\\utils\\__init__.py in iterate_with_context(context_manager_class, iterator) 442 try: --> 443 next_output = next(iterator) 444 except StopIteration: ~\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\dagster\\core\\execution\\plan\\compute.py in _execute_core_compute(compute_context, inputs, compute_fn) 104 all_results = [] --> 105 for step_output in _yield_compute_results(compute_context, inputs, compute_fn): 106 yield step_output ~\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\dagster\\core\\execution\\plan\\compute.py in _yield_compute_results(compute_context, inputs, compute_fn) 75 ---> 76 for event in user_event_sequence: 77 if isinstance(event, (Output, AssetMaterialization, Materialization, ExpectationResult)): ~\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\dagster\\core\\definitions\\decorators\\solid.py in compute(context, input_defs) 226 --> 227 result = fn(context, **kwargs) 228 <ipython-input-5-968917ba6d74> in download_latest_eumetsat_files(context, data_dir, metadata_db_fp, debug_fp, start_date) 15 ---> 16 dm = eumetsat.DownloadManager(context.solid_config['user_key'], context.solid_config['user_secret'], data_dir, metadata_db_fp, debug_fp, slack_webhook_url=context.solid_config['slack_webhook_url'], slack_id=context.solid_config['slack_id'], logger_name=context.log) 17 df_new_metadata = dm.download_datasets(start_date, end_date) c:\\users\\ayrto\\desktop\\freelance work\\fea\\work\\ocf\\satip\\satip\\eumetsat.py in __init__(self, user_key, user_secret, data_dir, metadata_db_fp, log_fp, main_logging_level, slack_logging_level, slack_webhook_url, slack_id, bucket_name, bucket_prefix, logger_name) 311 # Configuring the logger --> 312 self.logger = utils.set_up_logging(logger_name, log_fp, 313 main_logging_level, slack_logging_level, c:\\users\\ayrto\\desktop\\freelance work\\fea\\work\\ocf\\satip\\satip\\utils.py in set_up_logging(name, log_dir, main_logging_level, slack_logging_level, slack_webhook_url, slack_id) 132 --> 133 logger.setLevel(getattr(logging, main_logging_level)) 134 AttributeError: 'DagsterLogManager' object has no attribute 'setLevel' The above exception was the direct cause of the following exception: DagsterExecutionStepExecutionError Traceback (most recent call last) ~\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\dagster\\core\\execution\\plan\\execute_plan.py in _dagster_event_sequence_for_step(step_context, retries) 211 --> 212 for step_event in check.generator(step_events): 213 yield step_event ~\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\dagster\\core\\execution\\plan\\execute_step.py in core_dagster_event_sequence_for_step(step_context, prior_attempt_count) 285 # timer block above in order for time to be recorded accurately. --> 286 for user_event in check.generator( 287 _step_output_error_checked_user_event_sequence(step_context, user_event_sequence) ~\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\dagster\\core\\execution\\plan\\execute_step.py in _step_output_error_checked_user_event_sequence(step_context, user_event_sequence) 58 ---> 59 for user_event in user_event_sequence: 60 if not isinstance(user_event, Output): ~\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\dagster\\core\\execution\\plan\\execute_step.py in _user_event_sequence_for_step_compute_fn(step_context, evaluated_inputs) 475 for event in iterate_with_context(raise_interrupts_immediately, gen): --> 476 yield event 477 ~\\anaconda3\\envs\\satip_dev\\lib\\contextlib.py in __exit__(self, type, value, traceback) 130 try: --> 131 self.gen.throw(type, value, traceback) 132 except StopIteration as exc: ~\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\dagster\\core\\errors.py in user_code_error_boundary(error_cls, msg_fn, control_flow_exceptions, **kwargs) 189 # with the error reported further up the stack --> 190 raise_from( 191 error_cls(msg_fn(), user_exception=e, original_exc_info=sys.exc_info(), **kwargs), e ~\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\future\\utils\\__init__.py in raise_from(exc, cause) 402 execstr = \"raise __python_future_raise_from_exc from __python_future_raise_from_cause\" --> 403 exec(execstr, myglobals, mylocals) 404 ~\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\dagster\\core\\errors.py in <module> DagsterExecutionStepExecutionError: Error occurred during the execution of step: step key: \"download_latest_eumetsat_files.compute\" solid invocation: \"download_latest_eumetsat_files\" solid definition: \"download_latest_eumetsat_files\" During handling of the above exception, another exception occurred: AttributeError Traceback (most recent call last) <ipython-input-7-79fdcc951128> in <module> 35 } 36 ---> 37 execute_pipeline(download_latest_data_pipeline, run_config=run_config) ~\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\dagster\\core\\execution\\api.py in execute_pipeline(pipeline, run_config, mode, preset, tags, solid_selection, instance, raise_on_error) 322 323 with _ephemeral_instance_if_missing(instance) as execute_instance: --> 324 return _logged_execute_pipeline( 325 pipeline, 326 instance=execute_instance, ~\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\dagster\\core\\telemetry.py in wrap(*args, **kwargs) 87 start_time = datetime.datetime.now() 88 log_action(instance=instance, action=f.__name__ + \"_started\", client_time=start_time) ---> 89 result = f(*args, **kwargs) 90 end_time = datetime.datetime.now() 91 log_action( ~\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\dagster\\core\\execution\\api.py in _logged_execute_pipeline(pipeline, instance, run_config, mode, preset, tags, solid_selection, raise_on_error) 373 ) 374 --> 375 return execute_run(pipeline, pipeline_run, instance, raise_on_error=raise_on_error) 376 377 ~\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\dagster\\core\\execution\\api.py in execute_run(pipeline, pipeline_run, instance, raise_on_error) 175 ), 176 ) --> 177 event_list = list(_execute_run_iterable) 178 pipeline_context = _execute_run_iterable.pipeline_context 179 ~\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\dagster\\core\\execution\\api.py in __iter__(self) 725 try: 726 if self.pipeline_context: # False if we had a pipeline init failure --> 727 for event in self.iterator( 728 execution_plan=self.execution_plan, pipeline_context=self.pipeline_context, 729 ): ~\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\dagster\\core\\execution\\api.py in _pipeline_execution_iterator(pipeline_context, execution_plan) 663 generator_closed = False 664 try: --> 665 for event in pipeline_context.executor.execute(pipeline_context, execution_plan): 666 if event.is_step_failure: 667 failed_steps.append(event.step_key) ~\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\dagster\\core\\executor\\in_process.py in execute(self, pipeline_context, execution_plan) 34 35 with time_execution_scope() as timer_result: ---> 36 for event in inner_plan_execution_iterator(pipeline_context, execution_plan): 37 yield event 38 ~\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\dagster\\core\\execution\\plan\\execute_plan.py in inner_plan_execution_iterator(pipeline_context, execution_plan) 75 active_execution.mark_skipped(step.key) 76 else: ---> 77 for step_event in check.generator( 78 _dagster_event_sequence_for_step(step_context, retries) 79 ): ~\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\dagster\\core\\execution\\plan\\execute_plan.py in _dagster_event_sequence_for_step(step_context, retries) 270 271 if step_context.raise_on_error: --> 272 raise dagster_user_error.user_exception 273 274 # case (4) in top comment ~\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\dagster\\core\\errors.py in user_code_error_boundary(error_cls, msg_fn, control_flow_exceptions, **kwargs) 178 ) 179 try: --> 180 yield 181 except control_flow_exceptions as cf: 182 # A control flow exception has occurred and should be propagated ~\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\dagster\\core\\execution\\plan\\execute_step.py in _user_event_sequence_for_step_compute_fn(step_context, evaluated_inputs) 473 474 # Allow interrupts again during each step of the execution --> 475 for event in iterate_with_context(raise_interrupts_immediately, gen): 476 yield event 477 ~\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\dagster\\utils\\__init__.py in iterate_with_context(context_manager_class, iterator) 441 with context_manager_class(): 442 try: --> 443 next_output = next(iterator) 444 except StopIteration: 445 return ~\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\dagster\\core\\execution\\plan\\compute.py in _execute_core_compute(compute_context, inputs, compute_fn) 103 104 all_results = [] --> 105 for step_output in _yield_compute_results(compute_context, inputs, compute_fn): 106 yield step_output 107 if isinstance(step_output, Output): ~\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\dagster\\core\\execution\\plan\\compute.py in _yield_compute_results(compute_context, inputs, compute_fn) 74 return 75 ---> 76 for event in user_event_sequence: 77 if isinstance(event, (Output, AssetMaterialization, Materialization, ExpectationResult)): 78 yield event ~\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\dagster\\core\\definitions\\decorators\\solid.py in compute(context, input_defs) 225 kwargs[input_name] = input_defs[input_name] 226 --> 227 result = fn(context, **kwargs) 228 229 if inspect.isgenerator(result): <ipython-input-5-968917ba6d74> in download_latest_eumetsat_files(context, data_dir, metadata_db_fp, debug_fp, start_date) 14 end_date = pd.Timestamp.now().strftime('%Y-%m-%d %H:%M') 15 ---> 16 dm = eumetsat.DownloadManager(context.solid_config['user_key'], context.solid_config['user_secret'], data_dir, metadata_db_fp, debug_fp, slack_webhook_url=context.solid_config['slack_webhook_url'], slack_id=context.solid_config['slack_id'], logger_name=context.log) 17 df_new_metadata = dm.download_datasets(start_date, end_date) 18 c:\\users\\ayrto\\desktop\\freelance work\\fea\\work\\ocf\\satip\\satip\\eumetsat.py in __init__(self, user_key, user_secret, data_dir, metadata_db_fp, log_fp, main_logging_level, slack_logging_level, slack_webhook_url, slack_id, bucket_name, bucket_prefix, logger_name) 310 311 # Configuring the logger --> 312 self.logger = utils.set_up_logging(logger_name, log_fp, 313 main_logging_level, slack_logging_level, 314 slack_webhook_url, slack_id) c:\\users\\ayrto\\desktop\\freelance work\\fea\\work\\ocf\\satip\\satip\\utils.py in set_up_logging(name, log_dir, main_logging_level, slack_logging_level, slack_webhook_url, slack_id) 131 assert slack_logging_level in logging_levels, f\"slack_logging_level must be one of {', '.join(logging_levels)}\" 132 --> 133 logger.setLevel(getattr(logging, main_logging_level)) 134 135 # Defining global formatter AttributeError: 'DagsterLogManager' object has no attribute 'setLevel'","title":"Pipelines"},{"location":"05_pipeline/#end-to-end-pipeline","text":"#exports import pandas as pd import xarray as xr from satip import eumetsat , reproj , io , gcp_helpers from dagster import execute_pipeline , pipeline , solid , Field import os import dotenv C:\\Users\\Ayrto\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\google\\auth\\_default.py:69: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK without a quota project. You might receive a \"quota exceeded\" or \"API not enabled\" error. We recommend you rerun `gcloud auth application-default login` and make sure a quota project is added. Or you can use service accounts instead. For more information about service accounts, see https://cloud.google.com/docs/authentication/ warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING) Downloading: 100%|\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6| 1/1 [00:00<00:00, 3.21rows/s]","title":"End-to-End Pipeline"},{"location":"05_pipeline/#user-inputs","text":"data_dir = '../data/raw' sorted_dir = '../data/sorted' debug_fp = '../logs/EUMETSAT_download.txt' env_vars_fp = '../.env' metadata_db_fp = '../data/EUMETSAT_metadata.db' new_grid_fp = '../data/intermediate/new_grid_4km_TM.json' new_coords_fp = '../data/intermediate/reproj_coords_TM_4km.csv' zarr_bucket = 'solar-pv-nowcasting-data/satellite/EUMETSAT/SEVIRI_RSS/full_extent_TM_int16'","title":"User Inputs"},{"location":"05_pipeline/#loading-environment-variables","text":"#exports user_key = os . environ . get ( 'USER_KEY' ) user_secret = os . environ . get ( 'USER_SECRET' ) slack_id = os . environ . get ( 'SLACK_ID' ) slack_webhook_url = os . environ . get ( 'SLACK_WEBHOOK_URL' )","title":"Loading Environment Variables"},{"location":"05_pipeline/#dagster-pipeline","text":"We're now going to combine these steps into a pipeline using dagster #exports @solid ( config_schema = { 'user_key' : Field ( str , default_value = user_key , is_required = False ), 'user_secret' : Field ( str , default_value = user_secret , is_required = False ), 'slack_webhook_url' : Field ( str , default_value = slack_webhook_url , is_required = False ), 'slack_id' : Field ( str , default_value = slack_id , is_required = False ) } ) def download_latest_eumetsat_files ( context , data_dir : str , metadata_db_fp : str , debug_fp : str , table_id : str , project_id : str , start_date : str = '' ): if start_date == '' : sql_query = f 'select * from { table_id } where result_time = (select max(result_time) from { table_id } )' start_date = gcp_helpers . query ( sql_query , project_id )[ 'result_time' ] . iloc [ 0 ] end_date = pd . Timestamp . now () . strftime ( '%Y-%m- %d %H:%M' ) dm = eumetsat . DownloadManager ( context . solid_config [ 'user_key' ], context . solid_config [ 'user_secret' ], data_dir , metadata_db_fp , debug_fp , slack_webhook_url = context . solid_config [ 'slack_webhook_url' ], slack_id = context . solid_config [ 'slack_id' ]) df_new_metadata = dm . download_datasets ( start_date , end_date ) if df_new_metadata is None : df_new_metadata = pd . DataFrame ( columns = [ 'result_time' , 'file_name' ]) else : df_new_metadata = df_new_metadata . iloc [ 1 :] # the first entry is the last one we downloaded return df_new_metadata @solid () def df_metadata_to_dt_to_fp_map ( _ , df_new_metadata , data_dir : str ) -> dict : \"\"\" Here we'll then identify downloaded files in the metadata dataframe and return a mapping between datetimes and filenames \"\"\" datetime_to_filename = ( df_new_metadata . set_index ( 'result_time' ) [ 'file_name' ] . drop_duplicates () . to_dict () ) datetime_to_filepath = { datetime : f \" { data_dir } / { filename } .nat\" for datetime , filename in datetime_to_filename . items () if filename != {} } return datetime_to_filepath @solid () def reproject_datasets ( _ , datetime_to_filepath : dict , new_coords_fp : str , new_grid_fp : str ): reprojector = reproj . Reprojector ( new_coords_fp , new_grid_fp ) reprojected_dss = [ ( reprojector . reproject ( filepath , reproj_library = 'pyresample' ) . pipe ( io . add_constant_coord_to_da , 'time' , pd . to_datetime ( datetime )) ) for datetime , filepath in datetime_to_filepath . items () ] if len ( reprojected_dss ) > 0 : ds_combined_reproj = xr . concat ( reprojected_dss , 'time' , coords = 'all' , data_vars = 'all' ) return ds_combined_reproj else : return xr . Dataset () @solid () def compress_and_save_datasets ( _ , ds_combined_reproj , zarr_bucket : str , var_name : str = 'stacked_eumetsat_data' ): # Handle case where no new data exists if len ( ds_combined_reproj . dims ) == 0 : return # Compressing the datasets compressor = io . Compressor () var_name = var_name da_compressed = compressor . compress ( ds_combined_reproj [ var_name ]) # Saving to Zarr ds_compressed = io . save_da_to_zarr ( da_compressed , zarr_bucket ) return ds_compressed @solid () def save_metadata ( _ , df_new_metadata , table_id : str , project_id : str ): gcp_helpers . write_metadata_to_gcp ( df_new_metadata , table_id , project_id , append = True ) #exports @pipeline def download_latest_data_pipeline (): # Retrieving data, reprojecting, compressing, and saving to GCP df_new_metadata = download_latest_eumetsat_files () datetime_to_filepath = df_metadata_to_dt_to_fp_map ( df_new_metadata ) ds_combined_reproj = reproject_datasets ( datetime_to_filepath ) ds_combined_compressed = compress_and_save_datasets ( ds_combined_reproj ) save_metadata ( df_new_metadata ) run_config = { 'solids' : { 'download_latest_eumetsat_files' : { 'inputs' : { 'data_dir' : \"../data/raw\" , 'metadata_db_fp' : \"../data/EUMETSAT_metadata.db\" , 'debug_fp' : \"../logs/EUMETSAT_download.txt\" , 'table_id' : \"eumetsat.metadata\" , 'project_id' : \"solar-pv-nowcasting\" , 'start_date' : \"2020-12-16 19:30\" }, }, 'df_metadata_to_dt_to_fp_map' : { 'inputs' : { 'data_dir' : \"../data/raw\" } }, 'reproject_datasets' : { 'inputs' : { 'new_coords_fp' : \"../data/intermediate/reproj_coords_TM_4km.csv\" , 'new_grid_fp' : \"../data/intermediate/new_grid_4km_TM.json\" } }, 'compress_and_save_datasets' : { 'inputs' : { 'zarr_bucket' : \"solar-pv-nowcasting-data/satellite/EUMETSAT/SEVIRI_RSS/full_extent_TM_int16\" , 'var_name' : \"stacked_eumetsat_data\" } }, 'save_metadata' : { 'inputs' : { 'table_id' : \"eumetsat.metadata\" , 'project_id' : \"solar-pv-nowcasting\" }, } } } execute_pipeline ( download_latest_data_pipeline , run_config = run_config ) 2020-12-16 20:00:18 - dagster - DEBUG - download_latest_data_pipeline - 843966a6-a689-430d-97e0-7c937aefc85e - 20040 - ENGINE_EVENT - Starting initialization of resources [asset_store]. 2020-12-16 20:00:18 - dagster - DEBUG - download_latest_data_pipeline - 843966a6-a689-430d-97e0-7c937aefc85e - 20040 - ENGINE_EVENT - Finished initialization of resources [asset_store]. 2020-12-16 20:00:18 - dagster - DEBUG - download_latest_data_pipeline - 843966a6-a689-430d-97e0-7c937aefc85e - 20040 - PIPELINE_START - Started execution of pipeline \"download_latest_data_pipeline\". 2020-12-16 20:00:18 - dagster - DEBUG - download_latest_data_pipeline - 843966a6-a689-430d-97e0-7c937aefc85e - 20040 - ENGINE_EVENT - Executing steps in process (pid: 20040) 2020-12-16 20:00:18 - dagster - DEBUG - download_latest_data_pipeline - 843966a6-a689-430d-97e0-7c937aefc85e - 20040 - download_latest_eumetsat_files.compute - STEP_START - Started execution of step \"download_latest_eumetsat_files.compute\". 2020-12-16 20:00:18 - dagster - DEBUG - download_latest_data_pipeline - 843966a6-a689-430d-97e0-7c937aefc85e - 20040 - download_latest_eumetsat_files.compute - STEP_INPUT - Got input \"data_dir\" of type \"String\". (Type check passed). 2020-12-16 20:00:18 - dagster - DEBUG - download_latest_data_pipeline - 843966a6-a689-430d-97e0-7c937aefc85e - 20040 - download_latest_eumetsat_files.compute - STEP_INPUT - Got input \"metadata_db_fp\" of type \"String\". (Type check passed). 2020-12-16 20:00:18 - dagster - DEBUG - download_latest_data_pipeline - 843966a6-a689-430d-97e0-7c937aefc85e - 20040 - download_latest_eumetsat_files.compute - STEP_INPUT - Got input \"debug_fp\" of type \"String\". (Type check passed). 2020-12-16 20:00:18 - dagster - DEBUG - download_latest_data_pipeline - 843966a6-a689-430d-97e0-7c937aefc85e - 20040 - download_latest_eumetsat_files.compute - STEP_INPUT - Got input \"start_date\" of type \"String\". (Type check passed). 2020-12-16 20:00:19 - dagster - ERROR - download_latest_data_pipeline - 843966a6-a689-430d-97e0-7c937aefc85e - 20040 - download_latest_eumetsat_files.compute - STEP_FAILURE - Execution of step \"download_latest_eumetsat_files.compute\" failed. AttributeError: 'DagsterLogManager' object has no attribute 'setLevel' File \"C:\\Users\\Ayrto\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\dagster\\core\\errors.py\", line 180, in user_code_error_boundary yield File \"C:\\Users\\Ayrto\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\dagster\\core\\execution\\plan\\execute_step.py\", line 475, in _user_event_sequence_for_step_compute_fn for event in iterate_with_context(raise_interrupts_immediately, gen): File \"C:\\Users\\Ayrto\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\dagster\\utils\\__init__.py\", line 443, in iterate_with_context next_output = next(iterator) File \"C:\\Users\\Ayrto\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\dagster\\core\\execution\\plan\\compute.py\", line 105, in _execute_core_compute for step_output in _yield_compute_results(compute_context, inputs, compute_fn): File \"C:\\Users\\Ayrto\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\dagster\\core\\execution\\plan\\compute.py\", line 76, in _yield_compute_results for event in user_event_sequence: File \"C:\\Users\\Ayrto\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\dagster\\core\\definitions\\decorators\\solid.py\", line 227, in compute result = fn(context, **kwargs) File \"<ipython-input-5-968917ba6d74>\", line 16, in download_latest_eumetsat_files dm = eumetsat.DownloadManager(context.solid_config['user_key'], context.solid_config['user_secret'], data_dir, metadata_db_fp, debug_fp, slack_webhook_url=context.solid_config['slack_webhook_url'], slack_id=context.solid_config['slack_id'], logger_name=context.log) File \"c:\\users\\ayrto\\desktop\\freelance work\\fea\\work\\ocf\\satip\\satip\\eumetsat.py\", line 312, in __init__ self.logger = utils.set_up_logging(logger_name, log_fp, File \"c:\\users\\ayrto\\desktop\\freelance work\\fea\\work\\ocf\\satip\\satip\\utils.py\", line 133, in set_up_logging logger.setLevel(getattr(logging, main_logging_level)) 2020-12-16 20:00:19 - dagster - ERROR - download_latest_data_pipeline - 843966a6-a689-430d-97e0-7c937aefc85e - 20040 - PIPELINE_FAILURE - Execution of pipeline \"download_latest_data_pipeline\" failed. An exception was thrown during execution. AttributeError: 'DagsterLogManager' object has no attribute 'setLevel' File \"C:\\Users\\Ayrto\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\dagster\\core\\execution\\api.py\", line 665, in _pipeline_execution_iterator for event in pipeline_context.executor.execute(pipeline_context, execution_plan): File \"C:\\Users\\Ayrto\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\dagster\\core\\executor\\in_process.py\", line 36, in execute for event in inner_plan_execution_iterator(pipeline_context, execution_plan): File \"C:\\Users\\Ayrto\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\dagster\\core\\execution\\plan\\execute_plan.py\", line 77, in inner_plan_execution_iterator for step_event in check.generator( File \"C:\\Users\\Ayrto\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\dagster\\core\\execution\\plan\\execute_plan.py\", line 272, in _dagster_event_sequence_for_step raise dagster_user_error.user_exception File \"C:\\Users\\Ayrto\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\dagster\\core\\errors.py\", line 180, in user_code_error_boundary yield File \"C:\\Users\\Ayrto\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\dagster\\core\\execution\\plan\\execute_step.py\", line 475, in _user_event_sequence_for_step_compute_fn for event in iterate_with_context(raise_interrupts_immediately, gen): File \"C:\\Users\\Ayrto\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\dagster\\utils\\__init__.py\", line 443, in iterate_with_context next_output = next(iterator) File \"C:\\Users\\Ayrto\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\dagster\\core\\execution\\plan\\compute.py\", line 105, in _execute_core_compute for step_output in _yield_compute_results(compute_context, inputs, compute_fn): File \"C:\\Users\\Ayrto\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\dagster\\core\\execution\\plan\\compute.py\", line 76, in _yield_compute_results for event in user_event_sequence: File \"C:\\Users\\Ayrto\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\dagster\\core\\definitions\\decorators\\solid.py\", line 227, in compute result = fn(context, **kwargs) File \"<ipython-input-5-968917ba6d74>\", line 16, in download_latest_eumetsat_files dm = eumetsat.DownloadManager(context.solid_config['user_key'], context.solid_config['user_secret'], data_dir, metadata_db_fp, debug_fp, slack_webhook_url=context.solid_config['slack_webhook_url'], slack_id=context.solid_config['slack_id'], logger_name=context.log) File \"c:\\users\\ayrto\\desktop\\freelance work\\fea\\work\\ocf\\satip\\satip\\eumetsat.py\", line 312, in __init__ self.logger = utils.set_up_logging(logger_name, log_fp, File \"c:\\users\\ayrto\\desktop\\freelance work\\fea\\work\\ocf\\satip\\satip\\utils.py\", line 133, in set_up_logging logger.setLevel(getattr(logging, main_logging_level)) --------------------------------------------------------------------------- DagsterExecutionStepExecutionError Traceback (most recent call last) ~\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\dagster\\core\\execution\\plan\\execute_plan.py in _dagster_event_sequence_for_step(step_context, retries) 211 --> 212 for step_event in check.generator(step_events): 213 yield step_event ~\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\dagster\\core\\execution\\plan\\execute_step.py in core_dagster_event_sequence_for_step(step_context, prior_attempt_count) 285 # timer block above in order for time to be recorded accurately. --> 286 for user_event in check.generator( 287 _step_output_error_checked_user_event_sequence(step_context, user_event_sequence) ~\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\dagster\\core\\execution\\plan\\execute_step.py in _step_output_error_checked_user_event_sequence(step_context, user_event_sequence) 58 ---> 59 for user_event in user_event_sequence: 60 if not isinstance(user_event, Output): ~\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\dagster\\core\\execution\\plan\\execute_step.py in _user_event_sequence_for_step_compute_fn(step_context, evaluated_inputs) 475 for event in iterate_with_context(raise_interrupts_immediately, gen): --> 476 yield event 477 ~\\anaconda3\\envs\\satip_dev\\lib\\contextlib.py in __exit__(self, type, value, traceback) 130 try: --> 131 self.gen.throw(type, value, traceback) 132 except StopIteration as exc: ~\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\dagster\\core\\errors.py in user_code_error_boundary(error_cls, msg_fn, control_flow_exceptions, **kwargs) 189 # with the error reported further up the stack --> 190 raise_from( 191 error_cls(msg_fn(), user_exception=e, original_exc_info=sys.exc_info(), **kwargs), e ~\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\future\\utils\\__init__.py in raise_from(exc, cause) 402 execstr = \"raise __python_future_raise_from_exc from __python_future_raise_from_cause\" --> 403 exec(execstr, myglobals, mylocals) 404 ~\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\dagster\\core\\errors.py in <module> DagsterExecutionStepExecutionError: Error occurred during the execution of step: step key: \"download_latest_eumetsat_files.compute\" solid invocation: \"download_latest_eumetsat_files\" solid definition: \"download_latest_eumetsat_files\" During handling of the above exception, another exception occurred: AttributeError Traceback (most recent call last) [... skipping hidden 1 frame] <ipython-input-7-79fdcc951128> in <module> 36 ---> 37 execute_pipeline(download_latest_data_pipeline, run_config=run_config) ~\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\dagster\\core\\execution\\api.py in execute_pipeline(pipeline, run_config, mode, preset, tags, solid_selection, instance, raise_on_error) 323 with _ephemeral_instance_if_missing(instance) as execute_instance: --> 324 return _logged_execute_pipeline( 325 pipeline, ~\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\dagster\\core\\telemetry.py in wrap(*args, **kwargs) 88 log_action(instance=instance, action=f.__name__ + \"_started\", client_time=start_time) ---> 89 result = f(*args, **kwargs) 90 end_time = datetime.datetime.now() ~\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\dagster\\core\\execution\\api.py in _logged_execute_pipeline(pipeline, instance, run_config, mode, preset, tags, solid_selection, raise_on_error) 374 --> 375 return execute_run(pipeline, pipeline_run, instance, raise_on_error=raise_on_error) 376 ~\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\dagster\\core\\execution\\api.py in execute_run(pipeline, pipeline_run, instance, raise_on_error) 176 ) --> 177 event_list = list(_execute_run_iterable) 178 pipeline_context = _execute_run_iterable.pipeline_context ~\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\dagster\\core\\execution\\api.py in __iter__(self) 726 if self.pipeline_context: # False if we had a pipeline init failure --> 727 for event in self.iterator( 728 execution_plan=self.execution_plan, pipeline_context=self.pipeline_context, ~\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\dagster\\core\\execution\\api.py in _pipeline_execution_iterator(pipeline_context, execution_plan) 664 try: --> 665 for event in pipeline_context.executor.execute(pipeline_context, execution_plan): 666 if event.is_step_failure: ~\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\dagster\\core\\executor\\in_process.py in execute(self, pipeline_context, execution_plan) 35 with time_execution_scope() as timer_result: ---> 36 for event in inner_plan_execution_iterator(pipeline_context, execution_plan): 37 yield event ~\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\dagster\\core\\execution\\plan\\execute_plan.py in inner_plan_execution_iterator(pipeline_context, execution_plan) 76 else: ---> 77 for step_event in check.generator( 78 _dagster_event_sequence_for_step(step_context, retries) ~\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\dagster\\core\\execution\\plan\\execute_plan.py in _dagster_event_sequence_for_step(step_context, retries) 271 if step_context.raise_on_error: --> 272 raise dagster_user_error.user_exception 273 ~\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\dagster\\core\\errors.py in user_code_error_boundary(error_cls, msg_fn, control_flow_exceptions, **kwargs) 179 try: --> 180 yield 181 except control_flow_exceptions as cf: ~\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\dagster\\core\\execution\\plan\\execute_step.py in _user_event_sequence_for_step_compute_fn(step_context, evaluated_inputs) 474 # Allow interrupts again during each step of the execution --> 475 for event in iterate_with_context(raise_interrupts_immediately, gen): 476 yield event ~\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\dagster\\utils\\__init__.py in iterate_with_context(context_manager_class, iterator) 442 try: --> 443 next_output = next(iterator) 444 except StopIteration: ~\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\dagster\\core\\execution\\plan\\compute.py in _execute_core_compute(compute_context, inputs, compute_fn) 104 all_results = [] --> 105 for step_output in _yield_compute_results(compute_context, inputs, compute_fn): 106 yield step_output ~\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\dagster\\core\\execution\\plan\\compute.py in _yield_compute_results(compute_context, inputs, compute_fn) 75 ---> 76 for event in user_event_sequence: 77 if isinstance(event, (Output, AssetMaterialization, Materialization, ExpectationResult)): ~\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\dagster\\core\\definitions\\decorators\\solid.py in compute(context, input_defs) 226 --> 227 result = fn(context, **kwargs) 228 <ipython-input-5-968917ba6d74> in download_latest_eumetsat_files(context, data_dir, metadata_db_fp, debug_fp, start_date) 15 ---> 16 dm = eumetsat.DownloadManager(context.solid_config['user_key'], context.solid_config['user_secret'], data_dir, metadata_db_fp, debug_fp, slack_webhook_url=context.solid_config['slack_webhook_url'], slack_id=context.solid_config['slack_id'], logger_name=context.log) 17 df_new_metadata = dm.download_datasets(start_date, end_date) c:\\users\\ayrto\\desktop\\freelance work\\fea\\work\\ocf\\satip\\satip\\eumetsat.py in __init__(self, user_key, user_secret, data_dir, metadata_db_fp, log_fp, main_logging_level, slack_logging_level, slack_webhook_url, slack_id, bucket_name, bucket_prefix, logger_name) 311 # Configuring the logger --> 312 self.logger = utils.set_up_logging(logger_name, log_fp, 313 main_logging_level, slack_logging_level, c:\\users\\ayrto\\desktop\\freelance work\\fea\\work\\ocf\\satip\\satip\\utils.py in set_up_logging(name, log_dir, main_logging_level, slack_logging_level, slack_webhook_url, slack_id) 132 --> 133 logger.setLevel(getattr(logging, main_logging_level)) 134 AttributeError: 'DagsterLogManager' object has no attribute 'setLevel' The above exception was the direct cause of the following exception: DagsterExecutionStepExecutionError Traceback (most recent call last) ~\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\dagster\\core\\execution\\plan\\execute_plan.py in _dagster_event_sequence_for_step(step_context, retries) 211 --> 212 for step_event in check.generator(step_events): 213 yield step_event ~\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\dagster\\core\\execution\\plan\\execute_step.py in core_dagster_event_sequence_for_step(step_context, prior_attempt_count) 285 # timer block above in order for time to be recorded accurately. --> 286 for user_event in check.generator( 287 _step_output_error_checked_user_event_sequence(step_context, user_event_sequence) ~\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\dagster\\core\\execution\\plan\\execute_step.py in _step_output_error_checked_user_event_sequence(step_context, user_event_sequence) 58 ---> 59 for user_event in user_event_sequence: 60 if not isinstance(user_event, Output): ~\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\dagster\\core\\execution\\plan\\execute_step.py in _user_event_sequence_for_step_compute_fn(step_context, evaluated_inputs) 475 for event in iterate_with_context(raise_interrupts_immediately, gen): --> 476 yield event 477 ~\\anaconda3\\envs\\satip_dev\\lib\\contextlib.py in __exit__(self, type, value, traceback) 130 try: --> 131 self.gen.throw(type, value, traceback) 132 except StopIteration as exc: ~\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\dagster\\core\\errors.py in user_code_error_boundary(error_cls, msg_fn, control_flow_exceptions, **kwargs) 189 # with the error reported further up the stack --> 190 raise_from( 191 error_cls(msg_fn(), user_exception=e, original_exc_info=sys.exc_info(), **kwargs), e ~\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\future\\utils\\__init__.py in raise_from(exc, cause) 402 execstr = \"raise __python_future_raise_from_exc from __python_future_raise_from_cause\" --> 403 exec(execstr, myglobals, mylocals) 404 ~\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\dagster\\core\\errors.py in <module> DagsterExecutionStepExecutionError: Error occurred during the execution of step: step key: \"download_latest_eumetsat_files.compute\" solid invocation: \"download_latest_eumetsat_files\" solid definition: \"download_latest_eumetsat_files\" During handling of the above exception, another exception occurred: AttributeError Traceback (most recent call last) <ipython-input-7-79fdcc951128> in <module> 35 } 36 ---> 37 execute_pipeline(download_latest_data_pipeline, run_config=run_config) ~\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\dagster\\core\\execution\\api.py in execute_pipeline(pipeline, run_config, mode, preset, tags, solid_selection, instance, raise_on_error) 322 323 with _ephemeral_instance_if_missing(instance) as execute_instance: --> 324 return _logged_execute_pipeline( 325 pipeline, 326 instance=execute_instance, ~\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\dagster\\core\\telemetry.py in wrap(*args, **kwargs) 87 start_time = datetime.datetime.now() 88 log_action(instance=instance, action=f.__name__ + \"_started\", client_time=start_time) ---> 89 result = f(*args, **kwargs) 90 end_time = datetime.datetime.now() 91 log_action( ~\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\dagster\\core\\execution\\api.py in _logged_execute_pipeline(pipeline, instance, run_config, mode, preset, tags, solid_selection, raise_on_error) 373 ) 374 --> 375 return execute_run(pipeline, pipeline_run, instance, raise_on_error=raise_on_error) 376 377 ~\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\dagster\\core\\execution\\api.py in execute_run(pipeline, pipeline_run, instance, raise_on_error) 175 ), 176 ) --> 177 event_list = list(_execute_run_iterable) 178 pipeline_context = _execute_run_iterable.pipeline_context 179 ~\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\dagster\\core\\execution\\api.py in __iter__(self) 725 try: 726 if self.pipeline_context: # False if we had a pipeline init failure --> 727 for event in self.iterator( 728 execution_plan=self.execution_plan, pipeline_context=self.pipeline_context, 729 ): ~\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\dagster\\core\\execution\\api.py in _pipeline_execution_iterator(pipeline_context, execution_plan) 663 generator_closed = False 664 try: --> 665 for event in pipeline_context.executor.execute(pipeline_context, execution_plan): 666 if event.is_step_failure: 667 failed_steps.append(event.step_key) ~\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\dagster\\core\\executor\\in_process.py in execute(self, pipeline_context, execution_plan) 34 35 with time_execution_scope() as timer_result: ---> 36 for event in inner_plan_execution_iterator(pipeline_context, execution_plan): 37 yield event 38 ~\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\dagster\\core\\execution\\plan\\execute_plan.py in inner_plan_execution_iterator(pipeline_context, execution_plan) 75 active_execution.mark_skipped(step.key) 76 else: ---> 77 for step_event in check.generator( 78 _dagster_event_sequence_for_step(step_context, retries) 79 ): ~\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\dagster\\core\\execution\\plan\\execute_plan.py in _dagster_event_sequence_for_step(step_context, retries) 270 271 if step_context.raise_on_error: --> 272 raise dagster_user_error.user_exception 273 274 # case (4) in top comment ~\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\dagster\\core\\errors.py in user_code_error_boundary(error_cls, msg_fn, control_flow_exceptions, **kwargs) 178 ) 179 try: --> 180 yield 181 except control_flow_exceptions as cf: 182 # A control flow exception has occurred and should be propagated ~\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\dagster\\core\\execution\\plan\\execute_step.py in _user_event_sequence_for_step_compute_fn(step_context, evaluated_inputs) 473 474 # Allow interrupts again during each step of the execution --> 475 for event in iterate_with_context(raise_interrupts_immediately, gen): 476 yield event 477 ~\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\dagster\\utils\\__init__.py in iterate_with_context(context_manager_class, iterator) 441 with context_manager_class(): 442 try: --> 443 next_output = next(iterator) 444 except StopIteration: 445 return ~\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\dagster\\core\\execution\\plan\\compute.py in _execute_core_compute(compute_context, inputs, compute_fn) 103 104 all_results = [] --> 105 for step_output in _yield_compute_results(compute_context, inputs, compute_fn): 106 yield step_output 107 if isinstance(step_output, Output): ~\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\dagster\\core\\execution\\plan\\compute.py in _yield_compute_results(compute_context, inputs, compute_fn) 74 return 75 ---> 76 for event in user_event_sequence: 77 if isinstance(event, (Output, AssetMaterialization, Materialization, ExpectationResult)): 78 yield event ~\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\dagster\\core\\definitions\\decorators\\solid.py in compute(context, input_defs) 225 kwargs[input_name] = input_defs[input_name] 226 --> 227 result = fn(context, **kwargs) 228 229 if inspect.isgenerator(result): <ipython-input-5-968917ba6d74> in download_latest_eumetsat_files(context, data_dir, metadata_db_fp, debug_fp, start_date) 14 end_date = pd.Timestamp.now().strftime('%Y-%m-%d %H:%M') 15 ---> 16 dm = eumetsat.DownloadManager(context.solid_config['user_key'], context.solid_config['user_secret'], data_dir, metadata_db_fp, debug_fp, slack_webhook_url=context.solid_config['slack_webhook_url'], slack_id=context.solid_config['slack_id'], logger_name=context.log) 17 df_new_metadata = dm.download_datasets(start_date, end_date) 18 c:\\users\\ayrto\\desktop\\freelance work\\fea\\work\\ocf\\satip\\satip\\eumetsat.py in __init__(self, user_key, user_secret, data_dir, metadata_db_fp, log_fp, main_logging_level, slack_logging_level, slack_webhook_url, slack_id, bucket_name, bucket_prefix, logger_name) 310 311 # Configuring the logger --> 312 self.logger = utils.set_up_logging(logger_name, log_fp, 313 main_logging_level, slack_logging_level, 314 slack_webhook_url, slack_id) c:\\users\\ayrto\\desktop\\freelance work\\fea\\work\\ocf\\satip\\satip\\utils.py in set_up_logging(name, log_dir, main_logging_level, slack_logging_level, slack_webhook_url, slack_id) 131 assert slack_logging_level in logging_levels, f\"slack_logging_level must be one of {', '.join(logging_levels)}\" 132 --> 133 logger.setLevel(getattr(logging, main_logging_level)) 134 135 # Defining global formatter AttributeError: 'DagsterLogManager' object has no attribute 'setLevel'","title":"Dagster Pipeline"},{"location":"103_loading/","text":"Loading from Zarr \u00b6 from satip import io import matplotlib.pyplot as plt import cartopy.crs as ccrs C:\\Users\\Ayrto\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\google\\auth\\_default.py:69: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK without a quota project. You might receive a \"quota exceeded\" or \"API not enabled\" error. We recommend you rerun `gcloud auth application-default login` and make sure a quota project is added. Or you can use service accounts instead. For more information about service accounts, see https://cloud.google.com/docs/authentication/ warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING) User Inputs \u00b6 We have to specify the bucket where the data is located zarr_bucket = 'solar-pv-nowcasting-data/satellite/EUMETSAT/SEVIRI_RSS/full_extent_TM_int16' Loading Data \u00b6 Then the satip wrapper for loading data will generate an xarray Dataset ds = io . load_from_zarr_bucket ( zarr_bucket ) ds We can then index this as we would any other xarray object da_HRV_sample = ds [ 'stacked_eumetsat_data' ] . isel ( time = 0 ) . sel ( variable = 'HRV' ) da_HRV_sample As well as visualise it, here we'll use cartopy to plot the data with a coastline overlay. The darker area on the right hand side of the image are the areas where the sun has already set. fig = plt . figure ( dpi = 250 , figsize = ( 10 , 10 )) ax = plt . axes ( projection = ccrs . TransverseMercator ()) da_HRV_sample . T . plot . imshow ( ax = ax , cmap = 'magma' , vmin =- 200 , vmax = 400 ) ax . coastlines ( resolution = '50m' , alpha = 0.8 , color = 'white' )","title":"Loading from Zarr"},{"location":"103_loading/#loading-from-zarr","text":"from satip import io import matplotlib.pyplot as plt import cartopy.crs as ccrs C:\\Users\\Ayrto\\anaconda3\\envs\\satip_dev\\lib\\site-packages\\google\\auth\\_default.py:69: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK without a quota project. You might receive a \"quota exceeded\" or \"API not enabled\" error. We recommend you rerun `gcloud auth application-default login` and make sure a quota project is added. Or you can use service accounts instead. For more information about service accounts, see https://cloud.google.com/docs/authentication/ warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)","title":"Loading from Zarr"},{"location":"103_loading/#user-inputs","text":"We have to specify the bucket where the data is located zarr_bucket = 'solar-pv-nowcasting-data/satellite/EUMETSAT/SEVIRI_RSS/full_extent_TM_int16'","title":"User Inputs"},{"location":"103_loading/#loading-data","text":"Then the satip wrapper for loading data will generate an xarray Dataset ds = io . load_from_zarr_bucket ( zarr_bucket ) ds We can then index this as we would any other xarray object da_HRV_sample = ds [ 'stacked_eumetsat_data' ] . isel ( time = 0 ) . sel ( variable = 'HRV' ) da_HRV_sample As well as visualise it, here we'll use cartopy to plot the data with a coastline overlay. The darker area on the right hand side of the image are the areas where the sun has already set. fig = plt . figure ( dpi = 250 , figsize = ( 10 , 10 )) ax = plt . axes ( projection = ccrs . TransverseMercator ()) da_HRV_sample . T . plot . imshow ( ax = ax , cmap = 'magma' , vmin =- 200 , vmax = 400 ) ax . coastlines ( resolution = '50m' , alpha = 0.8 , color = 'white' )","title":"Loading Data"},{"location":"API/01-parsers/","text":"Parsers \u00b6 ::: satip.parsers","title":"Parsers"},{"location":"API/01-parsers/#parsers","text":"::: satip.parsers","title":"Parsers"},{"location":"about/OCF/","text":"Open Climate Fix \u00b6 Open Climate Fix is a new non-profit research and development lab, totally focused on reducing greenhouse gas emissions as rapidly as possible. Every part of the organisation is designed to maximise climate impact, such as our open and collaborative approach, our rapid prototyping, and our attention on finding scalable & practical solutions. By using an open-source approach, we can draw upon a much larger pool of expertise than any individual company, so combining existing islands of knowledge and accelerating progress. Our approach will be to search for ML (Machine Learning) problems where, if we solve a well-defined ML task, then there is likely to be a large climate impact. Then, for each of these challenges, we will: Collate & release data , and write software tools to make it super-easy for people to consume this data. Run a collaborative \u201cglobal research project\u201d where everyone from 16-year-olds to PhD students to corporate research labs can help solve the ML task (and, over the last 6 weeks, we have received over 300 emails from people who\u2019d love to get involved). Help to put good solutions into production , once the community has developed them, so we can be reducing emissions ASAP. :fontawesome-regular-envelope: Sign up to our newsletter","title":"Parsers"},{"location":"about/OCF/#open-climate-fix","text":"Open Climate Fix is a new non-profit research and development lab, totally focused on reducing greenhouse gas emissions as rapidly as possible. Every part of the organisation is designed to maximise climate impact, such as our open and collaborative approach, our rapid prototyping, and our attention on finding scalable & practical solutions. By using an open-source approach, we can draw upon a much larger pool of expertise than any individual company, so combining existing islands of knowledge and accelerating progress. Our approach will be to search for ML (Machine Learning) problems where, if we solve a well-defined ML task, then there is likely to be a large climate impact. Then, for each of these challenges, we will: Collate & release data , and write software tools to make it super-easy for people to consume this data. Run a collaborative \u201cglobal research project\u201d where everyone from 16-year-olds to PhD students to corporate research labs can help solve the ML task (and, over the last 6 weeks, we have received over 300 emails from people who\u2019d love to get involved). Help to put good solutions into production , once the community has developed them, so we can be reducing emissions ASAP. :fontawesome-regular-envelope: Sign up to our newsletter","title":"Open Climate Fix"}]}